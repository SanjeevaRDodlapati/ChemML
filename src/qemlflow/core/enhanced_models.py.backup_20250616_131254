Enhanced ML Models for ChemML
============================

and specialized architectures for chemistry and drug discovery applications.

Key Features:
- Ensemble methods (Voting, Stacking, Bagging)
- Gradient boosting (XGBoost, LightGBM, CatBoost)
- Advanced neural architectures (CNN, LSTM, Attention)
- Multi-task and transfer learning
- Automated model selection and hyperparameter optimization
"""

import warnings
from abc import ABC, abstractmethod
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin
from sklearn.ensemble import (
    BaggingClassifier,
    BaggingRegressor,
    VotingClassifier,
    VotingRegressor,
)
from sklearn.metrics import (
    accuracy_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    roc_auc_score,
)
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Import base model from core
try:
    from .models import BaseModel
except ImportError:
    from chemml.core.models import BaseModel

# Optional imports with graceful fallbacks
try:
    import xgboost as xgb

    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False

try:
    import lightgbm as lgb

    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False

try:
    import catboost as cb

    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, TensorDataset

    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import optuna

    HAS_OPTUNA = True
except ImportError:
    HAS_OPTUNA = False


class EnsembleModel(BaseModel):
    """
    Ensemble model combining multiple base models using voting or stacking.

    Supports both classification and regression with automatic model selection.
    """

    def __init__(
        self,
        base_models: Optional[List[BaseModel]] = None,
        ensemble_method: str = "voting",
        voting_strategy: str = "soft",
        meta_model: Optional[BaseModel] = None,
        cv_folds: int = 5,
        **kwargs,
    ):
        """
        Initialize ensemble model.

        Args:
            base_models: List of base models to ensemble
            ensemble_method: 'voting' or 'stacking'
            voting_strategy: 'soft' or 'hard' (classification only)
            meta_model: Meta-learner for stacking (if None, uses LinearRegression)
            cv_folds: Cross-validation folds for stacking
        """
        super().__init__(**kwargs)
        self.base_models = base_models or self._get_default_models()
        self.ensemble_method = ensemble_method
        self.voting_strategy = voting_strategy
        self.meta_model = meta_model
        self.cv_folds = cv_folds
        self.ensemble = None

    def _get_default_models(self) -> List[BaseModel]:
        """Get default set of base models."""
        from .models import LinearModel, RandomForestModel, SVMModel

        models = [
            LinearModel(task_type=self.task_type, regularization="ridge"),
            RandomForestModel(task_type=self.task_type, n_estimators=100),
            SVMModel(task_type=self.task_type, kernel="rbf"),
        ]
        return models

    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
        """Fit ensemble model."""
        if self.ensemble_method == "voting":
            return self._fit_voting(X, y, **kwargs)
        elif self.ensemble_method == "stacking":
            return self._fit_stacking(X, y, **kwargs)
        else:
            raise ValueError(f"Unknown ensemble method: {self.ensemble_method}")

    def _fit_voting(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
        """Fit voting ensemble."""
        # Prepare estimators for sklearn ensemble
        estimators = []
        for i, model in enumerate(self.base_models):
            # Fit individual model
            model.fit(X, y)
            # Convert to sklearn-compatible estimator
            estimators.append((f"model_{i}", model.model))

        # Create ensemble
        if self.task_type == "regression":
            self.ensemble = VotingRegressor(estimators=estimators)
        else:
            self.ensemble = VotingClassifier(
                estimators=estimators, voting=self.voting_strategy
            )

        # Fit ensemble
        self.ensemble.fit(X, y)
        self.is_fitted = True

        # Evaluate on validation set
        from sklearn.model_selection import train_test_split

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        val_metrics = self.evaluate(X_val, y_val)

        return val_metrics

    def _fit_stacking(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
        """Fit stacking ensemble."""
        from sklearn.ensemble import StackingClassifier, StackingRegressor
        from sklearn.linear_model import LinearRegression, LogisticRegression

        # Prepare base estimators
        estimators = []
        for i, model in enumerate(self.base_models):
            model.fit(X, y)
            estimators.append((f"model_{i}", model.model))

        # Prepare meta-learner
        if self.meta_model is None:
            if self.task_type == "regression":
                meta_learner = LinearRegression()
            else:
                meta_learner = LogisticRegression(max_iter=1000)
        else:
            meta_learner = self.meta_model.model

        # Create stacking ensemble
        if self.task_type == "regression":
            self.ensemble = StackingRegressor(
                estimators=estimators, final_estimator=meta_learner, cv=self.cv_folds
            )
        else:
            self.ensemble = StackingClassifier(
                estimators=estimators, final_estimator=meta_learner, cv=self.cv_folds
            )

        # Fit ensemble
        self.ensemble.fit(X, y)
        self.is_fitted = True

        # Evaluate

        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        val_metrics = self.evaluate(X_val, y_val)

        return val_metrics

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using ensemble."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before making predictions")
        return self.ensemble.predict(X)


if HAS_XGBOOST:

    class XGBoostModel(BaseModel):
        """
        XGBoost model for regression and classification.

        High-performance gradient boosting with excellent default parameters.
        """

        def __init__(
            self,
            n_estimators: int = 100,
            learning_rate: float = 0.1,
            max_depth: int = 6,
            subsample: float = 0.8,
            colsample_bytree: float = 0.8,
            random_state: int = 42,
            **kwargs,
        ):
            """Initialize XGBoost model."""
            super().__init__(**kwargs)
            self.n_estimators = n_estimators
            self.learning_rate = learning_rate
            self.max_depth = max_depth
            self.subsample = subsample
            self.colsample_bytree = colsample_bytree
            self.random_state = random_state

            if self.task_type == "regression":
                self.model = xgb.XGBRegressor(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    max_depth=max_depth,
                    subsample=subsample,
                    colsample_bytree=colsample_bytree,
                    random_state=random_state,
                    verbosity=0,
                )
            else:
                self.model = xgb.XGBClassifier(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    max_depth=max_depth,
                    subsample=subsample,
                    colsample_bytree=colsample_bytree,
                    random_state=random_state,
                    verbosity=0,
                )

        def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
            """Fit XGBoost model."""

            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Fit with early stopping
            if self.task_type == "regression":
                eval_metric = "rmse"
            else:
                eval_metric = "logloss"

            self.model.fit(
                X_train,
                y_train,
                eval_set=[(X_val, y_val)],
                eval_metric=eval_metric,
                early_stopping_rounds=20,
                verbose=False,
            )

            self.is_fitted = True
            val_metrics = self.evaluate(X_val, y_val)
            return val_metrics

        def predict(self, X: np.ndarray) -> np.ndarray:
            """Make predictions."""
            if not self.is_fitted:
                raise ValueError("Model must be fitted before making predictions")
            return self.model.predict(X)

        def get_feature_importance(self) -> np.ndarray:
            """Get feature importance scores."""
            if not self.is_fitted:
                raise ValueError("Model must be fitted first")
            return self.model.feature_importances_


if HAS_LIGHTGBM:

    class LightGBMModel(BaseModel):
        """
        LightGBM model for fast gradient boosting.

        Memory-efficient and fast training with excellent performance.
        """

        def __init__(
            self,
            n_estimators: int = 100,
            learning_rate: float = 0.1,
            num_leaves: int = 31,
            feature_fraction: float = 0.8,
            bagging_fraction: float = 0.8,
            random_state: int = 42,
            **kwargs,
        ):
            """Initialize LightGBM model."""
            super().__init__(**kwargs)
            self.n_estimators = n_estimators
            self.learning_rate = learning_rate
            self.num_leaves = num_leaves
            self.feature_fraction = feature_fraction
            self.bagging_fraction = bagging_fraction
            self.random_state = random_state

            if self.task_type == "regression":
                self.model = lgb.LGBMRegressor(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    num_leaves=num_leaves,
                    feature_fraction=feature_fraction,
                    bagging_fraction=bagging_fraction,
                    random_state=random_state,
                    verbosity=-1,
                )
            else:
                self.model = lgb.LGBMClassifier(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    num_leaves=num_leaves,
                    feature_fraction=feature_fraction,
                    bagging_fraction=bagging_fraction,
                    random_state=random_state,
                    verbosity=-1,
                )

        def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
            """Fit LightGBM model."""

            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            self.model.fit(
                X_train,
                y_train,
                eval_set=[(X_val, y_val)],
                eval_metric="rmse" if self.task_type == "regression" else "logloss",
                early_stopping_rounds=20,
                verbose=False,
            )

            self.is_fitted = True
            val_metrics = self.evaluate(X_val, y_val)
            return val_metrics

        def predict(self, X: np.ndarray) -> np.ndarray:
            """Make predictions."""
            if not self.is_fitted:
                raise ValueError("Model must be fitted before making predictions")
            return self.model.predict(X)

        def get_feature_importance(self) -> np.ndarray:
            """Get feature importance scores."""
            if not self.is_fitted:
                raise ValueError("Model must be fitted first")
            return self.model.feature_importances_


if HAS_TORCH:

    class ConvolutionalNN(BaseModel, nn.Module):
        """
        1D Convolutional Neural Network for molecular fingerprints.

        Effective for learning patterns in molecular bit vectors and descriptors.
        """

        def __init__(
            self,
            input_dim: int,
            num_conv_layers: int = 3,
            conv_channels: List[int] = [64, 128, 256],
            kernel_sizes: List[int] = [3, 3, 3],
            pool_sizes: List[int] = [2, 2, 2],
            hidden_dims: List[int] = [512, 256],
            dropout_rate: float = 0.3,
            output_dim: int = 1,
            **kwargs,
        ):
            """Initialize 1D CNN."""
            BaseModel.__init__(self, **kwargs)
            nn.Module.__init__(self)

            self.input_dim = input_dim
            self.num_conv_layers = num_conv_layers
            self.conv_channels = conv_channels or [64] * num_conv_layers
            self.kernel_sizes = kernel_sizes or [3] * num_conv_layers
            self.pool_sizes = pool_sizes or [2] * num_conv_layers
            self.hidden_dims = hidden_dims
            self.dropout_rate = dropout_rate
            self.output_dim = output_dim

            # Build convolutional layers
            conv_layers = []
            in_channels = 1  # Treat input as 1D signal
            current_length = input_dim

            for i in range(num_conv_layers):
                out_channels = (
                    self.conv_channels[i] if i < len(self.conv_channels) else 64
                )
                kernel_size = self.kernel_sizes[i] if i < len(self.kernel_sizes) else 3
                pool_size = self.pool_sizes[i] if i < len(self.pool_sizes) else 2

                conv_layers.extend(
                    [
                        nn.Conv1d(
                            in_channels,
                            out_channels,
                            kernel_size,
                            padding=kernel_size // 2,
                        ),
                        nn.ReLU(),
                        nn.MaxPool1d(pool_size),
                        nn.Dropout(dropout_rate),
                    ]
                )

                in_channels = out_channels
                current_length = current_length // pool_size

            self.conv_layers = nn.Sequential(*conv_layers)

            # Calculate flattened size
            flattened_size = in_channels * current_length

            # Build fully connected layers
            fc_layers = []
            prev_dim = flattened_size

            for hidden_dim in hidden_dims:
                fc_layers.extend(
                    [
                        nn.Linear(prev_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Dropout(dropout_rate),
                    ]
                )
                prev_dim = hidden_dim

            fc_layers.append(nn.Linear(prev_dim, output_dim))
            self.fc_layers = nn.Sequential(*fc_layers)

            self.scaler = StandardScaler()

        def forward(self, x):
            """Forward pass."""
            # Reshape for 1D conv: (batch_size, 1, features)
            x = x.unsqueeze(1)
            x = self.conv_layers(x)
            x = x.view(x.size(0), -1)  # Flatten
            x = self.fc_layers(x)
            return x

        def fit(
            self,
            X: np.ndarray,
            y: np.ndarray,
            epochs: int = 100,
            batch_size: int = 32,
            learning_rate: float = 0.001,
            **kwargs,
        ) -> Dict[str, float]:
            """Fit CNN model."""

            # Preprocess data
            X_scaled = self.scaler.fit_transform(X)
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )

            # Convert to tensors
            X_train_tensor = torch.FloatTensor(X_train)
            y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))
            X_val_tensor = torch.FloatTensor(X_val)
            y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1))

            # Create data loaders
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(
                train_dataset, batch_size=batch_size, shuffle=True
            )

            # Setup training
            optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
            criterion = (
                nn.MSELoss()
                if self.task_type == "regression"
                else nn.CrossEntropyLoss()
            )

            # Training loop
            self.train()
            for epoch in range(epochs):
                for batch_x, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = self.forward(batch_x)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()

                # Print progress occasionally
                if (epoch + 1) % 20 == 0:
                    self.eval()
                    with torch.no_grad():
                        val_outputs = self.forward(X_val_tensor)
                        val_loss = criterion(val_outputs, y_val_tensor)
                        print(
                            f"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss.item():.4f}"
                        )
                    self.train()

            self.is_fitted = True

            # Evaluate
            self.eval()
            with torch.no_grad():
                val_outputs = self.forward(X_val_tensor)
                val_predictions = val_outputs.numpy().flatten()

            if self.task_type == "regression":
                mse = mean_squared_error(y_val, val_predictions)
                r2 = r2_score(y_val, val_predictions)
                return {"mse": mse, "r2": r2, "rmse": np.sqrt(mse)}
            else:
                acc = accuracy_score(y_val, (val_predictions > 0.5).astype(int))
                return {"accuracy": acc}

        def predict(self, X: np.ndarray) -> np.ndarray:
            """Make predictions."""
            if not self.is_fitted:
                raise ValueError("Model must be fitted before making predictions")

            X_scaled = self.scaler.transform(X)
            X_tensor = torch.FloatTensor(X_scaled)

            self.eval()
            with torch.no_grad():
                outputs = self.forward(X_tensor)
                return outputs.numpy().flatten()


class AutoMLModel(BaseModel):
    """
    Automated Machine Learning model with hyperparameter optimization.

    Automatically selects best model and hyperparameters using optuna.
    """

    def __init__(
        self,
        model_types: Optional[List[str]] = None,
        optimization_metric: str = "auto",
        n_trials: int = 100,
        cv_folds: int = 5,
        timeout: Optional[int] = None,
        **kwargs,
    ):
        """
        Initialize AutoML model.

        Args:
            model_types: List of model types to try ('rf', 'xgb', 'lgb', 'svm', 'linear')
            optimization_metric: Metric to optimize ('rmse', 'r2', 'accuracy', 'auc', 'auto')
            n_trials: Number of optimization trials
            cv_folds: Cross-validation folds
            timeout: Timeout in seconds
        """
        super().__init__(**kwargs)
        self.model_types = model_types or ["rf", "xgb", "lgb", "svm", "linear"]
        self.optimization_metric = optimization_metric
        self.n_trials = n_trials
        self.cv_folds = cv_folds
        self.timeout = timeout
        self.best_model = None
        self.best_params = None
        self.study = None

        # Filter available models
        available_models = ["rf", "svm", "linear"]
        if HAS_XGBOOST:
            available_models.append("xgb")
        if HAS_LIGHTGBM:
            available_models.append("lgb")

        self.model_types = [m for m in self.model_types if m in available_models]

        if not self.model_types:
            raise ValueError("No available model types found")

    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, float]:
        """Fit AutoML model with hyperparameter optimization."""
        if not HAS_OPTUNA:
            warnings.warn("Optuna not available. Using default Random Forest.")
            from .models import RandomForestModel

            self.best_model = RandomForestModel(task_type=self.task_type)
            return self.best_model.fit(X, y)

        # Determine optimization metric
        if self.optimization_metric == "auto":
            if self.task_type == "regression":
                metric = "rmse"
                direction = "minimize"
            else:
                metric = "accuracy"
                direction = "maximize"
        else:
            metric = self.optimization_metric
            direction = "minimize" if metric in ["rmse", "mse", "mae"] else "maximize"

        # Create study
        self.study = optuna.create_study(direction=direction)

        # Optimize
        objective = self._create_objective(X, y, metric)
        self.study.optimize(objective, n_trials=self.n_trials, timeout=self.timeout)

        # Get best parameters and train final model
        self.best_params = self.study.best_params
        model_type = self.best_params.pop("model_type")
        self.best_model = self._create_model(model_type, self.best_params)

        # Train final model
        final_metrics = self.best_model.fit(X, y)
        self.is_fitted = True

        return final_metrics

    def _create_objective(self, X: np.ndarray, y: np.ndarray, metric: str):
        """Create optimization objective function."""

        def objective(trial):
            # Suggest model type
            model_type = trial.suggest_categorical("model_type", self.model_types)

            # Suggest hyperparameters based on model type
            params = {}

            if model_type == "rf":
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "max_depth": trial.suggest_int("max_depth", 3, 20),
                    "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
                    "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
                }
            elif model_type == "xgb" and HAS_XGBOOST:
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "subsample": trial.suggest_float("subsample", 0.6, 1.0),
                    "colsample_bytree": trial.suggest_float(
                        "colsample_bytree", 0.6, 1.0
                    ),
                }
            elif model_type == "lgb" and HAS_LIGHTGBM:
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 50, 300),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                    "num_leaves": trial.suggest_int("num_leaves", 10, 100),
                    "feature_fraction": trial.suggest_float(
                        "feature_fraction", 0.5, 1.0
                    ),
                    "bagging_fraction": trial.suggest_float(
                        "bagging_fraction", 0.5, 1.0
                    ),
                }
            elif model_type == "svm":
                params = {
                    "C": trial.suggest_float("C", 0.1, 100, log=True),
                    "gamma": trial.suggest_categorical("gamma", ["scale", "auto"]),
                    "kernel": trial.suggest_categorical(
                        "kernel", ["linear", "rbf", "poly"]
                    ),
                }
            elif model_type == "linear":
                params = {
                    "regularization": trial.suggest_categorical(
                        "regularization", ["none", "ridge", "lasso"]
                    ),
                    "alpha": trial.suggest_float("alpha", 0.001, 10, log=True),
                }

            # Create and evaluate model with robust error handling
            try:
                model = self._create_model(model_type, params)

                # Enhanced cross-validation with better error handling
                from sklearn.model_selection import (
                    KFold,
                    StratifiedKFold,
                    cross_val_score,
                )

                # Choose appropriate CV strategy
                if self.task_type == "classification" and len(np.unique(y)) > 1:
                    # Use stratified CV for classification
                    try:
                        cv_strategy = StratifiedKFold(
                            n_splits=min(self.cv_folds, len(np.unique(y))),
                            shuffle=True,
                            random_state=42,
                        )
                    except ValueError:
                        # Fall back to regular KFold if stratification fails
                        cv_strategy = KFold(
                            n_splits=min(self.cv_folds, len(y) // 2),
                            shuffle=True,
                            random_state=42,
                        )
                else:
                    # Use regular KFold for regression
                    cv_strategy = KFold(
                        n_splits=min(self.cv_folds, len(y) // 2),
                        shuffle=True,
                        random_state=42,
                    )

                # Perform cross-validation with error handling
                if metric == "rmse":
                    scores = cross_val_score(
                        model.model,
                        X,
                        y,
                        cv=cv_strategy,
                        scoring="neg_mean_squared_error",
                        error_score="raise",
                    )
                    return np.sqrt(-scores.mean()) if len(scores) > 0 else 1e6
                elif metric == "r2":
                    scores = cross_val_score(
                        model.model,
                        X,
                        y,
                        cv=cv_strategy,
                        scoring="r2",
                        error_score="raise",
                    )
                    return scores.mean() if len(scores) > 0 else -1e6
                elif metric == "accuracy":
                    scores = cross_val_score(
                        model.model,
                        X,
                        y,
                        cv=cv_strategy,
                        scoring="accuracy",
                        error_score="raise",
                    )
                    return scores.mean() if len(scores) > 0 else 0.0
                else:
                    # Default to mean squared error
                    scores = cross_val_score(
                        model.model,
                        X,
                        y,
                        cv=cv_strategy,
                        scoring="neg_mean_squared_error",
                        error_score="raise",
                    )
                    return -scores.mean() if len(scores) > 0 else 1e6

            except Exception as e:
                # Return poor score if model fails
                warnings.warn(f"Model {model_type} failed during CV: {e}")
                if metric in ["rmse", "neg_mse"]:
                    return 1e6  # Large error for minimization
                else:
                    return -1e6  # Large negative score for maximization

        return objective

    def _create_model(self, model_type: str, params: Dict) -> BaseModel:
        """Create model instance with given parameters."""

        if model_type == "rf":
            return RandomForestModel(task_type=self.task_type, **params)
        elif model_type == "xgb" and HAS_XGBOOST:
            return XGBoostModel(task_type=self.task_type, **params)
        elif model_type == "lgb" and HAS_LIGHTGBM:
            return LightGBMModel(task_type=self.task_type, **params)
        elif model_type == "svm":
            return SVMModel(task_type=self.task_type, **params)
        elif model_type == "linear":
            return LinearModel(task_type=self.task_type, **params)
        else:
            raise ValueError(f"Unknown model type: {model_type}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using best model."""
        if not self.is_fitted or self.best_model is None:
            raise ValueError("Model must be fitted before making predictions")
        return self.best_model.predict(X)

    def get_best_params(self) -> Dict[str, Any]:
        """Get best hyperparameters found."""
        if self.best_params is None:
            raise ValueError("Model must be fitted first")
        return self.best_params.copy()


# Convenience functions
def create_ensemble_model(
    base_models: Optional[List[BaseModel]] = None, **kwargs
) -> EnsembleModel:
    """Create an ensemble model."""
    return EnsembleModel(base_models=base_models, **kwargs)


def create_automl_model(**kwargs) -> AutoMLModel:
    """Create an AutoML model."""
    return AutoMLModel(**kwargs)


if HAS_XGBOOST:

    def create_xgboost_model(task_type: str = "regression", **kwargs) -> XGBoostModel:
        """Create an XGBoost model."""
        return XGBoostModel(task_type=task_type, **kwargs)


if HAS_LIGHTGBM:

    def create_lightgbm_model(task_type: str = "regression", **kwargs) -> LightGBMModel:
        """Create a LightGBM model."""
        return LightGBMModel(task_type=task_type, **kwargs)


if HAS_TORCH:

    def create_cnn_model(input_dim: int, **kwargs) -> ConvolutionalNN:
        """Create a 1D CNN model."""
        return ConvolutionalNN(input_dim=input_dim, **kwargs)


# Export all models and functions
__all__ = [
    "EnsembleModel",
    "AutoMLModel",
    "create_ensemble_model",
    "create_automl_model",
]

if HAS_XGBOOST:
    __all__.extend(["XGBoostModel", "create_xgboost_model"])

if HAS_LIGHTGBM:
    __all__.extend(["LightGBMModel", "create_lightgbm_model"])

if HAS_TORCH:
    __all__.extend(["ConvolutionalNN", "create_cnn_model"])
