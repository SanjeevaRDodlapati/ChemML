{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1184dab4",
   "metadata": {},
   "source": [
    "# Day 2 Project: Deep Learning for Molecules üß†\n",
    "\n",
    "## Advanced Neural Architectures - 6 Hours of Intensive Coding\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master Graph Neural Networks (GNNs) and Graph Attention Networks (GATs)\n",
    "- Implement transformer architectures for molecular data\n",
    "- Build generative models for molecule generation\n",
    "- Compare advanced deep learning approaches\n",
    "\n",
    "**Skills Building Path:**\n",
    "- **Section 1:** Graph Neural Networks Mastery (1.5 hours)\n",
    "- **Section 2:** Graph Attention Networks (GATs) (1.5 hours)\n",
    "- **Section 3:** Transformer Architectures for Chemistry (1.5 hours)\n",
    "- **Section 4:** Generative Models Implementation (1 hour)\n",
    "- **Section 5:** Advanced Integration & Benchmarking (0.5 hours)\n",
    "\n",
    "**Cross-References:**\n",
    "- üîó **Day 1:** Builds on molecular representations and basic GNNs\n",
    "- üîó **Week 7 Checkpoint:** Quantum chemistry computational methods\n",
    "- üîó **Week 8 Checkpoint:** Advanced modeling and virtual screening\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6ff88",
   "metadata": {},
   "source": [
    "## Section 1: Graph Neural Networks Mastery (1.5 hours)\n",
    "\n",
    "**Objective:** Deep dive into GNN architectures and message passing frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7935a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Assessment Framework Setup\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from assessment_framework import BootcampAssessment, create_widget, create_dashboard\n",
    "    print(\"‚úÖ Assessment framework loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Assessment framework not found - creating basic tracking\")\n",
    "    class BootcampAssessment:\n",
    "        def __init__(self, student_name, day):\n",
    "            self.student_name = student_name\n",
    "            self.day = day\n",
    "            self.activities = []\n",
    "        def record_activity(self, activity, data):\n",
    "            self.activities.append({\"activity\": activity, \"data\": data, \"timestamp\": datetime.now()})\n",
    "        def get_progress_summary(self):\n",
    "            return {\"overall_score\": 0.75, \"section_scores\": {}}\n",
    "    def create_widget(assessment, section, concepts, activities, time_target=90, section_type=\"assessment\"):\n",
    "        return type('MockWidget', (), {'display': lambda: print(f\"üìã {section} - Interactive assessment widget\")})()  \n",
    "\n",
    "# Initialize Assessment System\n",
    "student_name = input(\"üë®‚Äçüî¨ Enter your name: \") or \"Student\"\n",
    "assessment = BootcampAssessment(student_name, \"Day 2\")\n",
    "\n",
    "print(f\"\\nüéÜ Welcome {student_name} to Day 2: Deep Learning for Molecules!\")\n",
    "print(f\"üìÖ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Target completion: 6 hours of intensive deep learning\")\n",
    "\n",
    "# Start Day 2 assessment tracking\n",
    "assessment.record_activity(\"day2_start\", {\n",
    "    \"day\": \"Day 2: Deep Learning for Molecules\",\n",
    "    \"start_time\": datetime.now().isoformat(),\n",
    "    \"target_duration_hours\": 6,\n",
    "    \"sections\": 5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0412052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Section 1 Assessment: Graph Neural Networks Mastery\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SECTION 1 ASSESSMENT: Graph Neural Networks Mastery\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create assessment widget for GNN section\n",
    "section1_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 1: Graph Neural Networks Mastery\",\n",
    "    concepts=[\n",
    "        \"Graph representation of molecules\",\n",
    "        \"Message passing neural networks\",\n",
    "        \"GCN (Graph Convolutional Networks) architecture\",\n",
    "        \"Node and graph-level predictions\",\n",
    "        \"PyTorch Geometric framework usage\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Convert molecules to graph structures\",\n",
    "        \"Implement GCN layers for molecular property prediction\",\n",
    "        \"Train graph neural networks on chemical datasets\",\n",
    "        \"Compare GNN performance with traditional ML methods\",\n",
    "        \"Visualize learned molecular representations\"\n",
    "    ],\n",
    "    time_estimate=90\n",
    ")\n",
    "\n",
    "section1_widget.display()\n",
    "\n",
    "print(\"\\nüß† Prerequisites Check:\")\n",
    "print(\"1. Day 1 molecular representations mastered?\")\n",
    "print(\"2. PyTorch basics understood?\")\n",
    "print(\"3. Graph theory concepts familiar?\")\n",
    "print(\"4. Ready for advanced deep learning architectures?\")\n",
    "\n",
    "# Record section start\n",
    "section1_start = datetime.now()\n",
    "assessment.record_activity(\"section1_start\", {\n",
    "    \"section\": \"GNN Mastery\",\n",
    "    \"start_time\": section1_start.isoformat(),\n",
    "    \"prerequisites_checked\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced imports for deep learning on molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ Starting Day 2: Deep Learning for Molecules\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üíª Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Hands-On Exercise 2.1: Molecular Graph Construction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ†Ô∏è HANDS-ON EXERCISE 2.1: Molecular Graph Construction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def mol_to_graph(mol):\n",
    "    \"\"\"\n",
    "    Convert RDKit molecule to PyTorch Geometric graph\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Get atom features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetDegree(),\n",
    "            atom.GetFormalCharge(),\n",
    "            int(atom.GetHybridization()),\n",
    "            int(atom.GetIsAromatic())\n",
    "        ]\n",
    "        atom_features.append(features)\n",
    "    \n",
    "    # Get bond information (edges)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Add edge in both directions (undirected graph)\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        \n",
    "        # Bond features\n",
    "        bond_type = bond.GetBondType()\n",
    "        bond_features = [\n",
    "            float(bond_type == Chem.rdchem.BondType.SINGLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.DOUBLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.TRIPLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.AROMATIC),\n",
    "            float(bond.GetIsConjugated())\n",
    "        ]\n",
    "        edge_features.extend([bond_features, bond_features])  # Both directions\n",
    "    \n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float) if edge_features else None\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Test with sample molecules\n",
    "test_molecules = {\n",
    "    'Benzene': 'c1ccccc1',\n",
    "    'Caffeine': 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',\n",
    "    'Aspirin': 'CC(=O)OC1=CC=CC=C1C(=O)O'\n",
    "}\n",
    "\n",
    "print(\"üß™ Converting molecules to graphs:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "mol_graphs = {}\n",
    "for name, smiles in test_molecules.items():\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    graph = mol_to_graph(mol)\n",
    "    mol_graphs[name] = graph\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Atoms: {graph.x.size(0)}\")\n",
    "    print(f\"  Bonds: {graph.edge_index.size(1)//2}\")\n",
    "    print(f\"  Node features: {graph.x.size(1)}\")\n",
    "    print()\n",
    "\n",
    "# Record exercise completion\n",
    "assessment.record_activity(\"exercise_2_1\", {\n",
    "    \"exercise\": \"Molecular Graph Construction\",\n",
    "    \"molecules_processed\": len(mol_graphs),\n",
    "    \"graph_features_implemented\": True,\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Molecular graph construction mastered!\")\n",
    "print(\"üöÄ Ready to build Graph Neural Networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Hands-On Exercise 2.2: Graph Convolutional Network Implementation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ†Ô∏è HANDS-ON EXERCISE 2.2: GCN Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class MolecularGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network for molecular property prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Graph-level prediction layers\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for mean+max pooling\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolutions with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Global pooling to get graph-level representation\n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        \n",
    "        # Concatenate different pooling strategies\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "print(\"üèÆ Building Molecular GCN Model:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Determine input features from sample graph\n",
    "sample_graph = list(mol_graphs.values())[0]\n",
    "num_features = sample_graph.x.size(1)\n",
    "\n",
    "model = MolecularGCN(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=1,  # For regression (e.g., solubility prediction)\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input features: {num_features}\")\n",
    "print(f\"  Hidden dimension: 64\")\n",
    "print(f\"  Output classes: 1 (regression)\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass with sample data\n",
    "with torch.no_grad():\n",
    "    sample_batch = torch.zeros(sample_graph.x.size(0), dtype=torch.long)\n",
    "    output = model(sample_graph.x.to(device), \n",
    "                  sample_graph.edge_index.to(device), \n",
    "                  sample_batch.to(device))\n",
    "    print(f\"  Sample output shape: {output.shape}\")\n",
    "\n",
    "# Record model implementation\n",
    "assessment.record_activity(\"exercise_2_2\", {\n",
    "    \"exercise\": \"GCN Implementation\",\n",
    "    \"model_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "    \"architecture_layers\": 3,\n",
    "    \"pooling_strategies\": [\"mean\", \"max\"],\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ Graph Convolutional Network implemented successfully!\")\n",
    "print(\"üöÄ Ready for training on molecular datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Section 1 Completion Assessment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ SECTION 1 COMPLETION ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment for Section 1\n",
    "section1_completion = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 1 Completion: Graph Neural Networks Mastery\",\n",
    "    concepts=[\n",
    "        \"Molecular graph representation and conversion\",\n",
    "        \"PyTorch Geometric framework proficiency\",\n",
    "        \"GCN architecture understanding and implementation\",\n",
    "        \"Message passing mechanisms in molecular contexts\",\n",
    "        \"Graph-level pooling strategies for molecular properties\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Successfully converted molecules to graph structures\",\n",
    "        \"Implemented complete GCN model for molecular prediction\",\n",
    "        \"Tested forward pass with molecular graph data\",\n",
    "        \"Understood node features and edge attributes\",\n",
    "        \"Ready for advanced graph attention mechanisms\"\n",
    "    ],\n",
    "    time_estimate=90  # 1.5 hour section\n",
    ")\n",
    "\n",
    "section1_completion.display()\n",
    "\n",
    "# Calculate section timing\n",
    "section1_end = datetime.now()\n",
    "section1_duration = (section1_end - section1_start).total_seconds() / 60\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Section 1 Timing:\")\n",
    "print(f\"   Time spent: {section1_duration:.1f} minutes\")\n",
    "print(f\"   Target time: 90 minutes\")\n",
    "print(f\"   Efficiency: {'On track' if section1_duration <= 100 else 'Consider speeding up'}\")\n",
    "\n",
    "# Progress summary\n",
    "current_progress = assessment.get_progress_summary()\n",
    "print(f\"\\nüìä Current Progress Summary:\")\n",
    "print(f\"   Overall completion: {current_progress.get('completion_rate', 0)*100:.1f}%\")\n",
    "print(f\"   Concepts mastered: {current_progress.get('concepts_completed', 0)}\")\n",
    "print(f\"   Exercises completed: {len(assessment.session_data.get('activities', []))}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to move to Section 2: Graph Attention Networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment Framework Integration for Day 2\n",
    "from assessment_framework import create_assessment, create_widget, create_dashboard\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize assessment for Day 2\n",
    "student_id = input(\"Enter your student ID (or name): \").strip() or \"student_demo\"\n",
    "track = input(\"Choose track (quick/standard/intensive/extended): \").strip() or \"standard\"\n",
    "\n",
    "assessment = create_assessment(student_id=student_id, day=2, track=track)\n",
    "print(f\"\\nüéØ Assessment initialized for {student_id} - Day 2 ({track} track)\")\n",
    "print(f\"üìã Building on Day 1: ML & Cheminformatics Foundations\")\n",
    "print(f\"üìä Target completion time: {assessment.track_configs[track]['target_hours']} hours\")\n",
    "print(f\"üéØ Minimum completion rate: {assessment.track_configs[track]['min_completion']*100}%\")\n",
    "print(f\"üß™ Focus: Advanced neural architectures for molecular data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecular dataset and convert to PyTorch Geometric format\n",
    "print(\"üìä Preparing Molecular Graph Dataset:\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Load HIV dataset from DeepChem\n",
    "tasks, datasets, transformers = dc.molnet.load_hiv(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "print(f\"‚úÖ HIV Dataset loaded:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(valid_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Task: {tasks[0]} (HIV replication inhibition)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"üéÜ Ready for advanced deep learning on molecular data!\")print(\"\\n‚úÖ All libraries imported successfully!\")    print(\"üíª Using CPU - some operations may be slower\")else:    print(f\"üéÆ GPU detected: {torch.cuda.get_device_name(0)}\")if torch.cuda.is_available():print(f\"üíª Computing device: {device}\")device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# Check GPU availabilityprint(\"=\" * 50)print(\"üöÄ Starting Day 2: Deep Learning for Molecules\")warnings.filterwarnings('ignore')import warningsfrom rdkit.Chem import Descriptors, AllChemfrom rdkit import Chemimport deepchem as dcfrom torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_poolfrom torch_geometric.data import Data, DataLoaderimport torch.nn.functional as Fimport torch.nn as nnimport torchimport seaborn as snsimport matplotlib.pyplot as pltimport pandas as pd\n",
    "\n",
    "# Check GPU availability (continuing original content)# Original Day 2 notebook content continues with advanced implementations...\n",
    "import numpy as np# Advanced imports for deep learning on molecules\n",
    "# Note: Assessment framework integration complete for Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f49220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DeepChem data to PyTorch Geometric format\n",
    "def deepchem_to_pyg(dc_dataset, max_samples=1000):\n",
    "    \"\"\"Convert DeepChem dataset to PyTorch Geometric format\"\"\"\n",
    "    pyg_data_list = []\n",
    "    \n",
    "    print(f\"Converting {min(len(dc_dataset), max_samples)} samples to PyG format...\")\n",
    "    \n",
    "    for i in range(min(len(dc_dataset), max_samples)):\n",
    "        # Get graph from DeepChem\n",
    "        dc_graph = dc_dataset.X[i]\n",
    "        label = dc_dataset.y[i]\n",
    "        \n",
    "        # Extract node features and adjacency\n",
    "        node_features = torch.tensor(dc_graph.node_features, dtype=torch.float)\n",
    "        edge_index = torch.tensor(dc_graph.edge_index, dtype=torch.long)\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor([label[0]], dtype=torch.float)\n",
    "        )\n",
    "        pyg_data_list.append(data)\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f\"   Processed {i+1} samples...\")\n",
    "    \n",
    "    return pyg_data_list\n",
    "\n",
    "# Convert datasets\n",
    "train_pyg = deepchem_to_pyg(train_dataset, max_samples=800)\n",
    "valid_pyg = deepchem_to_pyg(valid_dataset, max_samples=200)\n",
    "test_pyg = deepchem_to_pyg(test_dataset, max_samples=200)\n",
    "\n",
    "print(f\"\\n‚úÖ PyG conversion complete:\")\n",
    "print(f\"   Train: {len(train_pyg)} graphs\")\n",
    "print(f\"   Valid: {len(valid_pyg)} graphs\")\n",
    "print(f\"   Test: {len(test_pyg)} graphs\")\n",
    "\n",
    "# Analyze graph structure\n",
    "sample_graph = train_pyg[0]\n",
    "print(f\"\\nüìä Sample Graph Analysis:\")\n",
    "print(f\"   Nodes: {sample_graph.x.shape[0]}\")\n",
    "print(f\"   Node features: {sample_graph.x.shape[1]}\")\n",
    "print(f\"   Edges: {sample_graph.edge_index.shape[1]}\")\n",
    "print(f\"   Label: {sample_graph.y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Graph Convolutional Network\n",
    "class MolecularGCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//4, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.conv1(x, edge_index))\n",
    "        x1 = F.dropout(x1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x2, edge_index))\n",
    "        \n",
    "        # Global pooling\n",
    "        x_pooled = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x_pooled)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "num_features = train_pyg[0].x.shape[1]\n",
    "model_gcn = MolecularGCN(num_features=num_features, hidden_dim=128).to(device)\n",
    "\n",
    "print(f\"üß† MolecularGCN Architecture:\")\n",
    "print(f\"   Input features: {num_features}\")\n",
    "print(f\"   Hidden dimension: 128\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_gcn.parameters()):,}\")\n",
    "print(f\"   Device: {next(model_gcn.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup and data loaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_pyg, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_pyg, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_pyg, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training configuration\n",
    "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"üèãÔ∏è Training Configuration:\")\n",
    "print(f\"   Batch size: 32\")\n",
    "print(f\"   Learning rate: 0.001\")\n",
    "print(f\"   Optimizer: Adam\")\n",
    "print(f\"   Loss function: Binary Cross Entropy\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for GCN\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = (out > 0.5).float()\n",
    "        correct += (pred == batch.y.unsqueeze(1)).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y.unsqueeze(1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = (out > 0.5).float()\n",
    "            correct += (pred == batch.y.unsqueeze(1)).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Train the GCN model\n",
    "print(\"üöÄ Training GCN Model:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, valid_losses = [], []\n",
    "train_accs, valid_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_gcn, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_gcn, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = evaluate(model_gcn, test_loader, criterion)\n",
    "print(f\"\\n‚úÖ Final GCN Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e3aa1",
   "metadata": {},
   "source": [
    "## Section 2: Graph Attention Networks (GATs) (1.5 hours)\n",
    "\n",
    "**Objective:** Implement attention mechanisms for molecular graphs and compare with standard GCNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd25d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Network implementation\n",
    "class MolecularGAT(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_heads=4, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGAT, self).__init__()\n",
    "        \n",
    "        # Graph attention layers\n",
    "        self.gat1 = GATConv(num_features, hidden_dim, heads=num_heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)\n",
    "        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim//2, heads=1, dropout=dropout)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//4, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Multi-head attention layers\n",
    "        x1 = F.relu(self.gat1(x, edge_index))\n",
    "        x1 = F.dropout(x1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.gat2(x1, edge_index))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gat3(x2, edge_index))\n",
    "        \n",
    "        # Global attention pooling\n",
    "        x_pooled = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x_pooled)\n",
    "        return out\n",
    "\n",
    "# Initialize GAT model\n",
    "model_gat = MolecularGAT(\n",
    "    num_features=num_features, \n",
    "    hidden_dim=64, \n",
    "    num_heads=4,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"üß† MolecularGAT Architecture:\")\n",
    "print(f\"   Input features: {num_features}\")\n",
    "print(f\"   Hidden dimension: 64\")\n",
    "print(f\"   Attention heads: 4\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_gat.parameters()):,}\")\n",
    "\n",
    "# Training setup for GAT\n",
    "optimizer_gat = torch.optim.Adam(model_gat.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97223fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT model\n",
    "print(\"üéØ Training GAT Model:\")\n",
    "print(\"=\" * 23)\n",
    "\n",
    "train_losses_gat, valid_losses_gat = [], []\n",
    "train_accs_gat, valid_accs_gat = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_gat, train_loader, optimizer_gat, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_gat, valid_loader, criterion)\n",
    "    \n",
    "    train_losses_gat.append(train_loss)\n",
    "    valid_losses_gat.append(valid_loss)\n",
    "    train_accs_gat.append(train_acc)\n",
    "    valid_accs_gat.append(valid_acc)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Evaluate GAT\n",
    "test_loss_gat, test_acc_gat = evaluate(model_gat, test_loader, criterion)\n",
    "print(f\"\\n‚úÖ Final GAT Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc_gat:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss_gat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GCN vs GAT performance\n",
    "print(\"üìä GCN vs GAT Comparison:\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['GCN', 'GAT'],\n",
    "    'Test_Accuracy': [test_acc, test_acc_gat],\n",
    "    'Test_Loss': [test_loss, test_loss_gat],\n",
    "    'Parameters': [\n",
    "        sum(p.numel() for p in model_gcn.parameters()),\n",
    "        sum(p.numel() for p in model_gat.parameters())\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Training loss\n",
    "axes[0,0].plot(train_losses, label='GCN', linewidth=2)\n",
    "axes[0,0].plot(train_losses_gat, label='GAT', linewidth=2)\n",
    "axes[0,0].set_title('Training Loss')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[0,1].plot(valid_losses, label='GCN', linewidth=2)\n",
    "axes[0,1].plot(valid_losses_gat, label='GAT', linewidth=2)\n",
    "axes[0,1].set_title('Validation Loss')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "axes[1,0].plot(train_accs, label='GCN', linewidth=2)\n",
    "axes[1,0].plot(train_accs_gat, label='GAT', linewidth=2)\n",
    "axes[1,0].set_title('Training Accuracy')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1,1].plot(valid_accs, label='GCN', linewidth=2)\n",
    "axes[1,1].plot(valid_accs_gat, label='GAT', linewidth=2)\n",
    "axes[1,1].set_title('Validation Accuracy')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine better model\n",
    "better_model = 'GAT' if test_acc_gat > test_acc else 'GCN'\n",
    "improvement = abs(test_acc_gat - test_acc)\n",
    "print(f\"\\nüèÜ Winner: {better_model}\")\n",
    "print(f\"   Improvement: {improvement:.4f} accuracy points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Section 2 Completion Assessment: Graph Attention Networks (GATs)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SECTION 2 COMPLETION: Graph Attention Networks (GATs)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for GAT section\n",
    "section2_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 2 Completion: Graph Attention Networks (GATs)\",\n",
    "    concepts=[\n",
    "        \"Attention mechanisms in graph neural networks\",\n",
    "        \"Multi-head attention for molecular graphs\",\n",
    "        \"Graph pooling strategies\",\n",
    "        \"Edge features and node embeddings\",\n",
    "        \"Attention weight interpretation\",\n",
    "        \"GAT vs GCN performance comparison\",\n",
    "        \"Hyperparameter tuning for attention models\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"GAT architecture implementation\",\n",
    "        \"Multi-head attention configuration\",\n",
    "        \"Attention visualization analysis\",\n",
    "        \"Performance comparison with GCN\",\n",
    "        \"Edge analysis and graph clustering\",\n",
    "        \"Hyperparameter optimization\",\n",
    "        \"Attention weight interpretation\"\n",
    "    ],\n",
    "    time_target=90,  # 1.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Section 2 Complete: Graph Attention Networks Mastery\")\n",
    "print(\"üöÄ Ready to advance to Section 3: Transformer Architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f990bfc",
   "metadata": {},
   "source": [
    "## Section 3: Transformer Architectures for Chemistry (1.5 hours)\n",
    "\n",
    "**Objective:** Implement transformer models for molecular sequence data and SMILES processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular Transformer for SMILES sequences\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class MolecularTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, \n",
    "                 max_length=128, num_classes=1, dropout=0.1):\n",
    "        super(MolecularTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self._generate_positional_encoding(max_length, d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _generate_positional_encoding(self, max_length, d_model):\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        batch_size, seq_length = x.shape\n",
    "        \n",
    "        # Token embedding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_length, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if padding_mask is not None:\n",
    "            # Mask out padded positions\n",
    "            mask = (~padding_mask).unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "print(\"ü§ñ Molecular Transformer Architecture Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889721ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES tokenization and vocabulary\n",
    "def tokenize_smiles(smiles_list):\n",
    "    \"\"\"Simple character-level tokenization for SMILES\"\"\"\n",
    "    # Define vocabulary\n",
    "    vocab = set()\n",
    "    for smiles in smiles_list:\n",
    "        for char in smiles:\n",
    "            vocab.add(char)\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab.update(['<PAD>', '<UNK>', '<START>', '<END>'])\n",
    "    \n",
    "    # Create mappings\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "def encode_smiles(smiles, char_to_idx, max_length=128):\n",
    "    \"\"\"Encode SMILES to token indices\"\"\"\n",
    "    tokens = [char_to_idx.get(char, char_to_idx['<UNK>']) for char in smiles]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([char_to_idx['<PAD>']] * (max_length - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Prepare SMILES data for transformer\n",
    "print(\"üìù Preparing SMILES Data for Transformer:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Get SMILES from DeepChem dataset (first 1000 samples)\n",
    "smiles_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i in range(min(1000, len(train_dataset))):\n",
    "    # Convert graph back to SMILES (simplified approach)\n",
    "    # In practice, you'd store original SMILES\n",
    "    smiles_list.append(f\"C{'C' * (i % 10)}O\")  # Simplified for demo\n",
    "    labels_list.append(train_dataset.y[i][0])\n",
    "\n",
    "# Create vocabulary\n",
    "char_to_idx, idx_to_char = tokenize_smiles(smiles_list)\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(f\"‚úÖ Vocabulary created:\")\n",
    "print(f\"   Vocabulary size: {vocab_size}\")\n",
    "print(f\"   Sample characters: {list(char_to_idx.keys())[:10]}\")\n",
    "\n",
    "# Encode SMILES\n",
    "encoded_smiles = [encode_smiles(smi, char_to_idx) for smi in smiles_list]\n",
    "encoded_tensor = torch.tensor(encoded_smiles, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "print(f\"   Encoded tensor shape: {encoded_tensor.shape}\")\n",
    "print(f\"   Labels tensor shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ada2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Molecular Transformer\n",
    "model_transformer = MolecularTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    max_length=128,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"ü§ñ Molecular Transformer:\")\n",
    "print(f\"   Vocabulary: {vocab_size}\")\n",
    "print(f\"   Model dimension: 128\")\n",
    "print(f\"   Attention heads: 8\")\n",
    "print(f\"   Layers: 4\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_transformer.parameters()):,}\")\n",
    "\n",
    "# Create dataset and dataloader for transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Split data\n",
    "n_train = int(0.8 * len(encoded_tensor))\n",
    "n_valid = int(0.1 * len(encoded_tensor))\n",
    "\n",
    "train_data = TensorDataset(encoded_tensor[:n_train], labels_tensor[:n_train])\n",
    "valid_data = TensorDataset(encoded_tensor[n_train:n_train+n_valid], \n",
    "                          labels_tensor[n_train:n_train+n_valid])\n",
    "test_data = TensorDataset(encoded_tensor[n_train+n_valid:], \n",
    "                         labels_tensor[n_train+n_valid:])\n",
    "\n",
    "train_loader_transformer = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader_transformer = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "test_loader_transformer = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"üìä Transformer dataset splits:\")\n",
    "print(f\"   Train: {len(train_data)}\")\n",
    "print(f\"   Valid: {len(valid_data)}\")\n",
    "print(f\"   Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions for transformer\n",
    "def train_transformer_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_data, batch_labels in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Create padding mask\n",
    "        padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch_data, padding_mask)\n",
    "        loss = criterion(out.squeeze(), batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = (out.squeeze() > 0.5).float()\n",
    "        correct += (pred == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate_transformer(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "            \n",
    "            out = model(batch_data, padding_mask)\n",
    "            loss = criterion(out.squeeze(), batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = (out.squeeze() > 0.5).float()\n",
    "            correct += (pred == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Train transformer\n",
    "optimizer_transformer = torch.optim.Adam(model_transformer.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"üöÄ Training Molecular Transformer:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "num_epochs_transformer = 15\n",
    "for epoch in range(num_epochs_transformer):\n",
    "    train_loss, train_acc = train_transformer_epoch(\n",
    "        model_transformer, train_loader_transformer, optimizer_transformer, criterion\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate_transformer(\n",
    "        model_transformer, valid_loader_transformer, criterion\n",
    "    )\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss_transformer, test_acc_transformer = evaluate_transformer(\n",
    "    model_transformer, test_loader_transformer, criterion\n",
    ")\n",
    "print(f\"\\n‚úÖ Transformer Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc_transformer:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss_transformer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ce9f8",
   "metadata": {},
   "source": [
    "## Section 4: Generative Models Implementation (1 hour)\n",
    "\n",
    "**Objective:** Build generative models for novel molecule creation using VAEs and GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Section 3 Completion Assessment: Transformer Architectures for Chemistry\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SECTION 3 COMPLETION: Transformer Architectures for Chemistry\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Transformer section\n",
    "section3_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 3 Completion: Transformer Architectures for Chemistry\",\n",
    "    concepts=[\n",
    "        \"Self-attention mechanisms for molecular sequences\",\n",
    "        \"Positional encoding for SMILES data\",\n",
    "        \"Transformer encoder-decoder architectures\",\n",
    "        \"Multi-head attention for chemical understanding\",\n",
    "        \"Molecular sequence processing and tokenization\",\n",
    "        \"BERT-style pre-training for chemistry\",\n",
    "        \"Fine-tuning transformers for molecular property prediction\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Molecular transformer implementation\",\n",
    "        \"SMILES sequence encoding and processing\",\n",
    "        \"Multi-head attention configuration\",\n",
    "        \"Positional encoding integration\",\n",
    "        \"Model training and optimization\",\n",
    "        \"Performance evaluation vs graph models\",\n",
    "        \"Sequence generation and analysis\"\n",
    "    ],\n",
    "    time_target=90,  # 1.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Section 3 Complete: Transformer Architectures Mastery\")\n",
    "print(\"üöÄ Ready to advance to Section 4: Generative Models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc72c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Models for Molecule Generation\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MolecularVAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for SMILES generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, latent_dim=128, max_length=128):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        output, (hidden, _) = self.encoder_lstm(embedded)\n",
    "        # Take the last hidden state from both directions\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, target_seq=None):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        hidden = self.decoder_input(z).unsqueeze(0)  # [1, batch_size, hidden_dim]\n",
    "        cell = torch.zeros_like(hidden)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        if target_seq is not None:\n",
    "            # Training mode - teacher forcing\n",
    "            target_embedded = self.embedding(target_seq)  # [batch_size, seq_len, embedding_dim]\n",
    "            \n",
    "            for i in range(target_seq.size(1)):\n",
    "                # Concatenate latent vector with current input\n",
    "                z_expanded = z.unsqueeze(1)  # [batch_size, 1, latent_dim]\n",
    "                decoder_input = torch.cat([target_embedded[:, i:i+1, :], z_expanded], dim=-1)\n",
    "                \n",
    "                output, (hidden, cell) = self.decoder_lstm(decoder_input, (hidden, cell))\n",
    "                output = self.output_layer(output.squeeze(1))\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            # Inference mode\n",
    "            current_input = torch.zeros(batch_size, 1, self.embedding_dim).to(z.device)\n",
    "            \n",
    "            for i in range(self.max_length):\n",
    "                z_expanded = z.unsqueeze(1)\n",
    "                decoder_input = torch.cat([current_input, z_expanded], dim=-1)\n",
    "                \n",
    "                output, (hidden, cell) = self.decoder_lstm(decoder_input, (hidden, cell))\n",
    "                output = self.output_layer(output.squeeze(1))\n",
    "                outputs.append(output)\n",
    "                \n",
    "                # Use output as next input\n",
    "                next_token = torch.argmax(output, dim=-1, keepdim=True)\n",
    "                current_input = self.embedding(next_token).unsqueeze(1)\n",
    "                \n",
    "            return torch.stack(outputs, dim=1)\n",
    "    \n",
    "    def forward(self, x, target_seq=None):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, target_seq)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"VAE loss with KL divergence and reconstruction loss\"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.cross_entropy(recon_x.view(-1, recon_x.size(-1)), x.view(-1), reduction='mean')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "print(\"üß¨ Building Molecular VAE for Generation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize VAE\n",
    "vae_model = MolecularVAE(\n",
    "    vocab_size=len(char_to_idx),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    latent_dim=64,\n",
    "    max_length=max_length\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ VAE Model created with {sum(p.numel() for p in vae_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "def train_vae_epoch(model, loader, optimizer, beta=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    \n",
    "    for batch_data, _ in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, logvar = model(batch_data, batch_data[:, :-1])\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, recon_loss, kl_loss = vae_loss_function(\n",
    "            recon_batch, batch_data[:, 1:], mu, logvar, beta\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_loss += kl_loss.item()\n",
    "    \n",
    "    return (total_loss / len(loader), \n",
    "            total_recon_loss / len(loader), \n",
    "            total_kl_loss / len(loader))\n",
    "\n",
    "# Train VAE\n",
    "optimizer_vae = torch.optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"üöÄ Training Molecular VAE:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "num_epochs_vae = 10\n",
    "beta_schedule = [min(1.0, i * 0.1) for i in range(num_epochs_vae)]  # Beta annealing\n",
    "\n",
    "for epoch in range(num_epochs_vae):\n",
    "    beta = beta_schedule[epoch]\n",
    "    \n",
    "    total_loss, recon_loss, kl_loss = train_vae_epoch(\n",
    "        vae_model, train_loader_transformer, optimizer_vae, beta\n",
    "    )\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} (Œ≤={beta:.1f}): Loss={total_loss:.4f}, \"\n",
    "              f\"Recon={recon_loss:.4f}, KL={kl_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ VAE Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6342eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecule Generation with VAE\n",
    "def generate_molecules(model, num_samples=10, temperature=1.0):\n",
    "    \"\"\"Generate novel molecules using trained VAE\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    generated_smiles = []\n",
    "    valid_molecules = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Sample from latent space\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device) * temperature\n",
    "        \n",
    "        # Decode to SMILES\n",
    "        outputs = model.decode(z)  # [num_samples, max_length, vocab_size]\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Convert logits to tokens\n",
    "            tokens = torch.argmax(outputs[i], dim=-1).cpu().numpy()\n",
    "            \n",
    "            # Convert tokens to SMILES\n",
    "            smiles = ''.join([idx_to_char[token] for token in tokens if token != char_to_idx['<PAD>']])\n",
    "            smiles = smiles.replace('<START>', '').replace('<END>', '')\n",
    "            \n",
    "            # Validate molecule\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    valid_molecules += 1\n",
    "                    canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                    generated_smiles.append(canonical_smiles)\n",
    "                else:\n",
    "                    generated_smiles.append(smiles + \" (INVALID)\")\n",
    "            except:\n",
    "                generated_smiles.append(smiles + \" (ERROR)\")\n",
    "    \n",
    "    return generated_smiles, valid_molecules / num_samples\n",
    "\n",
    "# Generate novel molecules\n",
    "print(\"üß™ Generating Novel Molecules with VAE:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "generated_mols, validity_rate = generate_molecules(vae_model, num_samples=20, temperature=0.8)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(generated_mols)} molecules\")\n",
    "print(f\"‚úÖ Validity Rate: {validity_rate:.2%}\")\n",
    "print(\"\\nüìã Sample Generated Molecules:\")\n",
    "for i, smiles in enumerate(generated_mols[:10]):\n",
    "    print(f\"   {i+1:2d}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular Property Optimization using VAE\n",
    "class PropertyOptimizer:\n",
    "    \"\"\"Optimize molecules for specific properties using VAE latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, vae_model, property_predictor):\n",
    "        self.vae_model = vae_model\n",
    "        self.property_predictor = property_predictor\n",
    "        \n",
    "    def encode_molecule(self, smiles):\n",
    "        \"\"\"Encode SMILES to latent vector\"\"\"\n",
    "        tokens = self.smiles_to_tokens(smiles)\n",
    "        tokens_tensor = torch.tensor([tokens]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.vae_model.encode(tokens_tensor)\n",
    "            z = self.vae_model.reparameterize(mu, logvar)\n",
    "        \n",
    "        return z.cpu().numpy()[0]\n",
    "    \n",
    "    def decode_latent(self, z):\n",
    "        \"\"\"Decode latent vector to SMILES\"\"\"\n",
    "        z_tensor = torch.tensor([z]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.vae_model.decode(z_tensor)\n",
    "            tokens = torch.argmax(outputs[0], dim=-1).cpu().numpy()\n",
    "        \n",
    "        smiles = ''.join([idx_to_char[token] for token in tokens if token != char_to_idx['<PAD>']])\n",
    "        return smiles.replace('<START>', '').replace('<END>', '')\n",
    "    \n",
    "    def smiles_to_tokens(self, smiles):\n",
    "        \"\"\"Convert SMILES to token sequence\"\"\"\n",
    "        smiles = '<START>' + smiles + '<END>'\n",
    "        tokens = [char_to_idx.get(c, char_to_idx['<UNK>']) for c in smiles]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokens) < max_length:\n",
    "            tokens.extend([char_to_idx['<PAD>']] * (max_length - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def optimize_property(self, target_property_value, num_iterations=100, learning_rate=0.1):\n",
    "        \"\"\"Optimize molecules for target property using gradient ascent in latent space\"\"\"\n",
    "        \n",
    "        # Start from random point in latent space\n",
    "        z = np.random.randn(self.vae_model.latent_dim) * 0.5\n",
    "        best_z = z.copy()\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        trajectory = []\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Generate molecule from current latent point\n",
    "            smiles = self.decode_latent(z)\n",
    "            \n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    # Calculate molecular properties\n",
    "                    mw = Descriptors.MolWt(mol)\n",
    "                    logp = Descriptors.MolLogP(mol)\n",
    "                    \n",
    "                    # Simple scoring function (can be replaced with learned predictor)\n",
    "                    score = -(abs(mw - target_property_value) / 100.0)  # Target molecular weight\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_z = z.copy()\n",
    "                    \n",
    "                    trajectory.append({\n",
    "                        'iteration': iteration,\n",
    "                        'smiles': smiles,\n",
    "                        'mw': mw,\n",
    "                        'logp': logp,\n",
    "                        'score': score\n",
    "                    })\n",
    "                else:\n",
    "                    score = -10  # Penalty for invalid molecules\n",
    "            except:\n",
    "                score = -10\n",
    "            \n",
    "            # Update latent vector (simple random walk with momentum)\n",
    "            if iteration > 0:\n",
    "                noise = np.random.randn(self.vae_model.latent_dim) * learning_rate\n",
    "                z = z + noise\n",
    "                \n",
    "                # Stay within reasonable bounds\n",
    "                z = np.clip(z, -3, 3)\n",
    "        \n",
    "        return best_z, trajectory\n",
    "\n",
    "# Property optimization example\n",
    "print(\"üéØ Property-Based Molecule Optimization:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "optimizer = PropertyOptimizer(vae_model, None)\n",
    "\n",
    "# Optimize for molecules with MW around 300\n",
    "target_mw = 300\n",
    "best_z, optimization_trajectory = optimizer.optimize_property(\n",
    "    target_mw, num_iterations=50, learning_rate=0.05\n",
    ")\n",
    "\n",
    "# Generate optimized molecules\n",
    "optimized_smiles = optimizer.decode_latent(best_z)\n",
    "\n",
    "print(f\"‚úÖ Target Molecular Weight: {target_mw}\")\n",
    "print(f\"‚úÖ Best Generated Molecule: {optimized_smiles}\")\n",
    "\n",
    "# Check if valid\n",
    "try:\n",
    "    mol = Chem.MolFromSmiles(optimized_smiles)\n",
    "    if mol is not None:\n",
    "        actual_mw = Descriptors.MolWt(mol)\n",
    "        actual_logp = Descriptors.MolLogP(mol)\n",
    "        print(f\"‚úÖ Actual MW: {actual_mw:.2f}\")\n",
    "        print(f\"‚úÖ LogP: {actual_logp:.2f}\")\n",
    "        print(f\"‚úÖ Molecule is valid!\")\n",
    "    else:\n",
    "        print(\"‚ùå Generated molecule is invalid\")\n",
    "except:\n",
    "    print(\"‚ùå Error processing molecule\")\n",
    "\n",
    "# Show optimization trajectory\n",
    "valid_trajectory = [t for t in optimization_trajectory if 'mw' in t]\n",
    "if valid_trajectory:\n",
    "    print(f\"\\nüìà Optimization Progress (showing last 10 valid molecules):\")\n",
    "    for t in valid_trajectory[-10:]:\n",
    "        print(f\"   Iter {t['iteration']:2d}: MW={t['mw']:6.2f}, Score={t['score']:6.3f}, SMILES={t['smiles'][:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0863a",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Integration & Benchmarking (0.5 hours)\n",
    "\n",
    "**Objective:** Compare all models and integrate advanced deep learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Section 4 Completion Assessment: Generative Models Implementation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SECTION 4 COMPLETION: Generative Models Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Generative Models section\n",
    "section4_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 4 Completion: Generative Models Implementation\",\n",
    "    concepts=[\n",
    "        \"Variational Autoencoders (VAEs) for molecular generation\",\n",
    "        \"Generative Adversarial Networks (GANs) for chemistry\",\n",
    "        \"Latent space representation of molecular properties\",\n",
    "        \"Reconstruction loss and KL divergence\",\n",
    "        \"Molecular validity and diversity metrics\",\n",
    "        \"Property-guided molecular optimization\",\n",
    "        \"Conditional generation and molecular design\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Molecular VAE implementation and training\",\n",
    "        \"Latent space exploration and sampling\",\n",
    "        \"Property optimization in latent space\",\n",
    "        \"Generated molecule validation analysis\",\n",
    "        \"Molecular diversity assessment\",\n",
    "        \"Conditional generation experiments\",\n",
    "        \"Model comparison and benchmarking\"\n",
    "    ],\n",
    "    time_target=60,  # 1 hour\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Section 4 Complete: Generative Models Mastery\")\n",
    "print(\"üöÄ Ready to advance to Section 5: Advanced Integration & Benchmarking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Benchmarking\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Comprehensive benchmarking for molecular deep learning models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = defaultdict(dict)\n",
    "        \n",
    "    def benchmark_model(self, model_name, model, test_loader, criterion, model_type='classification'):\n",
    "        \"\"\"Benchmark a model on test data\"\"\"\n",
    "        model.eval()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if model_type == 'graph':\n",
    "                    # Graph models\n",
    "                    batch_data = batch.to(device)\n",
    "                    batch_labels = batch.y.float()\n",
    "                    \n",
    "                    out = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "                    loss = criterion(out.squeeze(), batch_labels)\n",
    "                    \n",
    "                    pred = (out.squeeze() > 0.5).float()\n",
    "                    correct += (pred == batch_labels).sum().item()\n",
    "                    total += batch_labels.size(0)\n",
    "                    \n",
    "                    predictions.extend(pred.cpu().numpy())\n",
    "                    actuals.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                elif model_type == 'transformer':\n",
    "                    # Transformer models\n",
    "                    batch_data, batch_labels = batch\n",
    "                    batch_data = batch_data.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "                    \n",
    "                    padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "                    out = model(batch_data, padding_mask)\n",
    "                    loss = criterion(out.squeeze(), batch_labels)\n",
    "                    \n",
    "                    pred = (out.squeeze() > 0.5).float()\n",
    "                    correct += (pred == batch_labels).sum().item()\n",
    "                    total += batch_labels.size(0)\n",
    "                    \n",
    "                    predictions.extend(pred.cpu().numpy())\n",
    "                    actuals.extend(batch_labels.cpu().numpy())\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = correct / total\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        precision = precision_score(actuals, predictions, average='binary')\n",
    "        recall = recall_score(actuals, predictions, average='binary')\n",
    "        f1 = f1_score(actuals, predictions, average='binary')\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(actuals, predictions)\n",
    "        except:\n",
    "            auc = 0.0\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'loss': avg_loss,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'inference_time': inference_time,\n",
    "            'parameters': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "        \n",
    "        return self.results[model_name]\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print comprehensive model comparison\"\"\"\n",
    "        print(\"üèÜ Model Performance Comparison\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'Model':<15} {'Acc':<8} {'F1':<8} {'AUC':<8} {'Time':<8} {'Params':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Sort by F1 score\n",
    "        sorted_models = sorted(self.results.items(), key=lambda x: x[1]['f1_score'], reverse=True)\n",
    "        \n",
    "        for model_name, metrics in sorted_models:\n",
    "            print(f\"{model_name:<15} \"\n",
    "                  f\"{metrics['accuracy']:<8.4f} \"\n",
    "                  f\"{metrics['f1_score']:<8.4f} \"\n",
    "                  f\"{metrics['auc']:<8.4f} \"\n",
    "                  f\"{metrics['inference_time']:<8.2f} \"\n",
    "                  f\"{metrics['parameters']:<10,}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Best model summary\n",
    "        best_model = sorted_models[0]\n",
    "        print(f\"ü•á Best Model: {best_model[0]}\")\n",
    "        print(f\"   F1 Score: {best_model[1]['f1_score']:.4f}\")\n",
    "        print(f\"   Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
    "        print(f\"   Parameters: {best_model[1]['parameters']:,}\")\n",
    "\n",
    "# Initialize benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "\n",
    "print(\"üî¨ Benchmarking All Models:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Benchmark GCN\n",
    "gcn_results = benchmark.benchmark_model(\n",
    "    'GCN', model_gcn, test_loader, criterion, 'graph'\n",
    ")\n",
    "print(f\"‚úÖ GCN benchmarked: F1={gcn_results['f1_score']:.4f}\")\n",
    "\n",
    "# Benchmark GAT\n",
    "gat_results = benchmark.benchmark_model(\n",
    "    'GAT', model_gat, test_loader, criterion, 'graph'\n",
    ")\n",
    "print(f\"‚úÖ GAT benchmarked: F1={gat_results['f1_score']:.4f}\")\n",
    "\n",
    "# Benchmark Transformer\n",
    "transformer_results = benchmark.benchmark_model(\n",
    "    'Transformer', model_transformer, test_loader_transformer, criterion, 'transformer'\n",
    ")\n",
    "print(f\"‚úÖ Transformer benchmarked: F1={transformer_results['f1_score']:.4f}\")\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\")\n",
    "benchmark.print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Integration: Ensemble Methods\n",
    "class EnsemblePredictor:\n",
    "    \"\"\"Ensemble different model types for improved performance\"\"\"\n",
    "    \n",
    "    def __init__(self, models_info):\n",
    "        \"\"\"\n",
    "        models_info: list of dicts with 'model', 'type', 'weight' keys\n",
    "        \"\"\"\n",
    "        self.models_info = models_info\n",
    "        \n",
    "    def predict(self, graph_data, transformer_data):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        predictions = []\n",
    "        weights = []\n",
    "        \n",
    "        for model_info in self.models_info:\n",
    "            model = model_info['model']\n",
    "            model_type = model_info['type']\n",
    "            weight = model_info['weight']\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if model_type == 'graph':\n",
    "                    out = model(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "                    pred = torch.sigmoid(out.squeeze()).cpu().numpy()\n",
    "                elif model_type == 'transformer':\n",
    "                    padding_mask = (transformer_data == char_to_idx['<PAD>'])\n",
    "                    out = model(transformer_data, padding_mask)\n",
    "                    pred = torch.sigmoid(out.squeeze()).cpu().numpy()\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                weights.append(weight)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        return ensemble_pred\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_models = [\n",
    "    {'model': model_gcn, 'type': 'graph', 'weight': 0.3},\n",
    "    {'model': model_gat, 'type': 'graph', 'weight': 0.4},\n",
    "    {'model': model_transformer, 'type': 'transformer', 'weight': 0.3}\n",
    "]\n",
    "\n",
    "ensemble = EnsemblePredictor(ensemble_models)\n",
    "\n",
    "print(\"üéº Ensemble Model Integration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test ensemble on a few samples\n",
    "test_batch_graph = next(iter(test_loader))\n",
    "test_batch_transformer = next(iter(test_loader_transformer))\n",
    "\n",
    "ensemble_preds = ensemble.predict(test_batch_graph.to(device), test_batch_transformer[0].to(device))\n",
    "\n",
    "print(f\"‚úÖ Ensemble predictions generated for {len(ensemble_preds)} samples\")\n",
    "print(f\"‚úÖ Sample predictions: {ensemble_preds[:5]}\")\n",
    "\n",
    "# Compare with individual models\n",
    "actual_labels = test_batch_graph.y.cpu().numpy()\n",
    "ensemble_binary = (ensemble_preds > 0.5).astype(int)\n",
    "ensemble_accuracy = (ensemble_binary == actual_labels).mean()\n",
    "\n",
    "print(f\"‚úÖ Ensemble Accuracy: {ensemble_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd13e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 2 Project Summary and Portfolio Integration\n",
    "class Day2Portfolio:\n",
    "    \"\"\"Portfolio class for Day 2 achievements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models_trained = []\n",
    "        self.best_performances = {}\n",
    "        self.generated_molecules = []\n",
    "        \n",
    "    def add_model(self, name, performance):\n",
    "        self.models_trained.append(name)\n",
    "        self.best_performances[name] = performance\n",
    "        \n",
    "    def add_generated_molecules(self, molecules):\n",
    "        self.generated_molecules.extend(molecules)\n",
    "        \n",
    "    def generate_summary(self):\n",
    "        print(\"üìã Day 2 Project Portfolio Summary\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        print(\"üß† Models Implemented:\")\n",
    "        for i, model in enumerate(self.models_trained, 1):\n",
    "            perf = self.best_performances.get(model, {})\n",
    "            f1 = perf.get('f1_score', 0)\n",
    "            params = perf.get('parameters', 0)\n",
    "            print(f\"   {i}. {model} - F1: {f1:.4f}, Params: {params:,}\")\n",
    "        \n",
    "        print(f\"\\nüß™ Molecules Generated: {len(self.generated_molecules)}\")\n",
    "        if self.generated_molecules:\n",
    "            valid_count = sum(1 for mol in self.generated_molecules if 'INVALID' not in mol and 'ERROR' not in mol)\n",
    "            print(f\"   Valid Molecules: {valid_count} ({valid_count/len(self.generated_molecules)*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\nüéØ Key Achievements:\")\n",
    "        print(\"   ‚úÖ Mastered Graph Neural Networks (GCN)\")\n",
    "        print(\"   ‚úÖ Implemented Graph Attention Networks (GAT)\")\n",
    "        print(\"   ‚úÖ Built Molecular Transformers\")\n",
    "        print(\"   ‚úÖ Created Variational Autoencoder for molecule generation\")\n",
    "        print(\"   ‚úÖ Developed property optimization algorithms\")\n",
    "        print(\"   ‚úÖ Implemented ensemble methods\")\n",
    "        \n",
    "        print(\"\\nüîó Week 7-8 Readiness:\")\n",
    "        print(\"   ‚úÖ Advanced neural architectures ‚ûú Quantum chemistry methods\")\n",
    "        print(\"   ‚úÖ Generative models ‚ûú Virtual screening pipelines\")\n",
    "        print(\"   ‚úÖ Property optimization ‚ûú Drug discovery workflows\")\n",
    "\n",
    "# Create portfolio\n",
    "portfolio = Day2Portfolio()\n",
    "\n",
    "# Add models\n",
    "portfolio.add_model('Graph Convolutional Network', gcn_results)\n",
    "portfolio.add_model('Graph Attention Network', gat_results)\n",
    "portfolio.add_model('Molecular Transformer', transformer_results)\n",
    "portfolio.add_model('Molecular VAE', {'f1_score': 0.0, 'parameters': sum(p.numel() for p in vae_model.parameters())})\n",
    "\n",
    "# Add generated molecules\n",
    "portfolio.add_generated_molecules(generated_mols)\n",
    "\n",
    "# Generate summary\n",
    "portfolio.generate_summary()\n",
    "\n",
    "print(f\"\\nüéâ Day 2 Complete! Total Training Time: ~6 hours\")\n",
    "print(\"üìö Next: Day 3 - Molecular Docking & Virtual Screening\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ASSESSMENT CHECKPOINT 2.3: Graph Attention Networks (GATs) Mastery\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ASSESSMENT CHECKPOINT 2.3: Graph Attention Networks Mastery\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assessment.start_section(\"gat_mastery\")\n",
    "\n",
    "# GAT Assessment Questions\n",
    "gat_concepts = {\n",
    "    \"attention_mechanism\": {\n",
    "        \"question\": \"What is the key advantage of attention mechanisms in GATs over GCNs?\",\n",
    "        \"options\": [\n",
    "            \"a) Faster training speed\",\n",
    "            \"b) Different weights for different neighbors based on importance\",\n",
    "            \"c) Lower memory usage\",\n",
    "            \"d) Simpler implementation\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"GATs use attention to assign different weights to neighbors based on their importance, allowing for more flexible information aggregation.\"\n",
    "    },\n",
    "    \"multi_head_attention\": {\n",
    "        \"question\": \"Why do GATs typically use multi-head attention?\",\n",
    "        \"options\": [\n",
    "            \"a) To reduce computational cost\",\n",
    "            \"b) To capture different types of relationships simultaneously\",\n",
    "            \"c) To prevent overfitting\",\n",
    "            \"d) To increase model depth\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Multi-head attention allows GATs to focus on different aspects of neighbor relationships simultaneously.\"\n",
    "    },\n",
    "    \"attention_computation\": {\n",
    "        \"question\": \"In GAT attention computation, what determines the attention coefficients?\",\n",
    "        \"options\": [\n",
    "            \"a) Node degrees only\",\n",
    "            \"b) Edge features only\",\n",
    "            \"c) Learned compatibility function between node features\",\n",
    "            \"d) Random initialization\"\n",
    "        ],\n",
    "        \"correct\": \"c\",\n",
    "        \"explanation\": \"GAT attention coefficients are computed using a learned compatibility function that considers both source and target node features.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Present GAT assessment\n",
    "for concept, data in gat_concepts.items():\n",
    "    print(f\"\\nüìö {concept.replace('_', ' ').title()}:\")\n",
    "    print(f\"Q: {data['question']}\")\n",
    "    for option in data['options']:\n",
    "        print(f\"   {option}\")\n",
    "    \n",
    "    user_answer = input(\"\\nYour answer (a/b/c/d): \").lower().strip()\n",
    "    \n",
    "    if user_answer == data['correct']:\n",
    "        print(f\"‚úÖ Correct! {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"correct\", {\"score\": 1.0})\n",
    "    else:\n",
    "        print(f\"‚ùå Incorrect. {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"incorrect\", {\"score\": 0.0})\n",
    "\n",
    "# GAT Implementation Activity Assessment\n",
    "print(f\"\\nüõ†Ô∏è Hands-On: GAT Implementation Analysis\")\n",
    "print(\"Analyze your GAT implementation performance:\")\n",
    "\n",
    "if 'gat_results' in locals():\n",
    "    gat_performance = gat_results['f1_score']\n",
    "    print(f\"Your GAT F1-Score: {gat_performance:.4f}\")\n",
    "    \n",
    "    if gat_performance > 0.85:\n",
    "        print(\"üåü Excellent GAT implementation!\")\n",
    "        assessment.record_activity(\"gat_implementation\", \"excellent\", {\"score\": 1.0, \"f1_score\": gat_performance})\n",
    "    elif gat_performance > 0.75:\n",
    "        print(\"üëç Good GAT implementation!\")\n",
    "        assessment.record_activity(\"gat_implementation\", \"good\", {\"score\": 0.8, \"f1_score\": gat_performance})\n",
    "    else:\n",
    "        print(\"üìà GAT needs improvement - consider hyperparameter tuning\")\n",
    "        assessment.record_activity(\"gat_implementation\", \"needs_improvement\", {\"score\": 0.6, \"f1_score\": gat_performance})\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GAT implementation not found - please complete the exercise\")\n",
    "    assessment.record_activity(\"gat_implementation\", \"incomplete\", {\"score\": 0.0})\n",
    "\n",
    "assessment.end_section(\"gat_mastery\")\n",
    "\n",
    "# ASSESSMENT CHECKPOINT 2.4: Transformer Architectures\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ASSESSMENT CHECKPOINT 2.4: Molecular Transformers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assessment.start_section(\"transformer_architectures\")\n",
    "\n",
    "transformer_concepts = {\n",
    "    \"self_attention\": {\n",
    "        \"question\": \"What is the main advantage of self-attention in molecular transformers?\",\n",
    "        \"options\": [\n",
    "            \"a) Faster computation than RNNs\",\n",
    "            \"b) Parallel processing of all sequence positions\",\n",
    "            \"c) Lower memory requirements\",\n",
    "            \"d) Simpler architecture\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Self-attention allows transformers to process all positions in parallel, capturing long-range dependencies efficiently.\"\n",
    "    },\n",
    "    \"positional_encoding\": {\n",
    "        \"question\": \"Why is positional encoding crucial in molecular transformers?\",\n",
    "        \"options\": [\n",
    "            \"a) To reduce model complexity\",\n",
    "            \"b) To provide sequence order information since attention is permutation-invariant\",\n",
    "            \"c) To prevent overfitting\",\n",
    "            \"d) To increase model capacity\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"Since attention mechanisms are permutation-invariant, positional encodings provide crucial sequence order information for SMILES processing.\"\n",
    "    },\n",
    "    \"molecular_representation\": {\n",
    "        \"question\": \"How do molecular transformers typically handle SMILES sequences?\",\n",
    "        \"options\": [\n",
    "            \"a) Character-level tokenization only\",\n",
    "            \"b) Word-level tokenization only\",\n",
    "            \"c) Character or subword tokenization with special tokens\",\n",
    "            \"d) Image-based representation\"\n",
    "        ],\n",
    "        \"correct\": \"c\",\n",
    "        \"explanation\": \"Molecular transformers use character or subword tokenization with special tokens like START, END, and PAD for effective SMILES processing.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Present Transformer assessment\n",
    "for concept, data in transformer_concepts.items():\n",
    "    print(f\"\\nüìö {concept.replace('_', ' ').title()}:\")\n",
    "    print(f\"Q: {data['question']}\")\n",
    "    for option in data['options']:\n",
    "        print(f\"   {option}\")\n",
    "    \n",
    "    user_answer = input(\"\\nYour answer (a/b/c/d): \").lower().strip()\n",
    "    \n",
    "    if user_answer == data['correct']:\n",
    "        print(f\"‚úÖ Correct! {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"correct\", {\"score\": 1.0})\n",
    "    else:\n",
    "        print(f\"‚ùå Incorrect. {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"incorrect\", {\"score\": 0.0})\n",
    "\n",
    "# Transformer Implementation Assessment\n",
    "print(f\"\\nüõ†Ô∏è Hands-On: Transformer Implementation Analysis\")\n",
    "\n",
    "if 'transformer_results' in locals():\n",
    "    transformer_performance = transformer_results['f1_score']\n",
    "    print(f\"Your Transformer F1-Score: {transformer_performance:.4f}\")\n",
    "    \n",
    "    if transformer_performance > 0.85:\n",
    "        print(\"üåü Excellent Transformer implementation!\")\n",
    "        assessment.record_activity(\"transformer_implementation\", \"excellent\", {\"score\": 1.0, \"f1_score\": transformer_performance})\n",
    "    elif transformer_performance > 0.75:\n",
    "        print(\"üëç Good Transformer implementation!\")\n",
    "        assessment.record_activity(\"transformer_implementation\", \"good\", {\"score\": 0.8, \"f1_score\": transformer_performance})\n",
    "    else:\n",
    "        print(\"üìà Transformer needs improvement\")\n",
    "        assessment.record_activity(\"transformer_implementation\", \"needs_improvement\", {\"score\": 0.6, \"f1_score\": transformer_performance})\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Transformer implementation not found\")\n",
    "    assessment.record_activity(\"transformer_implementation\", \"incomplete\", {\"score\": 0.0})\n",
    "\n",
    "assessment.end_section(\"transformer_architectures\")\n",
    "\n",
    "# ASSESSMENT CHECKPOINT 2.5: Generative Models & VAEs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ASSESSMENT CHECKPOINT 2.5: Generative Models & VAEs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assessment.start_section(\"generative_models\")\n",
    "\n",
    "vae_concepts = {\n",
    "    \"latent_space\": {\n",
    "        \"question\": \"What is the purpose of the latent space in a Variational Autoencoder?\",\n",
    "        \"options\": [\n",
    "            \"a) To store training data\",\n",
    "            \"b) To provide a continuous, lower-dimensional representation for generation\",\n",
    "            \"c) To increase model complexity\",\n",
    "            \"d) To prevent overfitting\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"The latent space provides a continuous, compressed representation that enables smooth interpolation and generation of new molecules.\"\n",
    "    },\n",
    "    \"reparameterization\": {\n",
    "        \"question\": \"Why is the reparameterization trick necessary in VAEs?\",\n",
    "        \"options\": [\n",
    "            \"a) To reduce computational cost\",\n",
    "            \"b) To make the sampling operation differentiable for backpropagation\",\n",
    "            \"c) To increase model accuracy\",\n",
    "            \"d) To prevent mode collapse\"\n",
    "        ],\n",
    "        \"correct\": \"b\",\n",
    "        \"explanation\": \"The reparameterization trick makes the stochastic sampling operation differentiable, enabling gradient-based optimization.\"\n",
    "    },\n",
    "    \"molecular_validity\": {\n",
    "        \"question\": \"What is a key challenge when generating molecules with VAEs?\",\n",
    "        \"options\": [\n",
    "            \"a) Training speed\",\n",
    "            \"b) Memory usage\",\n",
    "            \"c) Ensuring chemical validity of generated SMILES\",\n",
    "            \"d) Model interpretability\"\n",
    "        ],\n",
    "        \"correct\": \"c\",\n",
    "        \"explanation\": \"A major challenge is ensuring that generated SMILES strings represent chemically valid molecules that can be synthesized.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Present VAE assessment\n",
    "for concept, data in vae_concepts.items():\n",
    "    print(f\"\\nüìö {concept.replace('_', ' ').title()}:\")\n",
    "    print(f\"Q: {data['question']}\")\n",
    "    for option in data['options']:\n",
    "        print(f\"   {option}\")\n",
    "    \n",
    "    user_answer = input(\"\\nYour answer (a/b/c/d): \").lower().strip()\n",
    "    \n",
    "    if user_answer == data['correct']:\n",
    "        print(f\"‚úÖ Correct! {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"correct\", {\"score\": 1.0})\n",
    "    else:\n",
    "        print(f\"‚ùå Incorrect. {data['explanation']}\")\n",
    "        assessment.record_activity(concept, \"incorrect\", {\"score\": 0.0})\n",
    "\n",
    "# VAE Generation Assessment\n",
    "print(f\"\\nüõ†Ô∏è Hands-On: Molecule Generation Analysis\")\n",
    "\n",
    "if 'generated_mols' in locals() and 'validity_rate' in locals():\n",
    "    print(f\"Molecules Generated: {len(generated_mols)}\")\n",
    "    print(f\"Validity Rate: {validity_rate:.2%}\")\n",
    "    \n",
    "    if validity_rate > 0.7:\n",
    "        print(\"üåü Excellent generation quality!\")\n",
    "        assessment.record_activity(\"vae_generation\", \"excellent\", {\"score\": 1.0, \"validity_rate\": validity_rate})\n",
    "    elif validity_rate > 0.5:\n",
    "        print(\"üëç Good generation quality!\")\n",
    "        assessment.record_activity(\"vae_generation\", \"good\", {\"score\": 0.8, \"validity_rate\": validity_rate})\n",
    "    else:\n",
    "        print(\"üìà Generation quality needs improvement\")\n",
    "        assessment.record_activity(\"vae_generation\", \"needs_improvement\", {\"score\": 0.6, \"validity_rate\": validity_rate})\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Molecule generation not completed\")\n",
    "    assessment.record_activity(\"vae_generation\", \"incomplete\", {\"score\": 0.0})\n",
    "\n",
    "assessment.end_section(\"generative_models\")\n",
    "\n",
    "# FINAL DAY 2 COMPREHENSIVE ASSESSMENT\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ FINAL DAY 2 COMPREHENSIVE ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "assessment.start_section(\"day2_final_assessment\")\n",
    "\n",
    "# Calculate overall performance\n",
    "progress_summary = assessment.get_progress_summary()\n",
    "day2_score = progress_summary.get('overall_score', 0)\n",
    "\n",
    "print(f\"üìä Day 2 Overall Performance: {day2_score:.1%}\")\n",
    "\n",
    "# Performance breakdown\n",
    "section_scores = progress_summary.get('section_scores', {})\n",
    "print(f\"\\nüìà Section Performance Breakdown:\")\n",
    "for section, score in section_scores.items():\n",
    "    if section.startswith('day2_') or section in ['gnn_mastery', 'gat_mastery', 'transformer_architectures', 'generative_models']:\n",
    "        print(f\"   {section.replace('_', ' ').title()}: {score:.1%}\")\n",
    "\n",
    "# Model implementation summary\n",
    "implemented_models = []\n",
    "if 'gcn_results' in locals():\n",
    "    implemented_models.append(f\"GCN (F1: {gcn_results['f1_score']:.3f})\")\n",
    "if 'gat_results' in locals():\n",
    "    implemented_models.append(f\"GAT (F1: {gat_results['f1_score']:.3f})\")\n",
    "if 'transformer_results' in locals():\n",
    "    implemented_models.append(f\"Transformer (F1: {transformer_results['f1_score']:.3f})\")\n",
    "\n",
    "print(f\"\\nüß† Models Successfully Implemented ({len(implemented_models)}/3):\")\n",
    "for model in implemented_models:\n",
    "    print(f\"   ‚úÖ {model}\")\n",
    "\n",
    "# Day 3 Readiness Assessment\n",
    "print(f\"\\nüéØ Day 3 Readiness Assessment:\")\n",
    "\n",
    "readiness_criteria = {\n",
    "    \"deep_learning_fundamentals\": day2_score > 0.7,\n",
    "    \"graph_neural_networks\": section_scores.get('gnn_mastery', 0) > 0.7,\n",
    "    \"attention_mechanisms\": section_scores.get('gat_mastery', 0) > 0.7,\n",
    "    \"sequence_models\": section_scores.get('transformer_architectures', 0) > 0.7,\n",
    "    \"generative_models\": section_scores.get('generative_models', 0) > 0.7\n",
    "}\n",
    "\n",
    "readiness_score = sum(readiness_criteria.values()) / len(readiness_criteria)\n",
    "\n",
    "for criterion, ready in readiness_criteria.items():\n",
    "    status = \"‚úÖ\" if ready else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {criterion.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\nüéñÔ∏è Overall Day 3 Readiness: {readiness_score:.1%}\")\n",
    "\n",
    "if readiness_score >= 0.8:\n",
    "    print(\"üåü Excellent! You're well-prepared for molecular docking and virtual screening!\")\n",
    "    assessment.record_activity(\"day3_readiness\", \"excellent\", {\"score\": 1.0, \"readiness\": readiness_score})\n",
    "elif readiness_score >= 0.6:\n",
    "    print(\"üëç Good preparation! Review any flagged areas before proceeding.\")\n",
    "    assessment.record_activity(\"day3_readiness\", \"good\", {\"score\": 0.8, \"readiness\": readiness_score})\n",
    "else:\n",
    "    print(\"üìö Consider reviewing Day 2 concepts before advancing to Day 3.\")\n",
    "    assessment.record_activity(\"day3_readiness\", \"needs_review\", {\"score\": 0.6, \"readiness\": readiness_score})\n",
    "\n",
    "# Save Day 2 assessment report\n",
    "final_report = assessment.get_comprehensive_report()\n",
    "assessment.save_final_report(\"day2_deep_learning_assessment\")\n",
    "\n",
    "print(f\"\\nüíæ Day 2 assessment report saved\")\n",
    "print(f\"üìÅ Report contains {len(final_report.get('activities', []))} assessed activities\")\n",
    "\n",
    "assessment.end_section(\"day2_final_assessment\")\n",
    "\n",
    "# Generate Day 2 Progress Dashboard\n",
    "try:\n",
    "    from assessment_framework import create_dashboard\n",
    "    dashboard = create_dashboard(assessment, \"Day 2: Deep Learning for Molecules\")\n",
    "    \n",
    "    print(f\"\\nüìä Generating Day 2 Progress Dashboard...\")\n",
    "    dashboard.generate_dashboard()\n",
    "    print(f\"‚úÖ Dashboard saved as 'day2_progress_dashboard.html'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Dashboard generation skipped: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ DAY 2 COMPLETE - DEEP LEARNING FOR MOLECULES MASTERED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ Next Adventure: Day 3 - Molecular Docking & Virtual Screening\")\n",
    "print(\"üìö You'll learn: AutoDock Vina, PyMOL visualization, binding affinity prediction\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635053bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Section 5 Completion Assessment: Advanced Integration & Benchmarking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SECTION 5 COMPLETION: Advanced Integration & Benchmarking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Advanced Integration section\n",
    "section5_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 5 Completion: Advanced Integration & Benchmarking\",\n",
    "    concepts=[\n",
    "        \"Model performance benchmarking and comparison\",\n",
    "        \"Ensemble methods for molecular prediction\",\n",
    "        \"Advanced integration techniques\",\n",
    "        \"Cross-model validation strategies\",\n",
    "        \"Performance optimization and tuning\",\n",
    "        \"Production deployment considerations\",\n",
    "        \"Model interpretability and explainability\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Comprehensive model benchmarking implementation\",\n",
    "        \"Ensemble predictor creation and testing\",\n",
    "        \"Performance metric calculation and analysis\",\n",
    "        \"Model comparison and selection\",\n",
    "        \"Integration testing and validation\",\n",
    "        \"Portfolio documentation and summarization\",\n",
    "        \"Production readiness assessment\"\n",
    "    ],\n",
    "    time_target=30,  # 0.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Section 5 Complete: Advanced Integration & Benchmarking Mastery\")\n",
    "print(\"üöÄ Ready for comprehensive Day 2 final assessment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
