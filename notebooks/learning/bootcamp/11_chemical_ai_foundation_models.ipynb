{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd6d4ed",
   "metadata": {},
   "source": [
    "# 🤖 **Bootcamp 09: Advanced Chemical AI & Foundation Models**\n",
    "\n",
    "## ChemML Advanced Specialization Series\n",
    "\n",
    "**Specialization Focus:** Next-generation chemical AI and foundation model applications  \n",
    "**Duration:** 12 hours of intensive foundation model mastery  \n",
    "**Level:** Principal Scientist / Research Director  \n",
    "**Prerequisites:** Completion of Bootcamps 01-08 + 3+ years experience\n",
    "\n",
    "### 🎯 **Learning Objectives:**\n",
    "1. **Master chemical foundation models** and large language models for chemistry\n",
    "2. **Implement multi-modal chemical AI** systems with vision-language integration\n",
    "3. **Design prompt engineering** strategies for chemical applications\n",
    "4. **Build generative AI workflows** for end-to-end chemical intelligence\n",
    "5. **Deploy production systems** using state-of-the-art chemical AI\n",
    "\n",
    "### 📋 **Today's Advanced Sections:**\n",
    "1. **Chemical Foundation Models & Transfer Learning** (4 hours)\n",
    "2. **Multi-Modal Chemical AI Systems** (4 hours) \n",
    "3. **Prompt Engineering & Generative Workflows** (4 hours)\n",
    "\n",
    "### 🏥 **Real-World Applications:**\n",
    "- **Literature Mining**: Automated chemical knowledge extraction from scientific papers\n",
    "- **Patent Analysis**: AI-powered intellectual property landscape analysis\n",
    "- **Synthesis Planning**: Natural language to synthetic route generation\n",
    "- **Chemical Communication**: AI-assisted scientific writing and documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c787dd6",
   "metadata": {},
   "source": [
    "## 🧠 **Section 1: Chemical Foundation Models & Transfer Learning (4 hours)**\n",
    "\n",
    "### 🎯 **Objectives:**\n",
    "- Master state-of-the-art chemical foundation models (ChemBERTa, MolT5, GPT-Chem)\n",
    "- Implement advanced transfer learning for chemical applications\n",
    "- Design domain-specific fine-tuning strategies\n",
    "- Build production-ready chemical language model pipelines\n",
    "\n",
    "### 📚 **Key Concepts:**\n",
    "- **Chemical Language Models:** Transformer architectures for molecular understanding\n",
    "- **Transfer Learning:** Pre-trained model adaptation for specific chemical tasks\n",
    "- **Fine-Tuning Strategies:** Task-specific optimization and domain adaptation\n",
    "- **Model Evaluation:** Comprehensive assessment of chemical AI performance\n",
    "- **Production Deployment:** Scalable inference and model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02841943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, pipeline\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import selfies as sf\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize progress tracking for advanced specialization\n",
    "from src.chemml.tutorials.assessment import AdvancedAssessmentTracker\n",
    "from src.chemml.tutorials.progress import SpecializationProgressTracker\n",
    "\n",
    "# Initialize specialized assessment for principal scientist level\n",
    "assessment = AdvancedAssessmentTracker(\n",
    "    bootcamp_id=\"09_chemical_ai_foundation_models\",\n",
    "    level=\"principal_scientist\",\n",
    "    specialization=\"chemical_ai\"\n",
    ")\n",
    "\n",
    "progress_tracker = SpecializationProgressTracker(\"Advanced Chemical AI & Foundation Models\")\n",
    "\n",
    "print(\"🤖 BOOTCAMP 09: ADVANCED CHEMICAL AI & FOUNDATION MODELS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"🎯 Section 1: Chemical Foundation Models & Transfer Learning\")\n",
    "print(\"⚡ Principal Scientist Level Specialization Training\")\n",
    "print(\"🧠 Next-Generation Chemical Intelligence Systems\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73505ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemicalFoundationModel:\n",
    "    \"\"\"\n",
    "    Advanced Chemical Foundation Model Framework\n",
    "    \n",
    "    This class implements state-of-the-art chemical foundation models\n",
    "    including ChemBERTa, MolT5, and custom chemical transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='chemberta'):\n",
    "        self.model_type = model_type\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Model configurations for different chemical foundation models\n",
    "        self.model_configs = {\n",
    "            'chemberta': {\n",
    "                'model_name': 'DeepChem/ChemBERTa-77M-MLM',\n",
    "                'tokenizer_name': 'DeepChem/ChemBERTa-77M-MLM',\n",
    "                'input_type': 'smiles',\n",
    "                'max_length': 512\n",
    "            },\n",
    "            'molt5': {\n",
    "                'model_name': 'laituan245/molt5-small', \n",
    "                'tokenizer_name': 'laituan245/molt5-small',\n",
    "                'input_type': 'selfies',\n",
    "                'max_length': 512\n",
    "            },\n",
    "            'custom_chem_gpt': {\n",
    "                'model_name': 'microsoft/DialoGPT-medium',  # Base model for custom training\n",
    "                'tokenizer_name': 'microsoft/DialoGPT-medium',\n",
    "                'input_type': 'smiles',\n",
    "                'max_length': 1024\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self._initialize_model()\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the selected foundation model\"\"\"\n",
    "        config = self.model_configs[self.model_type]\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                config['tokenizer_name'],\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Add padding token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                config['model_name'],\n",
    "                trust_remote_code=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.max_length = config['max_length']\n",
    "            self.input_type = config['input_type']\n",
    "            \n",
    "            print(f\"✅ Initialized {self.model_type} foundation model\")\n",
    "            print(f\"   Model size: {sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M parameters\")\n",
    "            print(f\"   Input type: {self.input_type}\")\n",
    "            print(f\"   Max length: {self.max_length}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load {self.model_type} model: {str(e)}\")\n",
    "            print(\"🔄 Falling back to simplified chemical transformer...\")\n",
    "            self._initialize_simple_model()\n",
    "    \n",
    "    def _initialize_simple_model(self):\n",
    "        \"\"\"Initialize a simple chemical transformer for demonstration\"\"\"\n",
    "        # Create a simple transformer model for chemical data\n",
    "        from transformers import BertConfig, BertModel, BertTokenizer\n",
    "        \n",
    "        # Simple BERT configuration for chemical data\n",
    "        config = BertConfig(\n",
    "            vocab_size=32000,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=6,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "            max_position_embeddings=512\n",
    "        )\n",
    "        \n",
    "        self.model = BertModel(config).to(self.device)\n",
    "        \n",
    "        # Create simple tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = 512\n",
    "        self.input_type = 'smiles'\n",
    "        \n",
    "        print(\"✅ Initialized simplified chemical transformer\")\n",
    "    \n",
    "    def preprocess_molecules(self, molecules: List[str]) -> List[str]:\n",
    "        \"\"\"Preprocess molecules for the selected model type\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for mol_str in molecules:\n",
    "            try:\n",
    "                if self.input_type == 'selfies':\n",
    "                    # Convert SMILES to SELFIES\n",
    "                    mol = Chem.MolFromSmiles(mol_str)\n",
    "                    if mol is not None:\n",
    "                        canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                        selfies_str = sf.encoder(canonical_smiles)\n",
    "                        processed.append(selfies_str)\n",
    "                    else:\n",
    "                        processed.append('[INVALID]')\n",
    "                else:\n",
    "                    # Use SMILES directly\n",
    "                    mol = Chem.MolFromSmiles(mol_str)\n",
    "                    if mol is not None:\n",
    "                        canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                        processed.append(canonical_smiles)\n",
    "                    else:\n",
    "                        processed.append('[INVALID]')\n",
    "            except Exception as e:\n",
    "                processed.append('[INVALID]')\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def encode_molecules(self, molecules: List[str], return_tensors=True):\n",
    "        \"\"\"Encode molecules using the foundation model\"\"\"\n",
    "        # Preprocess molecules\n",
    "        processed_mols = self.preprocess_molecules(molecules)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            processed_mols,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt' if return_tensors else None\n",
    "        )\n",
    "        \n",
    "        if return_tensors:\n",
    "            # Move to device\n",
    "            encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    def get_embeddings(self, molecules: List[str], layer=-1):\n",
    "        \"\"\"Get molecular embeddings from the foundation model\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode molecules\n",
    "            encoded = self.encode_molecules(molecules)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = self.model(**encoded, output_hidden_states=True)\n",
    "            \n",
    "            # Extract embeddings from specified layer\n",
    "            if hasattr(outputs, 'hidden_states'):\n",
    "                embeddings = outputs.hidden_states[layer]\n",
    "            else:\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Pool embeddings (mean pooling)\n",
    "            attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "            embeddings = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            return embeddings.cpu().numpy()\n",
    "    \n",
    "    def fine_tune_for_task(self, train_data, task_type='classification', num_labels=2):\n",
    "        \"\"\"Fine-tune the foundation model for a specific task\"\"\"\n",
    "        print(f\"🔧 Fine-tuning {self.model_type} for {task_type} task...\")\n",
    "        \n",
    "        # Create task-specific model\n",
    "        if task_type == 'classification':\n",
    "            model_class = AutoModelForSequenceClassification\n",
    "        else:\n",
    "            raise ValueError(f\"Task type {task_type} not supported\")\n",
    "        \n",
    "        # Load model for fine-tuning\n",
    "        config = self.model_configs[self.model_type]\n",
    "        \n",
    "        try:\n",
    "            fine_tune_model = model_class.from_pretrained(\n",
    "                config['model_name'],\n",
    "                num_labels=num_labels,\n",
    "                trust_remote_code=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            return fine_tune_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Fine-tuning setup failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Initialize chemical foundation models\n",
    "print(\"🚀 Initializing Chemical Foundation Models...\")\n",
    "print(\"\" * 50)\n",
    "\n",
    "# Try different foundation models\n",
    "models = {}\n",
    "model_types = ['chemberta', 'custom_chem_gpt']\n",
    "\n",
    "for model_type in model_types:\n",
    "    try:\n",
    "        print(f\"\\n📚 Loading {model_type.upper()} Foundation Model...\")\n",
    "        models[model_type] = ChemicalFoundationModel(model_type)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load {model_type}: {str(e)}\")\n",
    "\n",
    "# Use the successfully loaded model\n",
    "primary_model = list(models.values())[0] if models else None\n",
    "\n",
    "if primary_model:\n",
    "    print(f\"\\n✅ Primary foundation model ready: {primary_model.model_type}\")\n",
    "else:\n",
    "    print(\"⚠️ No foundation models could be loaded - using simplified demo\")\n",
    "\n",
    "assessment.start_section(\"Section 1: Chemical Foundation Models & Transfer Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce50a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Chemical Foundation Model Demonstration\n",
    "\n",
    "print(\"🧠 CHEMICAL FOUNDATION MODEL DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demo molecules for foundation model testing\n",
    "demo_molecules = [\n",
    "    \"CCO\",  # Ethanol\n",
    "    \"CC(=O)OC1=CC=CC=C1C(=O)O\",  # Aspirin\n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",  # Caffeine\n",
    "    \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\",  # Ibuprofen\n",
    "    \"CC1=C(C=C(C=C1)C(=O)NCCC(=O)O)NC(=O)C\",  # PABA derivative\n",
    "    \"C1=CC=C(C=C1)C(C(=O)O)N\",  # Phenylalanine\n",
    "    \"CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O\",  # Salbutamol\n",
    "    \"CN(C)CCOC1=CC=C(C=C1)C(C2=CC=CC=C2)C3=CC=CC=C3\"  # Diphenhydramine\n",
    "]\n",
    "\n",
    "demo_names = [\n",
    "    \"Ethanol\", \"Aspirin\", \"Caffeine\", \"Ibuprofen\", \n",
    "    \"PABA derivative\", \"Phenylalanine\", \"Salbutamol\", \"Diphenhydramine\"\n",
    "]\n",
    "\n",
    "if primary_model:\n",
    "    print(f\"\\n🔬 Processing {len(demo_molecules)} molecules with {primary_model.model_type}...\")\n",
    "    \n",
    "    # Get molecular embeddings\n",
    "    embeddings = primary_model.get_embeddings(demo_molecules)\n",
    "    \n",
    "    print(f\"\\n📊 Embedding Results:\")\n",
    "    print(f\"   Shape: {embeddings.shape}\")\n",
    "    print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "    print(f\"   Mean embedding magnitude: {np.mean(np.linalg.norm(embeddings, axis=1)):.3f}\")\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Visualize similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        similarity_matrix,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        xticklabels=demo_names,\n",
    "        yticklabels=demo_names,\n",
    "        cmap='viridis',\n",
    "        cbar_kws={'label': 'Cosine Similarity'}\n",
    "    )\n",
    "    plt.title(f'Molecular Similarity Matrix ({primary_model.model_type.upper()})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most similar pairs\n",
    "    print(\"\\n🔍 Most Similar Molecule Pairs:\")\n",
    "    \n",
    "    # Get upper triangle indices (excluding diagonal)\n",
    "    triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "    similarities = similarity_matrix[triu_indices]\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    for i in range(min(3, len(sorted_indices))):\n",
    "        idx = sorted_indices[i]\n",
    "        row, col = triu_indices[0][idx], triu_indices[1][idx]\n",
    "        similarity = similarities[idx]\n",
    "        print(f\"   {demo_names[row]} ↔ {demo_names[col]}: {similarity:.3f}\")\n",
    "    \n",
    "    assessment.record_activity(\"foundation_model_embeddings\", \"success\", {\n",
    "        \"model_type\": primary_model.model_type,\n",
    "        \"embedding_dim\": embeddings.shape[1],\n",
    "        \"molecules_processed\": len(demo_molecules),\n",
    "        \"mean_similarity\": float(np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]))\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping foundation model demonstration - no models available\")\n",
    "    \n",
    "    # Create dummy embeddings for demonstration\n",
    "    embeddings = np.random.randn(len(demo_molecules), 768)\n",
    "    \n",
    "    assessment.record_activity(\"foundation_model_embeddings\", \"demo_mode\", {\n",
    "        \"note\": \"Using simulated embeddings for demonstration\"\n",
    "    })\n",
    "\n",
    "print(\"\\n✅ Foundation model demonstration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Transfer Learning for Chemical Property Prediction\n",
    "\n",
    "print(\"\\n🔄 TRANSFER LEARNING FOR CHEMICAL PROPERTY PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ChemicalPropertyDataset(Dataset):\n",
    "    \"\"\"Dataset for chemical property prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, molecules, properties, tokenizer, max_length=512):\n",
    "        self.molecules = molecules\n",
    "        self.properties = properties\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.molecules)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        molecule = self.molecules[idx]\n",
    "        property_value = self.properties[idx]\n",
    "        \n",
    "        # Tokenize molecule\n",
    "        encoding = self.tokenizer(\n",
    "            molecule,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(property_value, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Generate synthetic chemical property data\n",
    "print(\"📊 Generating synthetic chemical property dataset...\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extended molecule dataset with known properties\n",
    "training_molecules = [\n",
    "    # High bioavailability molecules\n",
    "    \"CCO\", \"CC(=O)OC1=CC=CC=C1C(=O)O\", \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",\n",
    "    \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\", \"C1=CC=C(C=C1)C(C(=O)O)N\",\n",
    "    \"CC(C)(C)NCC(C1=CC(=C(C=C1)O)CO)O\", \"CN(C)CCC1=CNC2=C1C=C(C=C2)CS(=O)(=O)N\",\n",
    "    # Medium bioavailability molecules  \n",
    "    \"CN(C)CCOC1=CC=C(C=C1)C(C2=CC=CC=C2)C3=CC=CC=C3\", \"CC1=CC=C(C=C1)C(=O)O\",\n",
    "    \"COC1=CC=C(C=C1)CCN\", \"CC(C)CCNC(=O)C1=CC=C(C=C1)NS(=O)(=O)C\",\n",
    "    \"CN1CCC(CC1)OC(=O)C(C2=CC=CC=C2)C3=CC=CC=C3\", \"CC1=C(C(=NO1)C)C(=O)NC2=CC=CC=C2\",\n",
    "    # Low bioavailability molecules\n",
    "    \"CC(C)(C)C1=CC=C(C=C1)C(C)(C)C\", \"C1=CC=C2C(=C1)C=CC=C2C=CC=C3C=CC=CC3\",\n",
    "    \"CC(C)(C)C1=CC=C(C=C1)C(C)(C)C2=CC=C(C=C2)C(C)(C)C\", \"CCCCCCCCCCCCCCCC(=O)O\",\n",
    "    \"C1=CC=C(C=C1)C2=CC=C(C=C2)C3=CC=C(C=C3)C4=CC=CC=C4\", \"CCCCCCCCCCCCCCCCCCC(=O)O\"\n",
    "]\n",
    "\n",
    "# Assign bioavailability labels (0: low, 1: medium, 2: high)\n",
    "bioavailability_labels = [2]*7 + [1]*6 + [0]*6\n",
    "\n",
    "# Extend dataset with some noise\n",
    "extended_molecules = training_molecules * 5  # Repeat for larger dataset\n",
    "extended_labels = bioavailability_labels * 5\n",
    "\n",
    "# Add some random noise to labels for realism\n",
    "for i in range(len(extended_labels)):\n",
    "    if np.random.random() < 0.1:  # 10% label noise\n",
    "        extended_labels[i] = np.random.randint(0, 3)\n",
    "\n",
    "print(f\"📊 Dataset created:\")\n",
    "print(f\"   Total molecules: {len(extended_molecules)}\")\n",
    "print(f\"   Label distribution: {np.bincount(extended_labels)}\")\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_mols, test_mols, train_labels, test_labels = train_test_split(\n",
    "    extended_molecules, extended_labels, test_size=0.2, random_state=42, stratify=extended_labels\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 Data split:\")\n",
    "print(f\"   Training: {len(train_mols)} molecules\")\n",
    "print(f\"   Testing: {len(test_mols)} molecules\")\n",
    "\n",
    "if primary_model and primary_model.tokenizer:\n",
    "    # Create datasets\n",
    "    train_dataset = ChemicalPropertyDataset(\n",
    "        train_mols, train_labels, primary_model.tokenizer, max_length=256\n",
    "    )\n",
    "    test_dataset = ChemicalPropertyDataset(\n",
    "        test_mols, test_labels, primary_model.tokenizer, max_length=256\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    print(\"\\n✅ Transfer learning datasets ready!\")\n",
    "    \n",
    "    assessment.record_activity(\"transfer_learning_data\", \"prepared\", {\n",
    "        \"train_size\": len(train_mols),\n",
    "        \"test_size\": len(test_mols),\n",
    "        \"num_classes\": 3,\n",
    "        \"task\": \"bioavailability_prediction\"\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping transfer learning setup - no tokenizer available\")\n",
    "    assessment.record_activity(\"transfer_learning_data\", \"skipped\", {\n",
    "        \"reason\": \"No foundation model available\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Fine-Tuning Demonstration\n",
    "\n",
    "print(\"\\n🎯 FINE-TUNING CHEMICAL FOUNDATION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SimplifiedChemicalClassifier(nn.Module):\n",
    "    \"\"\"Simplified chemical classifier for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=32000, hidden_size=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embed tokens\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        # Concatenate forward and backward hidden states\n",
    "        final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(final_hidden)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "def train_chemical_classifier(model, train_loader, test_loader, num_epochs=3):\n",
    "    \"\"\"Train chemical property classifier\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    print(f\"🚀 Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        training_history['loss'].append(avg_loss)\n",
    "        training_history['accuracy'].append(accuracy)\n",
    "        \n",
    "        print(f\"   Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "            \n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = test_correct / test_total\n",
    "    \n",
    "    print(f\"\\n📊 Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return training_history, test_accuracy, all_predictions, all_labels\n",
    "\n",
    "if primary_model and 'train_loader' in locals():\n",
    "    print(\"🔧 Setting up fine-tuning experiment...\")\n",
    "    \n",
    "    # Try to use the foundation model for fine-tuning\n",
    "    fine_tuned_model = primary_model.fine_tune_for_task(\n",
    "        train_dataset, task_type='classification', num_labels=3\n",
    "    )\n",
    "    \n",
    "    if fine_tuned_model is None:\n",
    "        print(\"🔄 Using simplified classifier for demonstration...\")\n",
    "        # Use simplified model\n",
    "        fine_tuned_model = SimplifiedChemicalClassifier(num_classes=3)\n",
    "    \n",
    "    # Train the model\n",
    "    history, test_acc, predictions, true_labels = train_chemical_classifier(\n",
    "        fine_tuned_model, train_loader, test_loader, num_epochs=2\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history['loss'], 'b-', label='Training Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(history['accuracy'], 'g-', label='Training Accuracy')\n",
    "    ax2.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Accuracy ({test_acc:.3f})')\n",
    "    ax2.set_title('Training Progress')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Low', 'Medium', 'High'],\n",
    "                yticklabels=['Low', 'Medium', 'High'])\n",
    "    plt.title('Bioavailability Prediction Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions, \n",
    "                               target_names=['Low', 'Medium', 'High']))\n",
    "    \n",
    "    assessment.record_activity(\"fine_tuning_experiment\", \"completed\", {\n",
    "        \"test_accuracy\": float(test_acc),\n",
    "        \"final_training_loss\": float(history['loss'][-1]),\n",
    "        \"epochs_trained\": len(history['loss']),\n",
    "        \"task\": \"bioavailability_classification\"\n",
    "    })\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping fine-tuning experiment - requirements not met\")\n",
    "    \n",
    "    # Simulate results for demonstration\n",
    "    simulated_accuracy = 0.75 + np.random.random() * 0.15\n",
    "    print(f\"\\n📊 Simulated Fine-Tuning Results:\")\n",
    "    print(f\"   Test Accuracy: {simulated_accuracy:.4f}\")\n",
    "    print(f\"   Task: Bioavailability Classification (3 classes)\")\n",
    "    \n",
    "    assessment.record_activity(\"fine_tuning_experiment\", \"simulated\", {\n",
    "        \"simulated_accuracy\": float(simulated_accuracy)\n",
    "    })\n",
    "\n",
    "print(\"\\n✅ Fine-tuning demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0b10e",
   "metadata": {},
   "source": [
    "### 🎯 **Section 1 Summary: Chemical Foundation Models & Transfer Learning**\n",
    "\n",
    "In this section, you've mastered:\n",
    "\n",
    "#### **✅ Foundation Model Architecture**\n",
    "- **Chemical Language Models**: ChemBERTa, MolT5, and custom transformer architectures\n",
    "- **Molecular Representations**: SMILES, SELFIES, and tokenization strategies\n",
    "- **Embedding Generation**: High-dimensional molecular representations\n",
    "- **Similarity Analysis**: Chemical space exploration using foundation models\n",
    "\n",
    "#### **✅ Transfer Learning Expertise**\n",
    "- **Pre-trained Model Adaptation**: Fine-tuning for specific chemical tasks\n",
    "- **Task-Specific Optimization**: Classification and regression applications\n",
    "- **Training Strategies**: Effective learning rate scheduling and optimization\n",
    "- **Performance Evaluation**: Comprehensive model assessment methodologies\n",
    "\n",
    "#### **✅ Production Applications**\n",
    "- **Property Prediction**: AI-driven molecular property estimation\n",
    "- **Chemical Classification**: Automated molecule categorization\n",
    "- **Similarity Search**: Efficient chemical space navigation\n",
    "- **Model Deployment**: Production-ready inference systems\n",
    "\n",
    "**🚀 Next**: Advanced multi-modal chemical AI systems combining vision, language, and molecular data!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
