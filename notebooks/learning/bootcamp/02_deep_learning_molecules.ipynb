{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1184dab4",
   "metadata": {},
   "source": [
    "# Day 2 Project: Deep Learning for Molecules 🧠\n",
    "\n",
    "## Advanced Neural Architectures - 6 Hours of Intensive Coding\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master Graph Neural Networks (GNNs) and Graph Attention Networks (GATs)\n",
    "- Implement transformer architectures for molecular data\n",
    "- Build generative models for molecule generation\n",
    "- Compare advanced deep learning approaches\n",
    "\n",
    "**Skills Building Path:**\n",
    "- **Section 1:** Graph Neural Networks Mastery (1.5 hours)\n",
    "- **Section 2:** Graph Attention Networks (GATs) (1.5 hours)\n",
    "- **Section 3:** Transformer Architectures for Chemistry (1.5 hours)\n",
    "- **Section 4:** Generative Models Implementation (1 hour)\n",
    "- **Section 5:** Advanced Integration & Benchmarking (0.5 hours)\n",
    "\n",
    "**Cross-References:**\n",
    "- 🔗 **Day 1:** Builds on molecular representations and basic GNNs\n",
    "- 🔗 **Week 7 Checkpoint:** Quantum chemistry computational methods\n",
    "- 🔗 **Week 8 Checkpoint:** Advanced modeling and virtual screening\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6ff88",
   "metadata": {},
   "source": [
    "## Section 1: Graph Neural Networks Mastery (1.5 hours)\n",
    "\n",
    "**Objective:** Deep dive into GNN architectures and message passing frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7935a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Assessment Framework Setup\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from assessment_framework import BootcampAssessment, create_widget, create_dashboard\n",
    "    print(\"✅ Assessment framework loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Assessment framework not found - creating basic tracking\")\n",
    "    class BootcampAssessment:\n",
    "        def __init__(self, student_name, day):\n",
    "            self.student_name = student_name\n",
    "            self.day = day\n",
    "            self.activities = []\n",
    "        def record_activity(self, activity, data):\n",
    "            self.activities.append({\"activity\": activity, \"data\": data, \"timestamp\": datetime.now()})\n",
    "        def get_progress_summary(self):\n",
    "            return {\"overall_score\": 0.75, \"section_scores\": {}}\n",
    "    def create_widget(assessment, section, concepts, activities, time_target=90, section_type=\"assessment\"):\n",
    "        return type('MockWidget', (), {'display': lambda: print(f\"📋 {section} - Interactive assessment widget\")})()  \n",
    "\n",
    "# Initialize Assessment System\n",
    "student_name = input(\"👨‍🔬 Enter your name: \") or \"Student\"\n",
    "assessment = BootcampAssessment(student_name, \"Day 2\")\n",
    "\n",
    "print(f\"\\n🎆 Welcome {student_name} to Day 2: Deep Learning for Molecules!\")\n",
    "print(f\"📅 Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🎯 Target completion: 6 hours of intensive deep learning\")\n",
    "\n",
    "# Start Day 2 assessment tracking\n",
    "assessment.record_activity(\"day2_start\", {\n",
    "    \"day\": \"Day 2: Deep Learning for Molecules\",\n",
    "    \"start_time\": datetime.now().isoformat(),\n",
    "    \"target_duration_hours\": 6,\n",
    "    \"sections\": 5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0412052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Section 1 Assessment: Graph Neural Networks Mastery\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 SECTION 1 ASSESSMENT: Graph Neural Networks Mastery\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create assessment widget for GNN section\n",
    "section1_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 1: Graph Neural Networks Mastery\",\n",
    "    concepts=[\n",
    "        \"Graph representation of molecules\",\n",
    "        \"Message passing neural networks\", \n",
    "        \"GCN (Graph Convolutional Networks) architecture\",\n",
    "        \"Node and graph-level predictions\",\n",
    "        \"PyTorch Geometric framework usage\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Convert molecules to graph structures\",\n",
    "        \"Implement GCN layers for molecular property prediction\", \n",
    "        \"Train graph neural networks on chemical datasets\",\n",
    "        \"Compare GNN performance with traditional ML methods\",\n",
    "        \"Visualize learned molecular representations\"\n",
    "    ]\n",
    "    # Removed time_estimate parameter that was causing the error\n",
    ")\n",
    "\n",
    "# Display the widget using proper method call\n",
    "print(\"📋 Section 1 - Interactive assessment widget\")\n",
    "\n",
    "print(\"\\n🧠 Prerequisites Check:\")\n",
    "print(\"1. Day 1 molecular representations mastered?\")\n",
    "print(\"2. PyTorch basics understood?\")\n",
    "print(\"3. Graph theory concepts familiar?\")\n",
    "print(\"4. Ready for advanced deep learning architectures?\")\n",
    "\n",
    "# Record section start\n",
    "from datetime import datetime\n",
    "section1_start = datetime.now()\n",
    "assessment.record_activity(\"section1_start\", {\n",
    "    \"section\": \"GNN Mastery\",\n",
    "    \"start_time\": section1_start.isoformat(),\n",
    "    \"prerequisites_checked\": True,\n",
    "    \"target_time_minutes\": 90  # Record timing info in metadata instead\n",
    "})\n",
    "\n",
    "print(f\"\\n⏱️  Section 1 started: {section1_start.strftime('%H:%M:%S')}\")\n",
    "print(\"🎯 Target completion: 90 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced imports for deep learning on molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "# Suppress RDKit warnings\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Also suppress general warnings if needed\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "print(\"✅ RDKit warnings suppressed\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 Starting Day 2: Deep Learning for Molecules\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"💻 Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80faf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from your imports and setup...\n",
    "\n",
    "# Check GPU and additional setup\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"💻 Using CPU - some operations may be slower\")\n",
    "\n",
    "print(\"\\n✅ All libraries imported successfully!\")\n",
    "\n",
    "# Enhanced device info\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "try:\n",
    "    import torch_geometric\n",
    "    print(f\"🔧 PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "except:\n",
    "    print(\"⚠️ PyTorch Geometric version check failed\")\n",
    "\n",
    "print(f\"🔧 DeepChem version: {dc.__version__}\")\n",
    "print(f\"🔧 RDKit available: {Chem is not None}\")\n",
    "\n",
    "print(\"\\n🎆 Ready for advanced deep learning on molecular data!\")\n",
    "print(\"📚 Building on Day 1 foundations...\")\n",
    "print(\"🎯 Today's Focus: Advanced Neural Architectures\")\n",
    "\n",
    "# Quick system status\n",
    "print(f\"\\n📊 System Status:\")\n",
    "print(f\"   Random seeds set: PyTorch={torch.initial_seed()}, NumPy=42\")\n",
    "print(f\"   Memory available: {torch.cuda.is_available()}\")\n",
    "print(f\"   Ready for: GCNs, GATs, Transformers, VAEs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Hands-On Exercise 2.1: Molecular Graph Construction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🛠️ HANDS-ON EXERCISE 2.1: Molecular Graph Construction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def mol_to_graph(mol):\n",
    "    \"\"\"\n",
    "    Convert RDKit molecule to PyTorch Geometric graph\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Get atom features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = [\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetDegree(),\n",
    "            atom.GetFormalCharge(),\n",
    "            int(atom.GetHybridization()),\n",
    "            int(atom.GetIsAromatic())\n",
    "        ]\n",
    "        atom_features.append(features)\n",
    "    \n",
    "    # Get bond information (edges)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Add edge in both directions (undirected graph)\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        \n",
    "        # Bond features\n",
    "        bond_type = bond.GetBondType()\n",
    "        bond_features = [\n",
    "            float(bond_type == Chem.rdchem.BondType.SINGLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.DOUBLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.TRIPLE),\n",
    "            float(bond_type == Chem.rdchem.BondType.AROMATIC),\n",
    "            float(bond.GetIsConjugated())\n",
    "        ]\n",
    "        edge_features.extend([bond_features, bond_features])  # Both directions\n",
    "    \n",
    "    # Convert to tensors\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float) if edge_features else None\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Test with sample molecules\n",
    "test_molecules = {\n",
    "    'Benzene': 'c1ccccc1',\n",
    "    'Caffeine': 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',\n",
    "    'Aspirin': 'CC(=O)OC1=CC=CC=C1C(=O)O'\n",
    "}\n",
    "\n",
    "print(\"🧪 Converting molecules to graphs:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "mol_graphs = {}\n",
    "for name, smiles in test_molecules.items():\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    graph = mol_to_graph(mol)\n",
    "    mol_graphs[name] = graph\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Atoms: {graph.x.size(0)}\")\n",
    "    print(f\"  Bonds: {graph.edge_index.size(1)//2}\")\n",
    "    print(f\"  Node features: {graph.x.size(1)}\")\n",
    "    print()\n",
    "\n",
    "# Record exercise completion\n",
    "assessment.record_activity(\"exercise_2_1\", {\n",
    "    \"exercise\": \"Molecular Graph Construction\",\n",
    "    \"molecules_processed\": len(mol_graphs),\n",
    "    \"graph_features_implemented\": True,\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(\"✅ Molecular graph construction mastered!\")\n",
    "print(\"🚀 Ready to build Graph Neural Networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Hands-On Exercise 2.2: Graph Convolutional Network Implementation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🛠️ HANDS-ON EXERCISE 2.2: GCN Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class MolecularGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network for molecular property prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Graph-level prediction layers\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for mean+max pooling\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolutions with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Global pooling to get graph-level representation\n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        \n",
    "        # Concatenate different pooling strategies\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "print(\"🏮 Building Molecular GCN Model:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Determine input features from sample graph\n",
    "sample_graph = list(mol_graphs.values())[0]\n",
    "num_features = sample_graph.x.size(1)\n",
    "\n",
    "model_gcn_original = MolecularGCN(\n",
    "    num_features=num_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=1,  # For regression (e.g., solubility prediction)\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Input features: {num_features}\")\n",
    "print(f\"  Hidden dimension: 64\")\n",
    "print(f\"  Output classes: 1 (regression)\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model_gcn_original.parameters()):,}\")\n",
    "\n",
    "# Test forward pass with sample data\n",
    "with torch.no_grad():\n",
    "    sample_batch = torch.zeros(sample_graph.x.size(0), dtype=torch.long)\n",
    "    output = model_gcn_original(sample_graph.x.to(device), \n",
    "                  sample_graph.edge_index.to(device), \n",
    "                  sample_batch.to(device))\n",
    "    print(f\"  Sample output shape: {output.shape}\")\n",
    "\n",
    "# Record model implementation\n",
    "assessment.record_activity(\"exercise_2_2\", {\n",
    "    \"exercise\": \"GCN Implementation\",\n",
    "    \"model_parameters\": sum(p.numel() for p in model_gcn_original.parameters()),\n",
    "    \"architecture_layers\": 3,\n",
    "    \"pooling_strategies\": [\"mean\", \"max\"],\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(\"\\n✅ Graph Convolutional Network implemented successfully!\")\n",
    "print(\"🚀 Ready for training on molecular datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3909272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconcile model implementations for training\n",
    "# Ensure we have the proper model for training with batch interface\n",
    "\n",
    "# Always use the correct MolecularGCN class (from later in notebook)\n",
    "# This ensures we use the version with proper forward(self, x, edge_index, batch) signature\n",
    "num_features = train_pyg[0].x.shape[1] if 'train_pyg' in locals() and len(train_pyg) > 0 else 75\n",
    "\n",
    "# Define the correct MolecularGCN class locally to avoid conflicts\n",
    "class CorrectMolecularGCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=1, dropout=0.2):\n",
    "        super(CorrectMolecularGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//4, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.conv1(x, edge_index))\n",
    "        x1 = F.dropout(x1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x2, edge_index))\n",
    "        \n",
    "        # Global pooling\n",
    "        x_pooled = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x_pooled)\n",
    "        return out\n",
    "\n",
    "# Create the model with correct class\n",
    "model_gcn = CorrectMolecularGCN(num_features=num_features, hidden_dim=128).to(device)\n",
    "print(f\"✅ Created comprehensive GCN model with {num_features} input features\")\n",
    "\n",
    "# For consistency, ensure we have test variables from exercise 2.1\n",
    "if 'model_gcn_original' in locals():\n",
    "    print(f\"✅ Exercise 2.1 model available: {sum(p.numel() for p in model_gcn_original.parameters()):,} parameters\")\n",
    "\n",
    "print(f\"✅ Training-ready model available: {sum(p.numel() for p in model_gcn.parameters()):,} parameters\")\n",
    "print(f\"🎯 Ready to proceed with dataset loading and training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecular dataset and convert to PyTorch Geometric format\n",
    "print(\"📊 Preparing Molecular Graph Dataset:\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Fix SSL certificate issues for dataset download\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Create unverified SSL context for downloading\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "try:\n",
    "    # Load HIV dataset from DeepChem\n",
    "    tasks, datasets, transformers = dc.molnet.load_hiv(featurizer='GraphConv')\n",
    "    train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "    print(f\"✅ HIV Dataset loaded:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(valid_dataset)}\")\n",
    "    print(f\"   Test samples: {len(test_dataset)}\")\n",
    "    print(f\"   Task: {tasks[0]} (HIV replication inhibition)\")\n",
    "    \n",
    "    # Improved DeepChem ConvMol to PyTorch Geometric conversion\n",
    "    def improved_deepchem_to_pyg(dc_dataset, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Improved conversion function that properly handles DeepChem ConvMol format\n",
    "        \"\"\"\n",
    "        pyg_data_list = []\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(f\"Converting {min(len(dc_dataset), max_samples)} samples to PyG format...\")\n",
    "        print(\"Using improved ConvMol extraction method...\")\n",
    "        \n",
    "        for i in range(min(len(dc_dataset), max_samples)):\n",
    "            try:\n",
    "                # Get ConvMol object and label\n",
    "                conv_mol = dc_dataset.X[i]\n",
    "                label = dc_dataset.y[i]\n",
    "                \n",
    "                if conv_mol is None:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Extract features from ConvMol using its internal structure\n",
    "                # ConvMol has these key attributes: atom_features, bond_features, adjacency_list\n",
    "                \n",
    "                # Get atom features - this is the node feature matrix\n",
    "                if hasattr(conv_mol, 'atom_features'):\n",
    "                    atom_features = conv_mol.atom_features\n",
    "                    if atom_features is None or len(atom_features) == 0:\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to numpy array if needed\n",
    "                    if not isinstance(atom_features, np.ndarray):\n",
    "                        atom_features = np.array(atom_features)\n",
    "                    \n",
    "                    # Ensure 2D shape\n",
    "                    if len(atom_features.shape) == 1:\n",
    "                        atom_features = atom_features.reshape(1, -1)\n",
    "                    \n",
    "                    num_atoms = atom_features.shape[0]\n",
    "                    \n",
    "                    # Get adjacency information\n",
    "                    edge_list = []\n",
    "                    \n",
    "                    # Use the correct method to get adjacency list\n",
    "                    if hasattr(conv_mol, 'get_adjacency_list'):\n",
    "                        try:\n",
    "                            adj_list = conv_mol.get_adjacency_list()\n",
    "                            if adj_list is not None and len(adj_list) > 0:\n",
    "                                for atom_idx, neighbors in enumerate(adj_list):\n",
    "                                    for neighbor_idx in neighbors:\n",
    "                                        if 0 <= neighbor_idx < num_atoms:  # Validate indices\n",
    "                                            edge_list.append([atom_idx, neighbor_idx])\n",
    "                                            edge_list.append([neighbor_idx, atom_idx])  # Add reverse edge\n",
    "                        except:\n",
    "                            pass  # Fall back to creating simple connectivity\n",
    "                    \n",
    "                    # If no adjacency list or empty, create a minimal connected graph\n",
    "                    if not edge_list:\n",
    "                        if num_atoms == 1:\n",
    "                            # Self-loop for single atom\n",
    "                            edge_list = [[0, 0]]\n",
    "                        else:\n",
    "                            # Create a simple chain for multiple atoms\n",
    "                            for j in range(num_atoms - 1):\n",
    "                                edge_list.append([j, j + 1])\n",
    "                                edge_list.append([j + 1, j])\n",
    "                            # Add self-loops\n",
    "                            for j in range(num_atoms):\n",
    "                                edge_list.append([j, j])\n",
    "                    \n",
    "                    # Remove duplicates and convert to tensor\n",
    "                    edge_list = list(set(tuple(edge) for edge in edge_list))\n",
    "                    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                    \n",
    "                    # Process label\n",
    "                    if isinstance(label, (list, tuple, np.ndarray)):\n",
    "                        label_value = float(label[0]) if len(label) > 0 else 0.0\n",
    "                    else:\n",
    "                        label_value = float(label)\n",
    "                    \n",
    "                    # Create PyTorch Geometric Data object\n",
    "                    data = Data(\n",
    "                        x=torch.tensor(atom_features, dtype=torch.float),\n",
    "                        edge_index=edge_index,\n",
    "                        y=torch.tensor([label_value], dtype=torch.float)\n",
    "                    )\n",
    "                    \n",
    "                    # Validate the data object\n",
    "                    if data.x.size(0) > 0 and data.edge_index.size(1) > 0:\n",
    "                        pyg_data_list.append(data)\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                        \n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                if i < 5:  # Print first few errors for debugging\n",
    "                    print(f\"   Error processing sample {i}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        success_rate = len(pyg_data_list)/(len(pyg_data_list)+skipped_count)*100 if (len(pyg_data_list)+skipped_count) > 0 else 0\n",
    "        print(f\"\\n✅ Conversion complete:\")\n",
    "        print(f\"   Valid samples: {len(pyg_data_list)}\")\n",
    "        print(f\"   Skipped samples: {skipped_count}\")\n",
    "        print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        return pyg_data_list\n",
    "    \n",
    "    # Convert the datasets using the improved method\n",
    "    print(\"\\n🔧 Converting to PyTorch Geometric format with improved method...\")\n",
    "    \n",
    "    train_pyg = improved_deepchem_to_pyg(train_dataset, max_samples=1000)\n",
    "    valid_pyg = improved_deepchem_to_pyg(valid_dataset, max_samples=200)\n",
    "    test_pyg = improved_deepchem_to_pyg(test_dataset, max_samples=200)\n",
    "    \n",
    "    # Check conversion success\n",
    "    converted_successfully = len(train_pyg) > 0 and len(valid_pyg) > 0 and len(test_pyg) > 0\n",
    "    \n",
    "    if converted_successfully:\n",
    "        print(f\"\\n✅ Successfully converted to PyG format:\")\n",
    "        print(f\"   Train: {len(train_pyg)} graphs\")\n",
    "        print(f\"   Valid: {len(valid_pyg)} graphs\")\n",
    "        print(f\"   Test: {len(test_pyg)} graphs\")\n",
    "    else:\n",
    "        print(\"⚠️ Conversion failed, falling back to synthetic data\")\n",
    "        converted_successfully = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Dataset download failed: {e}\")\n",
    "    print(\"📝 Creating synthetic dataset for demonstration...\")\n",
    "    converted_successfully = False\n",
    "\n",
    "# Fallback to synthetic data if download failed OR conversion failed\n",
    "if not converted_successfully or 'converted_successfully' not in locals():\n",
    "    print(\"📝 Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Create synthetic molecular dataset as fallback\n",
    "    import random\n",
    "    from torch_geometric.data import Data\n",
    "    \n",
    "    def create_synthetic_dataset(size=100):\n",
    "        data_list = []\n",
    "        for i in range(size):\n",
    "            # Random graph structure\n",
    "            num_nodes = random.randint(5, 20)\n",
    "            num_edges = random.randint(4, num_nodes * 2)\n",
    "            \n",
    "            # Node features (similar to GraphConv featurizer)\n",
    "            x = torch.randn(num_nodes, 75)  # 75 features like GraphConv\n",
    "            \n",
    "            # Random edges\n",
    "            edge_index = torch.randint(0, num_nodes, (2, num_edges))\n",
    "            \n",
    "            # Random binary label\n",
    "            y = torch.tensor([random.randint(0, 1)], dtype=torch.float)\n",
    "            \n",
    "            data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        \n",
    "        return data_list\n",
    "    \n",
    "    # Create synthetic datasets\n",
    "    train_data = create_synthetic_dataset(80)\n",
    "    valid_data = create_synthetic_dataset(10)\n",
    "    test_data = create_synthetic_dataset(10)\n",
    "    \n",
    "    # Create mock dataset objects for compatibility\n",
    "    class MockDataset:\n",
    "        def __init__(self, data_list):\n",
    "            self.X = [d.x for d in data_list]\n",
    "            self.y = [[d.y.item()] for d in data_list]\n",
    "            self.data_list = data_list\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data_list)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.data_list[idx]\n",
    "    \n",
    "    train_dataset = MockDataset(train_data)\n",
    "    valid_dataset = MockDataset(valid_data)\n",
    "    test_dataset = MockDataset(test_data)\n",
    "    train_pyg = train_data\n",
    "    valid_pyg = valid_data\n",
    "    test_pyg = test_data\n",
    "    tasks = ['HIV_active']\n",
    "    \n",
    "    print(f\"✅ Synthetic Dataset created:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(valid_dataset)}\")\n",
    "    print(f\"   Test samples: {len(test_dataset)}\")\n",
    "    print(f\"   Task: {tasks[0]} (synthetic HIV replication inhibition)\")\n",
    "\n",
    "print(\"\\n🎆 Ready for advanced deep learning on molecular data!\")\n",
    "print(f\"💻 Computing device: {device}\")\n",
    "\n",
    "# Record dataset loading completion\n",
    "assessment.record_activity(\"dataset_loading\", {\n",
    "    \"dataset\": \"HIV\" if converted_successfully else \"synthetic\",\n",
    "    \"train_size\": len(train_dataset),\n",
    "    \"valid_size\": len(valid_dataset),\n",
    "    \"test_size\": len(test_dataset),\n",
    "    \"pyg_train_size\": len(train_pyg) if 'train_pyg' in locals() else 0,\n",
    "    \"pyg_valid_size\": len(valid_pyg) if 'valid_pyg' in locals() else 0,\n",
    "    \"pyg_test_size\": len(test_pyg) if 'test_pyg' in locals() else 0,\n",
    "    \"conversion_successful\": converted_successfully if 'converted_successfully' in locals() else False,\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(f\"🎯 Dataset ready for Graph Neural Network training!\")\n",
    "if 'train_pyg' in locals() and len(train_pyg) > 0:\n",
    "    sample_graph = train_pyg[0]\n",
    "    print(f\"📊 Sample Graph: {sample_graph.x.shape[0]} nodes, {sample_graph.x.shape[1]} features, {sample_graph.edge_index.shape[1]} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f49220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional data analysis and validation\n",
    "if 'train_pyg' in locals() and len(train_pyg) > 0:\n",
    "    print(\"🔍 Analyzing converted PyG datasets:\")\n",
    "    print(f\"   Train: {len(train_pyg)} graphs\")\n",
    "    print(f\"   Valid: {len(valid_pyg)} graphs\")\n",
    "    print(f\"   Test: {len(test_pyg)} graphs\")\n",
    "    \n",
    "    # Analyze graph structure\n",
    "    sample_graph = train_pyg[0]\n",
    "    print(f\"\\n📊 Sample Graph Analysis:\")\n",
    "    print(f\"   Nodes: {sample_graph.x.shape[0]}\")\n",
    "    print(f\"   Node features: {sample_graph.x.shape[1]}\")\n",
    "    print(f\"   Edges: {sample_graph.edge_index.shape[1]}\")\n",
    "    print(f\"   Label: {sample_graph.y.item()}\")\n",
    "    \n",
    "    # Validate that we have consistent feature dimensions\n",
    "    feature_dims = [graph.x.shape[1] for graph in train_pyg[:5]]\n",
    "    print(f\"   Feature dimensions (first 5): {feature_dims}\")\n",
    "    \n",
    "    if len(set(feature_dims)) == 1:\n",
    "        print(\"✅ All graphs have consistent feature dimensions\")\n",
    "    else:\n",
    "        print(\"⚠️ Warning: Inconsistent feature dimensions detected\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ No valid PyG data found - using synthetic data\")\n",
    "    # Use the synthetic data from the fallback\n",
    "    if 'train_data' in locals():\n",
    "        train_pyg = train_data\n",
    "        valid_pyg = valid_data  \n",
    "        test_pyg = test_data\n",
    "        print(\"✅ Using synthetic datasets for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Graph Convolutional Network\n",
    "class MolecularGCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGCN, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//4, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph convolutions with residual connections\n",
    "        x1 = F.relu(self.conv1(x, edge_index))\n",
    "        x1 = F.dropout(x1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x1, edge_index))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.conv3(x2, edge_index))\n",
    "        \n",
    "        # Global pooling\n",
    "        x_pooled = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x_pooled)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "num_features = train_pyg[0].x.shape[1]\n",
    "model_gcn = MolecularGCN(num_features=num_features, hidden_dim=128).to(device)\n",
    "\n",
    "print(f\"🧠 MolecularGCN Architecture:\")\n",
    "print(f\"   Input features: {num_features}\")\n",
    "print(f\"   Hidden dimension: 128\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_gcn.parameters()):,}\")\n",
    "print(f\"   Device: {next(model_gcn.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup and data loaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_pyg, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_pyg, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_pyg, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training configuration\n",
    "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"🏋️ Training Configuration:\")\n",
    "print(f\"   Batch size: 32\")\n",
    "print(f\"   Learning rate: 0.001\")\n",
    "print(f\"   Optimizer: Adam\")\n",
    "print(f\"   Loss function: Binary Cross Entropy\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for GCN\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = criterion(out, batch.y.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = (out > 0.5).float()\n",
    "        correct += (pred == batch.y.unsqueeze(1)).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(out, batch.y.unsqueeze(1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = (out > 0.5).float()\n",
    "            correct += (pred == batch.y.unsqueeze(1)).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Train the GCN model\n",
    "print(\"🚀 Training GCN Model:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, valid_losses = [], []\n",
    "train_accs, valid_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_gcn, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_gcn, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = evaluate(model_gcn, test_loader, criterion)\n",
    "print(f\"\\n✅ Final GCN Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Record Section 1 completion\n",
    "assessment.record_activity(\"section1_completion\", {\n",
    "    \"section\": \"Graph Neural Networks Mastery\",\n",
    "    \"model_accuracy\": test_acc,\n",
    "    \"model_loss\": test_loss,\n",
    "    \"completion_time\": datetime.now().isoformat()\n",
    "})\n",
    "\n",
    "print(f\"\\n🎉 Section 1 Complete: Graph Neural Networks Mastery!\")\n",
    "print(f\"✅ Successfully implemented molecular graph construction\")\n",
    "print(f\"✅ Built and trained Graph Convolutional Network\")\n",
    "print(f\"✅ Achieved test accuracy: {test_acc:.3f}\")\n",
    "print(f\"🚀 Ready to advance to Section 2: Graph Attention Networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e3aa1",
   "metadata": {},
   "source": [
    "## Section 2: Graph Attention Networks (GATs) (1.5 hours)\n",
    "\n",
    "**Objective:** Implement attention mechanisms for molecular graphs and compare with standard GCNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd25d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Network implementation\n",
    "class MolecularGAT(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, num_heads=4, num_classes=1, dropout=0.2):\n",
    "        super(MolecularGAT, self).__init__()\n",
    "        \n",
    "        # Graph attention layers\n",
    "        self.gat1 = GATConv(num_features, hidden_dim, heads=num_heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout)\n",
    "        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim//2, heads=1, dropout=dropout)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//4, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Multi-head attention layers\n",
    "        x1 = F.relu(self.gat1(x, edge_index))\n",
    "        x1 = F.dropout(x1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2 = F.relu(self.gat2(x1, edge_index))\n",
    "        x2 = F.dropout(x2, self.dropout, training=self.training)\n",
    "        \n",
    "        x3 = F.relu(self.gat3(x2, edge_index))\n",
    "        \n",
    "        # Global attention pooling\n",
    "        x_pooled = global_mean_pool(x3, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x_pooled)\n",
    "        return out\n",
    "\n",
    "# Initialize GAT model\n",
    "model_gat = MolecularGAT(\n",
    "    num_features=num_features, \n",
    "    hidden_dim=64, \n",
    "    num_heads=4,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"🧠 MolecularGAT Architecture:\")\n",
    "print(f\"   Input features: {num_features}\")\n",
    "print(f\"   Hidden dimension: 64\")\n",
    "print(f\"   Attention heads: 4\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_gat.parameters()):,}\")\n",
    "\n",
    "# Training setup for GAT\n",
    "optimizer_gat = torch.optim.Adam(model_gat.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97223fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT model\n",
    "print(\"🎯 Training GAT Model:\")\n",
    "print(\"=\" * 23)\n",
    "\n",
    "train_losses_gat, valid_losses_gat = [], []\n",
    "train_accs_gat, valid_accs_gat = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_gat, train_loader, optimizer_gat, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_gat, valid_loader, criterion)\n",
    "    \n",
    "    train_losses_gat.append(train_loss)\n",
    "    valid_losses_gat.append(valid_loss)\n",
    "    train_accs_gat.append(train_acc)\n",
    "    valid_accs_gat.append(valid_acc)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Evaluate GAT\n",
    "test_loss_gat, test_acc_gat = evaluate(model_gat, test_loader, criterion)\n",
    "print(f\"\\n✅ Final GAT Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc_gat:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss_gat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GCN vs GAT performance\n",
    "print(\"📊 GCN vs GAT Comparison:\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['GCN', 'GAT'],\n",
    "    'Test_Accuracy': [test_acc, test_acc_gat],\n",
    "    'Test_Loss': [test_loss, test_loss_gat],\n",
    "    'Parameters': [\n",
    "        sum(p.numel() for p in model_gcn.parameters()),\n",
    "        sum(p.numel() for p in model_gat.parameters())\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Training loss\n",
    "axes[0,0].plot(train_losses, label='GCN', linewidth=2)\n",
    "axes[0,0].plot(train_losses_gat, label='GAT', linewidth=2)\n",
    "axes[0,0].set_title('Training Loss')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[0,1].plot(valid_losses, label='GCN', linewidth=2)\n",
    "axes[0,1].plot(valid_losses_gat, label='GAT', linewidth=2)\n",
    "axes[0,1].set_title('Validation Loss')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "axes[1,0].plot(train_accs, label='GCN', linewidth=2)\n",
    "axes[1,0].plot(train_accs_gat, label='GAT', linewidth=2)\n",
    "axes[1,0].set_title('Training Accuracy')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1,1].plot(valid_accs, label='GCN', linewidth=2)\n",
    "axes[1,1].plot(valid_accs_gat, label='GAT', linewidth=2)\n",
    "axes[1,1].set_title('Validation Accuracy')\n",
    "axes[1,1].set_ylabel('Accuracy')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine better model\n",
    "better_model = 'GAT' if test_acc_gat > test_acc else 'GCN'\n",
    "improvement = abs(test_acc_gat - test_acc)\n",
    "print(f\"\\n🏆 Winner: {better_model}\")\n",
    "print(f\"   Improvement: {improvement:.4f} accuracy points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Section 2 Completion Assessment: Graph Attention Networks (GATs)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 SECTION 2 COMPLETION: Graph Attention Networks (GATs)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for GAT section\n",
    "section2_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 2 Completion: Graph Attention Networks (GATs)\",\n",
    "    concepts=[\n",
    "        \"Attention mechanisms in graph neural networks\",\n",
    "        \"Multi-head attention for molecular graphs\",\n",
    "        \"Graph pooling strategies\",\n",
    "        \"Edge features and node embeddings\",\n",
    "        \"Attention weight interpretation\",\n",
    "        \"GAT vs GCN performance comparison\",\n",
    "        \"Hyperparameter tuning for attention models\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"GAT architecture implementation\",\n",
    "        \"Multi-head attention configuration\",\n",
    "        \"Attention visualization analysis\",\n",
    "        \"Performance comparison with GCN\",\n",
    "        \"Edge analysis and graph clustering\",\n",
    "        \"Hyperparameter optimization\",\n",
    "        \"Attention weight interpretation\"\n",
    "    ],\n",
    "    time_target=90,  # 1.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Section 2 Complete: Graph Attention Networks Mastery\")\n",
    "print(\"🚀 Ready to advance to Section 3: Transformer Architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f990bfc",
   "metadata": {},
   "source": [
    "## Section 3: Transformer Architectures for Chemistry (1.5 hours)\n",
    "\n",
    "**Objective:** Implement transformer models for molecular sequence data and SMILES processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular Transformer for SMILES sequences\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class MolecularTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, \n",
    "                 max_length=128, num_classes=1, dropout=0.1):\n",
    "        super(MolecularTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self._generate_positional_encoding(max_length, d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _generate_positional_encoding(self, max_length, d_model):\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        batch_size, seq_length = x.shape\n",
    "        \n",
    "        # Token embedding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_length, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if padding_mask is not None:\n",
    "            # Mask out padded positions\n",
    "            mask = (~padding_mask).unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "print(\"🤖 Molecular Transformer Architecture Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889721ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES tokenization and vocabulary\n",
    "def tokenize_smiles(smiles_list):\n",
    "    \"\"\"Simple character-level tokenization for SMILES\"\"\"\n",
    "    # Define vocabulary\n",
    "    vocab = set()\n",
    "    for smiles in smiles_list:\n",
    "        for char in smiles:\n",
    "            vocab.add(char)\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab.update(['<PAD>', '<UNK>', '<START>', '<END>'])\n",
    "    \n",
    "    # Create mappings\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "def encode_smiles(smiles, char_to_idx, max_length=128):\n",
    "    \"\"\"Encode SMILES to token indices\"\"\"\n",
    "    tokens = [char_to_idx.get(char, char_to_idx['<UNK>']) for char in smiles]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([char_to_idx['<PAD>']] * (max_length - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Prepare SMILES data for transformer\n",
    "print(\"📝 Preparing SMILES Data for Transformer:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Get SMILES from DeepChem dataset (first 1000 samples)\n",
    "smiles_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i in range(min(1000, len(train_dataset))):\n",
    "    # Convert graph back to SMILES (simplified approach)\n",
    "    # In practice, you'd store original SMILES\n",
    "    smiles_list.append(f\"C{'C' * (i % 10)}O\")  # Simplified for demo\n",
    "    labels_list.append(train_dataset.y[i][0])\n",
    "\n",
    "# Create vocabulary\n",
    "char_to_idx, idx_to_char = tokenize_smiles(smiles_list)\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(f\"✅ Vocabulary created:\")\n",
    "print(f\"   Vocabulary size: {vocab_size}\")\n",
    "print(f\"   Sample characters: {list(char_to_idx.keys())[:10]}\")\n",
    "\n",
    "# Encode SMILES\n",
    "encoded_smiles = [encode_smiles(smi, char_to_idx) for smi in smiles_list]\n",
    "encoded_tensor = torch.tensor(encoded_smiles, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "\n",
    "print(f\"   Encoded tensor shape: {encoded_tensor.shape}\")\n",
    "print(f\"   Labels tensor shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ada2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Molecular Transformer\n",
    "model_transformer = MolecularTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    max_length=128,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"🤖 Molecular Transformer:\")\n",
    "print(f\"   Vocabulary: {vocab_size}\")\n",
    "print(f\"   Model dimension: 128\")\n",
    "print(f\"   Attention heads: 8\")\n",
    "print(f\"   Layers: 4\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_transformer.parameters()):,}\")\n",
    "\n",
    "# Create dataset and dataloader for transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Split data\n",
    "n_train = int(0.8 * len(encoded_tensor))\n",
    "n_valid = int(0.1 * len(encoded_tensor))\n",
    "\n",
    "train_data = TensorDataset(encoded_tensor[:n_train], labels_tensor[:n_train])\n",
    "valid_data = TensorDataset(encoded_tensor[n_train:n_train+n_valid], \n",
    "                          labels_tensor[n_train:n_train+n_valid])\n",
    "test_data = TensorDataset(encoded_tensor[n_train+n_valid:], \n",
    "                         labels_tensor[n_train+n_valid:])\n",
    "\n",
    "train_loader_transformer = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader_transformer = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "test_loader_transformer = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"📊 Transformer dataset splits:\")\n",
    "print(f\"   Train: {len(train_data)}\")\n",
    "print(f\"   Valid: {len(valid_data)}\")\n",
    "print(f\"   Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions for transformer\n",
    "def train_transformer_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_data, batch_labels in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Create padding mask\n",
    "        padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(batch_data, padding_mask)\n",
    "        loss = criterion(out.squeeze(), batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = (out.squeeze() > 0.5).float()\n",
    "        correct += (pred == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate_transformer(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "            \n",
    "            out = model(batch_data, padding_mask)\n",
    "            loss = criterion(out.squeeze(), batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = (out.squeeze() > 0.5).float()\n",
    "            correct += (pred == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# Train transformer\n",
    "optimizer_transformer = torch.optim.Adam(model_transformer.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"🚀 Training Molecular Transformer:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "num_epochs_transformer = 15\n",
    "for epoch in range(num_epochs_transformer):\n",
    "    train_loss, train_acc = train_transformer_epoch(\n",
    "        model_transformer, train_loader_transformer, optimizer_transformer, criterion\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate_transformer(\n",
    "        model_transformer, valid_loader_transformer, criterion\n",
    "    )\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "              f\"Valid Loss={valid_loss:.4f}, Acc={valid_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss_transformer, test_acc_transformer = evaluate_transformer(\n",
    "    model_transformer, test_loader_transformer, criterion\n",
    ")\n",
    "print(f\"\\n✅ Transformer Results:\")\n",
    "print(f\"   Test Accuracy: {test_acc_transformer:.4f}\")\n",
    "print(f\"   Test Loss: {test_loss_transformer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ce9f8",
   "metadata": {},
   "source": [
    "## Section 4: Generative Models Implementation (1 hour)\n",
    "\n",
    "**Objective:** Build generative models for novel molecule creation using VAEs and GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Section 3 Completion Assessment: Transformer Architectures for Chemistry\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 SECTION 3 COMPLETION: Transformer Architectures for Chemistry\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Transformer section\n",
    "section3_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 3 Completion: Transformer Architectures for Chemistry\",\n",
    "    concepts=[\n",
    "        \"Self-attention mechanisms for molecular sequences\",\n",
    "        \"Positional encoding for SMILES data\",\n",
    "        \"Transformer encoder-decoder architectures\",\n",
    "        \"Multi-head attention for chemical understanding\",\n",
    "        \"Molecular sequence processing and tokenization\",\n",
    "        \"BERT-style pre-training for chemistry\",\n",
    "        \"Fine-tuning transformers for molecular property prediction\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Molecular transformer implementation\",\n",
    "        \"SMILES sequence encoding and processing\",\n",
    "        \"Multi-head attention configuration\",\n",
    "        \"Positional encoding integration\",\n",
    "        \"Model training and optimization\",\n",
    "        \"Performance evaluation vs graph models\",\n",
    "        \"Sequence generation and analysis\"\n",
    "    ],\n",
    "    time_target=90,  # 1.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Section 3 Complete: Transformer Architectures Mastery\")\n",
    "print(\"🚀 Ready to advance to Section 4: Generative Models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc72c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Models for Molecule Generation\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MolecularVAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for SMILES generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, latent_dim=128, max_length=128):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        output, (hidden, _) = self.encoder_lstm(embedded)\n",
    "        # Take the last hidden state from both directions\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, target_seq=None):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        hidden = self.decoder_input(z).unsqueeze(0)  # [1, batch_size, hidden_dim]\n",
    "        cell = torch.zeros_like(hidden)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        if target_seq is not None:\n",
    "            # Training mode - teacher forcing\n",
    "            target_embedded = self.embedding(target_seq)  # [batch_size, seq_len, embedding_dim]\n",
    "            \n",
    "            for i in range(target_seq.size(1)):\n",
    "                # Concatenate latent vector with current input\n",
    "                z_expanded = z.unsqueeze(1)  # [batch_size, 1, latent_dim]\n",
    "                decoder_input = torch.cat([target_embedded[:, i:i+1, :], z_expanded], dim=-1)\n",
    "                \n",
    "                output, (hidden, cell) = self.decoder_lstm(decoder_input, (hidden, cell))\n",
    "                output = self.output_layer(output.squeeze(1))\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            # Inference mode\n",
    "            current_input = torch.zeros(batch_size, 1, self.embedding_dim).to(z.device)\n",
    "            \n",
    "            for i in range(self.max_length):\n",
    "                z_expanded = z.unsqueeze(1)  # [batch_size, 1, latent_dim]\n",
    "                decoder_input = torch.cat([current_input, z_expanded], dim=-1)\n",
    "                \n",
    "                output, (hidden, cell) = self.decoder_lstm(decoder_input, (hidden, cell))\n",
    "                output = self.output_layer(output.squeeze(1))\n",
    "                outputs.append(output)\n",
    "                \n",
    "                # Use output as next input\n",
    "                next_token = torch.argmax(output, dim=-1, keepdim=True)\n",
    "                current_input = self.embedding(next_token)  # Remove extra .unsqueeze(1)\n",
    "                \n",
    "            return torch.stack(outputs, dim=1)\n",
    "    \n",
    "    def forward(self, x, target_seq=None):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, target_seq)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"VAE loss with KL divergence and reconstruction loss\"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.cross_entropy(recon_x.reshape(-1, recon_x.size(-1)), x.reshape(-1), reduction='mean')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "print(\"🧬 Building Molecular VAE for Generation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define max_length for SMILES sequences\n",
    "max_length = 128\n",
    "\n",
    "# Initialize VAE\n",
    "vae_model = MolecularVAE(\n",
    "    vocab_size=len(char_to_idx),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    latent_dim=64,\n",
    "    max_length=max_length\n",
    ").to(device)\n",
    "\n",
    "print(f\"✅ VAE Model created with {sum(p.numel() for p in vae_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "def train_vae_epoch(model, loader, optimizer, beta=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    \n",
    "    for batch_data, _ in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, logvar = model(batch_data, batch_data[:, :-1])\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, recon_loss, kl_loss = vae_loss_function(\n",
    "            recon_batch, batch_data[:, 1:], mu, logvar, beta\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_loss += kl_loss.item()\n",
    "    \n",
    "    return (total_loss / len(loader), \n",
    "            total_recon_loss / len(loader), \n",
    "            total_kl_loss / len(loader))\n",
    "\n",
    "# Train VAE\n",
    "optimizer_vae = torch.optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"🚀 Training Molecular VAE:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "num_epochs_vae = 5\n",
    "beta_schedule = [min(1.0, i * 0.1) for i in range(num_epochs_vae)]  # Beta annealing\n",
    "\n",
    "for epoch in range(num_epochs_vae):\n",
    "    beta = beta_schedule[epoch]\n",
    "    \n",
    "    total_loss, recon_loss, kl_loss = train_vae_epoch(\n",
    "        vae_model, train_loader_transformer, optimizer_vae, beta\n",
    "    )\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} (β={beta:.1f}): Loss={total_loss:.4f}, \"\n",
    "              f\"Recon={recon_loss:.4f}, KL={kl_loss:.4f}\")\n",
    "\n",
    "print(\"✅ VAE Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6342eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecule Generation with VAE\n",
    "def generate_molecules(model, num_samples=10, temperature=1.0):\n",
    "    \"\"\"Generate novel molecules using trained VAE\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    generated_smiles = []\n",
    "    valid_molecules = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Sample from latent space\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device) * temperature\n",
    "        \n",
    "        # Decode to SMILES\n",
    "        outputs = model.decode(z)  # [num_samples, max_length, vocab_size]\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Convert logits to tokens\n",
    "            # Handle different tensor shapes - outputs might be 3D [batch, seq, vocab] or 4D\n",
    "            if len(outputs.shape) == 4:\n",
    "                # If 4D, take the batch dimension\n",
    "                sample_output = outputs[i].squeeze()\n",
    "            else:\n",
    "                # If 3D, directly index\n",
    "                sample_output = outputs[i]\n",
    "            \n",
    "            tokens = torch.argmax(sample_output, dim=-1).cpu().numpy()\n",
    "            \n",
    "            # Convert tokens to SMILES\n",
    "            smiles = ''.join([idx_to_char[token] for token in tokens if token != char_to_idx['<PAD>']])\n",
    "            smiles = smiles.replace('<START>', '').replace('<END>', '')\n",
    "            \n",
    "            # Validate molecule\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    valid_molecules += 1\n",
    "                    canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                    generated_smiles.append(canonical_smiles)\n",
    "                else:\n",
    "                    generated_smiles.append(smiles + \" (INVALID)\")\n",
    "            except:\n",
    "                generated_smiles.append(smiles + \" (ERROR)\")\n",
    "    \n",
    "    return generated_smiles, valid_molecules / num_samples\n",
    "\n",
    "# Generate novel molecules\n",
    "print(\"🧪 Generating Novel Molecules with VAE:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "generated_mols, validity_rate = generate_molecules(vae_model, num_samples=20, temperature=0.8)\n",
    "\n",
    "print(f\"✅ Generated {len(generated_mols)} molecules\")\n",
    "print(f\"✅ Validity Rate: {validity_rate:.2%}\")\n",
    "print(\"\\n📋 Sample Generated Molecules:\")\n",
    "for i, smiles in enumerate(generated_mols[:10]):\n",
    "    print(f\"   {i+1:2d}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular Property Optimization using VAE\n",
    "class PropertyOptimizer:\n",
    "    \"\"\"Optimize molecules for specific properties using VAE latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, vae_model, property_predictor):\n",
    "        self.vae_model = vae_model\n",
    "        self.property_predictor = property_predictor\n",
    "        \n",
    "    def encode_molecule(self, smiles):\n",
    "        \"\"\"Encode SMILES to latent vector\"\"\"\n",
    "        tokens = self.smiles_to_tokens(smiles)\n",
    "        tokens_tensor = torch.tensor([tokens]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.vae_model.encode(tokens_tensor)\n",
    "            z = self.vae_model.reparameterize(mu, logvar)\n",
    "        \n",
    "        return z.cpu().numpy()[0]\n",
    "    \n",
    "    def decode_latent(self, z):\n",
    "        \"\"\"Decode latent vector to SMILES\"\"\"\n",
    "        z_tensor = torch.tensor([z], dtype=torch.float32).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.vae_model.decode(z_tensor)\n",
    "            tokens = torch.argmax(outputs[0], dim=-1).cpu().numpy()\n",
    "        \n",
    "        smiles = ''.join([idx_to_char[token] for token in tokens if token != char_to_idx['<PAD>']])\n",
    "        return smiles.replace('<START>', '').replace('<END>', '')\n",
    "    \n",
    "    def smiles_to_tokens(self, smiles):\n",
    "        \"\"\"Convert SMILES to token sequence\"\"\"\n",
    "        smiles = '<START>' + smiles + '<END>'\n",
    "        tokens = [char_to_idx.get(c, char_to_idx['<UNK>']) for c in smiles]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokens) < max_length:\n",
    "            tokens.extend([char_to_idx['<PAD>']] * (max_length - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def optimize_property(self, target_property_value, num_iterations=100, learning_rate=0.1):\n",
    "        \"\"\"Optimize molecules for target property using gradient ascent in latent space\"\"\"\n",
    "        \n",
    "        # Start from random point in latent space\n",
    "        z = np.random.randn(self.vae_model.latent_dim) * 0.5\n",
    "        best_z = z.copy()\n",
    "        best_score = float('-inf')\n",
    "        \n",
    "        trajectory = []\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Generate molecule from current latent point\n",
    "            smiles = self.decode_latent(z)\n",
    "            \n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    # Calculate molecular properties\n",
    "                    mw = Descriptors.MolWt(mol)\n",
    "                    logp = Descriptors.MolLogP(mol)\n",
    "                    \n",
    "                    # Simple scoring function (can be replaced with learned predictor)\n",
    "                    score = -(abs(mw - target_property_value) / 100.0)  # Target molecular weight\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_z = z.copy()\n",
    "                    \n",
    "                    trajectory.append({\n",
    "                        'iteration': iteration,\n",
    "                        'smiles': smiles,\n",
    "                        'mw': mw,\n",
    "                        'logp': logp,\n",
    "                        'score': score\n",
    "                    })\n",
    "                else:\n",
    "                    score = -10  # Penalty for invalid molecules\n",
    "            except:\n",
    "                score = -10\n",
    "            \n",
    "            # Update latent vector (simple random walk with momentum)\n",
    "            if iteration > 0:\n",
    "                noise = np.random.randn(self.vae_model.latent_dim) * learning_rate\n",
    "                z = z + noise\n",
    "                \n",
    "                # Stay within reasonable bounds\n",
    "                z = np.clip(z, -3, 3)\n",
    "        \n",
    "        return best_z, trajectory\n",
    "\n",
    "# Property optimization example\n",
    "print(\"🎯 Property-Based Molecule Optimization:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "optimizer = PropertyOptimizer(vae_model, None)\n",
    "\n",
    "# Optimize for molecules with MW around 300\n",
    "target_mw = 300\n",
    "best_z, optimization_trajectory = optimizer.optimize_property(\n",
    "    target_mw, num_iterations=50, learning_rate=0.05\n",
    ")\n",
    "\n",
    "# Generate optimized molecules\n",
    "optimized_smiles = optimizer.decode_latent(best_z)\n",
    "\n",
    "print(f\"✅ Target Molecular Weight: {target_mw}\")\n",
    "print(f\"✅ Best Generated Molecule: {optimized_smiles}\")\n",
    "\n",
    "# Check if valid\n",
    "try:\n",
    "    mol = Chem.MolFromSmiles(optimized_smiles)\n",
    "    if mol is not None:\n",
    "        actual_mw = Descriptors.MolWt(mol)\n",
    "        actual_logp = Descriptors.MolLogP(mol)\n",
    "        print(f\"✅ Actual MW: {actual_mw:.2f}\")\n",
    "        print(f\"✅ LogP: {actual_logp:.2f}\")\n",
    "        print(f\"✅ Molecule is valid!\")\n",
    "    else:\n",
    "        print(\"❌ Generated molecule is invalid\")\n",
    "except:\n",
    "    print(\"❌ Error processing molecule\")\n",
    "\n",
    "# Show optimization trajectory\n",
    "valid_trajectory = [t for t in optimization_trajectory if 'mw' in t]\n",
    "if valid_trajectory:\n",
    "    print(f\"\\n📈 Optimization Progress (showing last 10 valid molecules):\")\n",
    "    for t in valid_trajectory[-10:]:\n",
    "        print(f\"   Iter {t['iteration']:2d}: MW={t['mw']:6.2f}, Score={t['score']:6.3f}, SMILES={t['smiles'][:30]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0863a",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Integration & Benchmarking (0.5 hours)\n",
    "\n",
    "**Objective:** Compare all models and integrate advanced deep learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Section 4 Completion Assessment: Generative Models Implementation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 SECTION 4 COMPLETION: Generative Models Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Generative Models section\n",
    "section4_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 4 Completion: Generative Models Implementation\",\n",
    "    concepts=[\n",
    "        \"Variational Autoencoders (VAEs) for molecular generation\",\n",
    "        \"Generative Adversarial Networks (GANs) for chemistry\",\n",
    "        \"Latent space representation of molecular properties\",\n",
    "        \"Reconstruction loss and KL divergence\",\n",
    "        \"Molecular validity and diversity metrics\",\n",
    "        \"Property-guided molecular optimization\",\n",
    "        \"Conditional generation and molecular design\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Molecular VAE implementation and training\",\n",
    "        \"Latent space exploration and sampling\",\n",
    "        \"Property optimization in latent space\",\n",
    "        \"Generated molecule validation analysis\",\n",
    "        \"Molecular diversity assessment\",\n",
    "        \"Conditional generation experiments\",\n",
    "        \"Model comparison and benchmarking\"\n",
    "    ],\n",
    "    time_target=60,  # 1 hour\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Section 4 Complete: Generative Models Mastery\")\n",
    "print(\"🚀 Ready to advance to Section 5: Advanced Integration & Benchmarking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Model Comparison and Benchmarking\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define loss criterion for benchmarking\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "class EnhancedModelBenchmark:\n",
    "    \"\"\"Comprehensive benchmarking for molecular deep learning models with statistical analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, num_runs: int = 3, confidence_level: float = 0.95):\n",
    "        self.results = defaultdict(list)  # Store multiple runs\n",
    "        self.num_runs = num_runs\n",
    "        self.confidence_level = confidence_level\n",
    "        self.summary_stats = {}\n",
    "        \n",
    "    def benchmark_model(self, model_name: str, model, test_loader, criterion, \n",
    "                       model_type: str = 'classification') -> Dict:\n",
    "        \"\"\"Benchmark a model multiple times for statistical reliability\"\"\"\n",
    "        print(f\"🔄 Running {self.num_runs} benchmark runs for {model_name}...\")\n",
    "        \n",
    "        run_results = []\n",
    "        \n",
    "        for run_idx in range(self.num_runs):\n",
    "            print(f\"  Run {run_idx + 1}/{self.num_runs}...\", end=\" \")\n",
    "            \n",
    "            try:\n",
    "                # Single run benchmark\n",
    "                run_result = self._single_benchmark_run(\n",
    "                    model, test_loader, criterion, model_type\n",
    "                )\n",
    "                run_results.append(run_result)\n",
    "                print(f\"✅ F1: {run_result['f1_score']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed: {str(e)[:50]}...\")\n",
    "                # Create default failed result\n",
    "                run_result = self._create_failed_result()\n",
    "                run_results.append(run_result)\n",
    "        \n",
    "        # Store all runs\n",
    "        self.results[model_name] = run_results\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = self._calculate_summary_statistics(model_name, run_results)\n",
    "        self.summary_stats[model_name] = summary\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _single_benchmark_run(self, model, test_loader, criterion, model_type: str) -> Dict:\n",
    "        \"\"\"Execute a single benchmark run\"\"\"\n",
    "        model.eval()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        batch_times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_loader):\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                try:\n",
    "                    if model_type == 'graph':\n",
    "                        # Graph models\n",
    "                        batch_data = batch.to(device)\n",
    "                        batch_labels = batch.y.float()\n",
    "\n",
    "                        out = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "                        loss = criterion(out.squeeze(), batch_labels)\n",
    "\n",
    "                        pred = (torch.sigmoid(out.squeeze()) > 0.5).float()\n",
    "                        correct += (pred == batch_labels).sum().item()\n",
    "                        total += batch_labels.size(0)\n",
    "\n",
    "                        predictions.extend(pred.cpu().numpy())\n",
    "                        actuals.extend(batch_labels.cpu().numpy())\n",
    "                        \n",
    "                    elif model_type == 'transformer':\n",
    "                        # Transformer models\n",
    "                        batch_data, batch_labels = batch\n",
    "                        batch_data = batch_data.to(device)\n",
    "                        batch_labels = batch_labels.to(device)\n",
    "\n",
    "                        padding_mask = (batch_data == char_to_idx['<PAD>'])\n",
    "                        out = model(batch_data, padding_mask)\n",
    "                        loss = criterion(out.squeeze(), batch_labels)\n",
    "\n",
    "                        pred = (torch.sigmoid(out.squeeze()) > 0.5).float()\n",
    "                        correct += (pred == batch_labels).sum().item()\n",
    "                        total += batch_labels.size(0)\n",
    "\n",
    "                        predictions.extend(pred.cpu().numpy())\n",
    "                        actuals.extend(batch_labels.cpu().numpy())\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n    ⚠️  Batch {batch_idx} failed: {str(e)[:30]}...\")\n",
    "                    continue\n",
    "                \n",
    "                batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics with error handling\n",
    "        try:\n",
    "            accuracy = correct / total if total > 0 else 0.0\n",
    "            avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else float('inf')\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "            \n",
    "            if len(set(actuals)) > 1 and len(actuals) > 0:  # Ensure we have both classes\n",
    "                precision = precision_score(actuals, predictions, average='binary', zero_division=0)\n",
    "                recall = recall_score(actuals, predictions, average='binary', zero_division=0)\n",
    "                f1 = f1_score(actuals, predictions, average='binary', zero_division=0)\n",
    "                \n",
    "                try:\n",
    "                    auc = roc_auc_score(actuals, predictions)\n",
    "                except:\n",
    "                    auc = 0.0\n",
    "            else:\n",
    "                precision = recall = f1 = auc = 0.0\n",
    "            \n",
    "            # Model analysis\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            model_size_mb = param_count * 4 / (1024 * 1024)  # Assuming float32\n",
    "            throughput = len(test_loader) / inference_time if inference_time > 0 else 0\n",
    "            avg_batch_time = np.mean(batch_times) if batch_times else 0\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'loss': avg_loss,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc': auc,\n",
    "                'inference_time': inference_time,\n",
    "                'parameters': param_count,\n",
    "                'model_size_mb': model_size_mb,\n",
    "                'throughput_batches_per_sec': throughput,\n",
    "                'avg_batch_time': avg_batch_time,\n",
    "                'total_samples': total,\n",
    "                'successful_batches': len(batch_times)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n    ❌ Metrics calculation failed: {str(e)}\")\n",
    "            return self._create_failed_result()\n",
    "    \n",
    "    def _create_failed_result(self) -> Dict:\n",
    "        \"\"\"Create a result dictionary for failed runs\"\"\"\n",
    "        return {\n",
    "            'accuracy': 0.0, 'loss': float('inf'), 'precision': 0.0,\n",
    "            'recall': 0.0, 'f1_score': 0.0, 'auc': 0.0,\n",
    "            'inference_time': float('inf'), 'parameters': 0,\n",
    "            'model_size_mb': 0.0, 'throughput_batches_per_sec': 0.0,\n",
    "            'avg_batch_time': float('inf'), 'total_samples': 0,\n",
    "            'successful_batches': 0\n",
    "        }\n",
    "    \n",
    "    def _calculate_summary_statistics(self, model_name: str, run_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate comprehensive summary statistics across multiple runs\"\"\"\n",
    "        if not run_results:\n",
    "            return {}\n",
    "        \n",
    "        # Extract metrics from all runs\n",
    "        metrics = {}\n",
    "        for key in run_results[0].keys():\n",
    "            values = [run[key] for run in run_results if not np.isinf(run[key])]\n",
    "            if values:\n",
    "                metrics[key] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values, ddof=1) if len(values) > 1 else 0.0,\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values),\n",
    "                    'median': np.median(values),\n",
    "                    'values': values\n",
    "                }\n",
    "                \n",
    "                # Calculate confidence interval\n",
    "                if len(values) > 1:\n",
    "                    confidence_interval = self._calculate_confidence_interval(values)\n",
    "                    metrics[key]['confidence_interval'] = confidence_interval\n",
    "                    metrics[key]['margin_of_error'] = confidence_interval[1] - metrics[key]['mean']\n",
    "                else:\n",
    "                    metrics[key]['confidence_interval'] = (metrics[key]['mean'], metrics[key]['mean'])\n",
    "                    metrics[key]['margin_of_error'] = 0.0\n",
    "            else:\n",
    "                # Handle case where all values are inf or invalid\n",
    "                metrics[key] = {\n",
    "                    'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0,\n",
    "                    'median': 0.0, 'values': [], 'confidence_interval': (0.0, 0.0),\n",
    "                    'margin_of_error': 0.0\n",
    "                }\n",
    "        \n",
    "        # Add derived metrics\n",
    "        f1_values = metrics['f1_score']['values']\n",
    "        if f1_values:\n",
    "            metrics['stability'] = 1.0 - (metrics['f1_score']['std'] / (metrics['f1_score']['mean'] + 1e-8))\n",
    "            metrics['consistency_score'] = 1.0 - (np.std(f1_values) / (np.mean(f1_values) + 1e-8))\n",
    "            metrics['efficiency'] = (metrics['f1_score']['mean'] * metrics['throughput_batches_per_sec']['mean']) / \\\n",
    "                                  (metrics['parameters']['mean'] / 1e6 + 1e-8)  # F1 * throughput / M_params\n",
    "        else:\n",
    "            metrics['stability'] = 0.0\n",
    "            metrics['consistency_score'] = 0.0\n",
    "            metrics['efficiency'] = 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_confidence_interval(self, values: List[float]) -> Tuple[float, float]:\n",
    "        \"\"\"Calculate confidence interval using t-distribution\"\"\"\n",
    "        if len(values) <= 1:\n",
    "            return (values[0], values[0]) if values else (0.0, 0.0)\n",
    "        \n",
    "        mean = np.mean(values)\n",
    "        std_err = stats.sem(values)  # Standard error of mean\n",
    "        dof = len(values) - 1  # Degrees of freedom\n",
    "        \n",
    "        # t-distribution critical value\n",
    "        alpha = 1 - self.confidence_level\n",
    "        t_critical = t.ppf(1 - alpha/2, dof)\n",
    "        \n",
    "        margin_error = t_critical * std_err\n",
    "        \n",
    "        return (mean - margin_error, mean + margin_error)\n",
    "    \n",
    "    def compare_models_statistically(self, model1: str, model2: str, metric: str = 'f1_score') -> Dict:\n",
    "        \"\"\"Perform statistical significance test between two models\"\"\"\n",
    "        if model1 not in self.summary_stats or model2 not in self.summary_stats:\n",
    "            return {'error': 'One or both models not found'}\n",
    "        \n",
    "        values1 = self.summary_stats[model1][metric]['values']\n",
    "        values2 = self.summary_stats[model2][metric]['values']\n",
    "        \n",
    "        if not values1 or not values2:\n",
    "            return {'error': 'Insufficient data for comparison'}\n",
    "        \n",
    "        # Paired t-test (assumes same test set)\n",
    "        try:\n",
    "            t_stat, p_value = stats.ttest_rel(values1, values2)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(values1) - 1) * np.var(values1, ddof=1) + \n",
    "                                (len(values2) - 1) * np.var(values2, ddof=1)) / \n",
    "                               (len(values1) + len(values2) - 2))\n",
    "            cohens_d = (np.mean(values1) - np.mean(values2)) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            # Interpretation\n",
    "            significant = p_value < 0.05\n",
    "            better_model = model1 if np.mean(values1) > np.mean(values2) else model2\n",
    "            \n",
    "            effect_size_interpretation = (\n",
    "                'large' if abs(cohens_d) >= 0.8 else \n",
    "                'medium' if abs(cohens_d) >= 0.5 else \n",
    "                'small' if abs(cohens_d) >= 0.2 else 'negligible'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'model1': model1, 'model2': model2, 'metric': metric,\n",
    "                'model1_mean': np.mean(values1), 'model2_mean': np.mean(values2),\n",
    "                't_statistic': t_stat, 'p_value': p_value,\n",
    "                'significant': significant, 'better_model': better_model,\n",
    "                'cohens_d': cohens_d, 'effect_size': effect_size_interpretation,\n",
    "                'difference': abs(np.mean(values1) - np.mean(values2))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': f'Statistical test failed: {str(e)}'}\n",
    "    \n",
    "    def print_comprehensive_comparison(self):\n",
    "        \"\"\"Print detailed model comparison with statistical insights\"\"\"\n",
    "        print(\"\\n🏆 COMPREHENSIVE MODEL PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not self.summary_stats:\n",
    "            print(\"❌ No benchmark results available\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for model_name, stats in self.summary_stats.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'F1_Mean': stats['f1_score']['mean'],\n",
    "                'F1_Std': stats['f1_score']['std'],\n",
    "                'F1_CI_Lower': stats['f1_score']['confidence_interval'][0],\n",
    "                'F1_CI_Upper': stats['f1_score']['confidence_interval'][1],\n",
    "                'Accuracy': stats['accuracy']['mean'],\n",
    "                'AUC': stats['auc']['mean'],\n",
    "                'Stability': stats['stability'],\n",
    "                'Efficiency': stats['efficiency'],\n",
    "                'Parameters_M': stats['parameters']['mean'] / 1e6,\n",
    "                'Size_MB': stats['model_size_mb']['mean'],\n",
    "                'Throughput': stats['throughput_batches_per_sec']['mean'],\n",
    "                'Inference_Time': stats['inference_time']['mean']\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df = df.sort_values('F1_Mean', ascending=False)\n",
    "        \n",
    "        # Print main results table\n",
    "        print(\"\\n📊 PERFORMANCE METRICS (with 95% Confidence Intervals)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Model':<12} {'F1 Score':<15} {'Accuracy':<10} {'AUC':<8} {'Stability':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            f1_display = f\"{row['F1_Mean']:.3f}±{row['F1_Std']:.3f}\"\n",
    "            print(f\"{row['Model']:<12} {f1_display:<15} \"\n",
    "                  f\"{row['Accuracy']:<10.3f} {row['AUC']:<8.3f} {row['Stability']:<10.3f}\")\n",
    "        \n",
    "        # Print efficiency and resource usage\n",
    "        print(\"\\n⚡ EFFICIENCY & RESOURCE USAGE\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Model':<12} {'Efficiency':<12} {'Params(M)':<12} {'Size(MB)':<12} {'Throughput':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            print(f\"{row['Model']:<12} {row['Efficiency']:<12.2f} \"\n",
    "                  f\"{row['Parameters_M']:<12.2f} {row['Size_MB']:<12.1f} {row['Throughput']:<12.2f}\")\n",
    "        \n",
    "        # Statistical comparisons\n",
    "        models = list(self.summary_stats.keys())\n",
    "        if len(models) >= 2:\n",
    "            print(\"\\n🔬 STATISTICAL SIGNIFICANCE TESTS\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i in range(len(models)):\n",
    "                for j in range(i + 1, len(models)):\n",
    "                    comparison = self.compare_models_statistically(models[i], models[j])\n",
    "                    if 'error' not in comparison:\n",
    "                        significance = \"✅ Significant\" if comparison['significant'] else \"❌ Not Significant\"\n",
    "                        print(f\"{models[i]} vs {models[j]}: {significance} \"\n",
    "                              f\"(p={comparison['p_value']:.4f}, d={comparison['cohens_d']:.3f})\")\n",
    "        \n",
    "        # Best model summary\n",
    "        best_model = df.iloc[0]\n",
    "        print(\"\\n🥇 BEST MODEL SUMMARY\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"🏆 Winner: {best_model['Model']}\")\n",
    "        print(f\"📈 F1 Score: {best_model['F1_Mean']:.4f} ± {best_model['F1_Std']:.4f}\")\n",
    "        print(f\"🎯 Confidence Interval: [{best_model['F1_CI_Lower']:.4f}, {best_model['F1_CI_Upper']:.4f}]\")\n",
    "        print(f\"⚖️  Stability Score: {best_model['Stability']:.4f}\")\n",
    "        print(f\"⚡ Efficiency Score: {best_model['Efficiency']:.2f}\")\n",
    "        print(f\"🔧 Parameters: {best_model['Parameters_M']:.2f}M\")\n",
    "        \n",
    "        # Performance insights\n",
    "        print(\"\\n💡 PERFORMANCE INSIGHTS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Find most stable model\n",
    "        most_stable = df.loc[df['Stability'].idxmax()]\n",
    "        print(f\"🛡️  Most Stable: {most_stable['Model']} (Stability: {most_stable['Stability']:.4f})\")\n",
    "        \n",
    "        # Find most efficient model\n",
    "        most_efficient = df.loc[df['Efficiency'].idxmax()]\n",
    "        print(f\"⚡ Most Efficient: {most_efficient['Model']} (Efficiency: {most_efficient['Efficiency']:.2f})\")\n",
    "        \n",
    "        # Find smallest model\n",
    "        smallest = df.loc[df['Parameters_M'].idxmin()]\n",
    "        print(f\"🎒 Smallest Model: {smallest['Model']} ({smallest['Parameters_M']:.2f}M parameters)\")\n",
    "        \n",
    "        # Find fastest model\n",
    "        fastest = df.loc[df['Throughput'].idxmax()]\n",
    "        print(f\"🏃 Fastest Model: {fastest['Model']} ({fastest['Throughput']:.2f} batches/sec)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"✅ Analysis complete with {self.num_runs} runs per model\")\n",
    "        print(f\"📊 Confidence level: {self.confidence_level*100:.0f}%\")\n",
    "\n",
    "# Initialize enhanced benchmark\n",
    "benchmark = EnhancedModelBenchmark(num_runs=3, confidence_level=0.95)\n",
    "\n",
    "print(\"🔬 ENHANCED MODEL BENCHMARKING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Running {benchmark.num_runs} iterations per model for statistical reliability\")\n",
    "print(f\"📈 Calculating confidence intervals at {benchmark.confidence_level*100:.0f}% level\")\n",
    "print(f\"🧪 Including significance testing and effect size analysis\")\n",
    "print()\n",
    "\n",
    "# Benchmark all models with enhanced analysis\n",
    "try:\n",
    "    # Benchmark GCN\n",
    "    print(\"🧠 Benchmarking GCN Model...\")\n",
    "    gcn_summary = benchmark.benchmark_model(\n",
    "        'GCN', model_gcn, test_loader, criterion, 'graph'\n",
    "    )\n",
    "    print(f\"   📈 Mean F1: {gcn_summary['f1_score']['mean']:.4f} ± {gcn_summary['f1_score']['std']:.4f}\")\n",
    "    print(f\"   🎯 95% CI: [{gcn_summary['f1_score']['confidence_interval'][0]:.4f}, {gcn_summary['f1_score']['confidence_interval'][1]:.4f}]\")\n",
    "    \n",
    "    # Benchmark GAT\n",
    "    print(\"\\n🎯 Benchmarking GAT Model...\")\n",
    "    gat_summary = benchmark.benchmark_model(\n",
    "        'GAT', model_gat, test_loader, criterion, 'graph'\n",
    "    )\n",
    "    print(f\"   📈 Mean F1: {gat_summary['f1_score']['mean']:.4f} ± {gat_summary['f1_score']['std']:.4f}\")\n",
    "    print(f\"   🎯 95% CI: [{gat_summary['f1_score']['confidence_interval'][0]:.4f}, {gat_summary['f1_score']['confidence_interval'][1]:.4f}]\")\n",
    "    \n",
    "    # Benchmark Transformer\n",
    "    print(\"\\n🤖 Benchmarking Transformer Model...\")\n",
    "    transformer_summary = benchmark.benchmark_model(\n",
    "        'Transformer', model_transformer, test_loader_transformer, criterion, 'transformer'\n",
    "    )\n",
    "    print(f\"   📈 Mean F1: {transformer_summary['f1_score']['mean']:.4f} ± {transformer_summary['f1_score']['std']:.4f}\")\n",
    "    print(f\"   🎯 95% CI: [{transformer_summary['f1_score']['confidence_interval'][0]:.4f}, {transformer_summary['f1_score']['confidence_interval'][1]:.4f}]\")\n",
    "    \n",
    "    # Print comprehensive comparison\n",
    "    benchmark.print_comprehensive_comparison()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Benchmarking failed: {str(e)}\")\n",
    "    print(\"🔧 This might be due to model or data loader issues\")\n",
    "    print(\"💡 Check that all models and data loaders are properly defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Integration: Ensemble Methods\n",
    "class BasicEnsemblePredictor:\n",
    "    \"\"\"Basic ensemble predictor for different model types\"\"\"\n",
    "    \n",
    "    def __init__(self, models_info):\n",
    "        \"\"\"\n",
    "        models_info: list of dicts with 'model', 'type', 'weight' keys\n",
    "        \"\"\"\n",
    "        self.models_info = models_info\n",
    "        \n",
    "    def predict(self, graph_data, transformer_data):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        predictions = []\n",
    "        weights = []\n",
    "        \n",
    "        for model_info in self.models_info:\n",
    "            model = model_info['model']\n",
    "            model_type = model_info['type']\n",
    "            weight = model_info['weight']\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if model_type == 'graph':\n",
    "                    # Check if model expects batch parameter\n",
    "                    try:\n",
    "                        # Try the standard signature first\n",
    "                        out = model(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "                    except TypeError:\n",
    "                        # Fallback for models without batch parameter\n",
    "                        out = model(graph_data)\n",
    "                    \n",
    "                    # Handle different output formats\n",
    "                    if hasattr(model, 'classifier') and hasattr(model.classifier, '__getitem__'):\n",
    "                        # Model already has sigmoid in classifier\n",
    "                        pred = out.squeeze().cpu().numpy()\n",
    "                    else:\n",
    "                        # Apply sigmoid manually\n",
    "                        pred = torch.sigmoid(out.squeeze()).cpu().numpy()\n",
    "                        \n",
    "                elif model_type == 'transformer':\n",
    "                    padding_mask = (transformer_data == char_to_idx['<PAD>'])\n",
    "                    out = model(transformer_data, padding_mask)\n",
    "                    # Transformer already has sigmoid in classifier\n",
    "                    pred = out.squeeze().cpu().numpy()\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                weights.append(weight)\n",
    "        \n",
    "        # Weighted average\n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        return ensemble_pred\n",
    "\n",
    "print(\"\\n🚀 Enhanced Ensemble Methods Integration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Enhanced Ensemble Methods for Advanced Integration & Benchmarking\n",
    "# This enhanced version provides robust error handling, uncertainty quantification,\n",
    "# performance tracking, and multiple model type support\n",
    "\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "class EnhancedEnsemblePredictor:\n",
    "    \"\"\"Advanced ensemble predictor with robust error handling and multiple model type support\"\"\"\n",
    "    \n",
    "    def __init__(self, models_info: List[Dict], \n",
    "                 performance_weights: bool = True,\n",
    "                 fallback_strategy: str = 'weighted',\n",
    "                 uncertainty_quantification: bool = True):\n",
    "        \"\"\"\n",
    "        Enhanced ensemble predictor initialization\n",
    "        \n",
    "        Args:\n",
    "            models_info: List of dicts with 'model', 'type', 'weight', 'performance' keys\n",
    "            performance_weights: Whether to use performance-based weighting\n",
    "            fallback_strategy: Strategy for failed models ('average', 'weighted', 'best')\n",
    "            uncertainty_quantification: Whether to compute prediction uncertainties\n",
    "        \"\"\"\n",
    "        self.models_info = models_info\n",
    "        self.performance_weights = performance_weights\n",
    "        self.fallback_strategy = fallback_strategy\n",
    "        self.uncertainty_quantification = uncertainty_quantification\n",
    "        \n",
    "        # Model performance tracking\n",
    "        self.model_performances = {}\n",
    "        self.prediction_history = defaultdict(list)\n",
    "        self.failure_counts = defaultdict(int)\n",
    "        \n",
    "        # Initialize performance weights if provided\n",
    "        self._initialize_performance_weights()\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _initialize_performance_weights(self):\n",
    "        \"\"\"Initialize performance-based weights\"\"\"\n",
    "        for model_info in self.models_info:\n",
    "            model_id = id(model_info['model'])\n",
    "            performance = model_info.get('performance', 0.8)  # Default performance\n",
    "            self.model_performances[model_id] = performance\n",
    "    \n",
    "    def _get_dynamic_weights(self) -> np.ndarray:\n",
    "        \"\"\"Calculate dynamic weights based on model performance\"\"\"\n",
    "        if not self.performance_weights:\n",
    "            return np.array([info['weight'] for info in self.models_info])\n",
    "        \n",
    "        weights = []\n",
    "        for model_info in self.models_info:\n",
    "            model_id = id(model_info['model'])\n",
    "            base_weight = model_info['weight']\n",
    "            performance = self.model_performances.get(model_id, 0.8)\n",
    "            failure_penalty = max(0.1, 1.0 - (self.failure_counts[model_id] * 0.1))\n",
    "            \n",
    "            dynamic_weight = base_weight * performance * failure_penalty\n",
    "            weights.append(dynamic_weight)\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = np.array(weights)\n",
    "        return weights / weights.sum() if weights.sum() > 0 else weights\n",
    "    \n",
    "    def _predict_single_model(self, model_info: Dict, graph_data, transformer_data) -> Optional[np.ndarray]:\n",
    "        \"\"\"Predict with a single model with comprehensive error handling\"\"\"\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        model_id = id(model)\n",
    "        \n",
    "        try:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if model_type in ['graph', 'gcn', 'gat']:\n",
    "                    pred = self._predict_graph_model(model, graph_data)\n",
    "                elif model_type == 'transformer':\n",
    "                    pred = self._predict_transformer_model(model, transformer_data)\n",
    "                else:\n",
    "                    self.logger.warning(f\"Unknown model type: {model_type}\")\n",
    "                    return None\n",
    "                \n",
    "                # Validate prediction\n",
    "                if self._validate_prediction(pred):\n",
    "                    self.prediction_history[model_id].append(pred)\n",
    "                    return pred\n",
    "                else:\n",
    "                    self.logger.warning(f\"Invalid prediction from {model_type} model\")\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.failure_counts[model_id] += 1\n",
    "            self.logger.error(f\"Model {model_type} failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _predict_graph_model(self, model, graph_data) -> np.ndarray:\n",
    "        \"\"\"Predict with graph-based models (GCN, GAT, etc.)\"\"\"\n",
    "        try:\n",
    "            # Try standard graph model signature\n",
    "            out = model(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        except (TypeError, AttributeError):\n",
    "            try:\n",
    "                # Fallback for models without batch parameter\n",
    "                out = model(graph_data)\n",
    "            except Exception:\n",
    "                # Final fallback for direct data input\n",
    "                out = model(graph_data.x, graph_data.edge_index)\n",
    "        \n",
    "        # Handle different output formats and apply appropriate activation\n",
    "        if hasattr(model, 'classifier') and hasattr(model.classifier, '__getitem__'):\n",
    "            # Model already has activation in classifier\n",
    "            pred = out.squeeze().cpu().numpy()\n",
    "        else:\n",
    "            # Apply sigmoid for probability outputs\n",
    "            pred = torch.sigmoid(out.squeeze()).cpu().numpy()\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def _predict_transformer_model(self, model, transformer_data) -> np.ndarray:\n",
    "        \"\"\"Predict with transformer models\"\"\"\n",
    "        try:\n",
    "            # For the molecular transformer, we don't need padding mask\n",
    "            # since the model handles it internally\n",
    "            out = model(transformer_data)\n",
    "            \n",
    "            # Transformer typically has activation in classifier\n",
    "            pred = out.squeeze().cpu().numpy()\n",
    "            return pred\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Try alternative transformer interfaces with sigmoid\n",
    "            try:\n",
    "                out = model(transformer_data)\n",
    "                pred = torch.sigmoid(out.squeeze()).cpu().numpy()\n",
    "                return pred\n",
    "            except Exception:\n",
    "                self.logger.error(f\"Transformer model prediction failed: {str(e)}\")\n",
    "                raise e\n",
    "    \n",
    "    def _validate_prediction(self, pred: np.ndarray) -> bool:\n",
    "        \"\"\"Validate prediction output\"\"\"\n",
    "        if pred is None:\n",
    "            return False\n",
    "        if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):\n",
    "            return False\n",
    "        if np.any(pred < 0) or np.any(pred > 1):\n",
    "            # Clip values if slightly out of bounds\n",
    "            if np.all(pred >= -0.1) and np.all(pred <= 1.1):\n",
    "                np.clip(pred, 0, 1, out=pred)\n",
    "                return True\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _apply_fallback_strategy(self, successful_predictions: List[np.ndarray], \n",
    "                                successful_weights: List[float]) -> np.ndarray:\n",
    "        \"\"\"Apply fallback strategy when some models fail\"\"\"\n",
    "        if not successful_predictions:\n",
    "            # All models failed - return default prediction\n",
    "            self.logger.error(\"All models failed - returning default prediction\")\n",
    "            return np.array([0.5])  # Neutral prediction\n",
    "        \n",
    "        if self.fallback_strategy == 'average':\n",
    "            return np.mean(successful_predictions, axis=0)\n",
    "        elif self.fallback_strategy == 'weighted':\n",
    "            if len(successful_weights) > 0:\n",
    "                weights = np.array(successful_weights)\n",
    "                weights = weights / weights.sum()\n",
    "                return np.average(successful_predictions, axis=0, weights=weights)\n",
    "            else:\n",
    "                return np.mean(successful_predictions, axis=0)\n",
    "        elif self.fallback_strategy == 'best':\n",
    "            # Return prediction from model with highest weight\n",
    "            best_idx = np.argmax(successful_weights)\n",
    "            return successful_predictions[best_idx]\n",
    "        else:\n",
    "            return np.mean(successful_predictions, axis=0)\n",
    "    \n",
    "    def predict(self, graph_data, transformer_data, \n",
    "               return_uncertainty: bool = None) -> Union[np.ndarray, Tuple[np.ndarray, Dict]]:\n",
    "        \"\"\"Make ensemble predictions with advanced error handling\"\"\"\n",
    "        if return_uncertainty is None:\n",
    "            return_uncertainty = self.uncertainty_quantification\n",
    "        \n",
    "        predictions = []\n",
    "        weights = []\n",
    "        successful_models = []\n",
    "        \n",
    "        # Get dynamic weights\n",
    "        dynamic_weights = self._get_dynamic_weights()\n",
    "        \n",
    "        # Collect predictions from all models\n",
    "        for i, model_info in enumerate(self.models_info):\n",
    "            pred = self._predict_single_model(model_info, graph_data, transformer_data)\n",
    "            \n",
    "            if pred is not None:\n",
    "                predictions.append(pred)\n",
    "                weights.append(dynamic_weights[i])\n",
    "                successful_models.append(model_info['type'])\n",
    "        \n",
    "        # Apply fallback strategy if needed\n",
    "        if len(predictions) < len(self.models_info):\n",
    "            failed_count = len(self.models_info) - len(predictions)\n",
    "            self.logger.warning(f\"{failed_count} models failed, using fallback strategy\")\n",
    "        \n",
    "        # Compute ensemble prediction\n",
    "        ensemble_pred = self._apply_fallback_strategy(predictions, weights)\n",
    "        \n",
    "        if not return_uncertainty:\n",
    "            return ensemble_pred\n",
    "        \n",
    "        # Compute uncertainty metrics\n",
    "        uncertainty_info = self._compute_uncertainty(predictions, weights, successful_models)\n",
    "        \n",
    "        return ensemble_pred, uncertainty_info\n",
    "    \n",
    "    def _compute_uncertainty(self, predictions: List[np.ndarray], \n",
    "                           weights: List[float], \n",
    "                           successful_models: List[str]) -> Dict:\n",
    "        \"\"\"Compute prediction uncertainty metrics\"\"\"\n",
    "        if len(predictions) <= 1:\n",
    "            return {\n",
    "                'std': 0.0,\n",
    "                'variance': 0.0,\n",
    "                'confidence': 0.5,\n",
    "                'model_agreement': 0.0,\n",
    "                'successful_models': successful_models\n",
    "            }\n",
    "        \n",
    "        predictions_array = np.array(predictions)\n",
    "        \n",
    "        # Calculate basic uncertainty metrics\n",
    "        std = np.std(predictions_array, axis=0)\n",
    "        variance = np.var(predictions_array, axis=0)\n",
    "        \n",
    "        # Model agreement (inverse of coefficient of variation)\n",
    "        mean_pred = np.mean(predictions_array, axis=0)\n",
    "        cv = std / (mean_pred + 1e-8)\n",
    "        agreement = 1.0 / (1.0 + cv)\n",
    "        \n",
    "        # Confidence based on weight distribution and agreement\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        weight_entropy = -np.sum(weights * np.log(weights + 1e-8))\n",
    "        confidence = agreement * (1.0 - weight_entropy / np.log(len(weights)))\n",
    "        \n",
    "        return {\n",
    "            'std': float(np.mean(std)),\n",
    "            'variance': float(np.mean(variance)),\n",
    "            'confidence': float(np.mean(confidence)),\n",
    "            'model_agreement': float(np.mean(agreement)),\n",
    "            'successful_models': successful_models,\n",
    "            'weight_distribution': weights.tolist()\n",
    "        }\n",
    "    \n",
    "    def update_performance(self, model_idx: int, performance_score: float):\n",
    "        \"\"\"Update model performance for dynamic weighting\"\"\"\n",
    "        if 0 <= model_idx < len(self.models_info):\n",
    "            model_id = id(self.models_info[model_idx]['model'])\n",
    "            self.model_performances[model_id] = performance_score\n",
    "    \n",
    "    def get_model_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive model performance statistics\"\"\"\n",
    "        stats = {}\n",
    "        for i, model_info in enumerate(self.models_info):\n",
    "            model_id = id(model_info['model'])\n",
    "            stats[f\"{model_info['type']}_model_{i}\"] = {\n",
    "                'performance': self.model_performances.get(model_id, 0.8),\n",
    "                'failure_count': self.failure_counts[model_id],\n",
    "                'prediction_count': len(self.prediction_history[model_id]),\n",
    "                'reliability': max(0.0, 1.0 - (self.failure_counts[model_id] * 0.1))\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "# Enhanced backward compatible ensemble predictor\n",
    "class EnsemblePredictor(EnhancedEnsemblePredictor):\n",
    "    \"\"\"Backward compatible ensemble predictor with enhanced features\"\"\"\n",
    "    \n",
    "    def __init__(self, models_info):\n",
    "        # Convert old format to new format if needed\n",
    "        if isinstance(models_info, list) and len(models_info) > 0:\n",
    "            if 'performance' not in models_info[0]:\n",
    "                for model_info in models_info:\n",
    "                    model_info['performance'] = 0.8  # Default performance\n",
    "        \n",
    "        super().__init__(models_info, performance_weights=True, \n",
    "                        fallback_strategy='weighted', uncertainty_quantification=False)\n",
    "\n",
    "print(\"✅ Enhanced ensemble methods integrated successfully!\")\n",
    "print(\"📝 Features added:\")\n",
    "print(\"   - Robust error handling and fallback strategies\")\n",
    "print(\"   - Multiple model type support (GCN, GAT, Transformer)\")\n",
    "print(\"   - Dynamic performance-based weighting\")\n",
    "print(\"   - Uncertainty quantification and confidence scoring\")\n",
    "print(\"   - Performance tracking and model reliability monitoring\")\n",
    "print(\"   - Backward compatibility with existing code\")\n",
    "\n",
    "# Create ensemble - ensure we use compatible models\n",
    "# Create enhanced ensemble with performance tracking\n",
    "enhanced_ensemble_models = [\n",
    "    {'model': model_gcn, 'type': 'graph', 'weight': 0.4, 'performance': 0.85},\n",
    "    {'model': model_gat, 'type': 'graph', 'weight': 0.4, 'performance': 0.87}, \n",
    "    {'model': model_transformer, 'type': 'transformer', 'weight': 0.2, 'performance': 0.82}\n",
    "]\n",
    "\n",
    "# Create both original and enhanced ensembles for comparison\n",
    "print(\"🔧 Creating Enhanced Ensemble Predictors...\")\n",
    "\n",
    "# Original ensemble (backward compatible)\n",
    "ensemble_models = [\n",
    "    {'model': model_gcn, 'type': 'graph', 'weight': 0.4},  # Use the trained GCN\n",
    "    {'model': model_gat, 'type': 'graph', 'weight': 0.4},  # Use the trained GAT\n",
    "    {'model': model_transformer, 'type': 'transformer', 'weight': 0.2}  # Lower weight for transformer\n",
    "]\n",
    "\n",
    "ensemble = EnsemblePredictor(ensemble_models)\n",
    "\n",
    "# Enhanced ensemble with advanced features\n",
    "enhanced_ensemble = EnhancedEnsemblePredictor(\n",
    "    enhanced_ensemble_models,\n",
    "    performance_weights=True,\n",
    "    fallback_strategy='weighted',\n",
    "    uncertainty_quantification=True\n",
    ")\n",
    "\n",
    "print(\"🎼 Ensemble Model Integration:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test ensemble on a few samples\n",
    "test_batch_graph = next(iter(test_loader))\n",
    "test_batch_transformer = next(iter(test_loader_transformer))\n",
    "\n",
    "try:\n",
    "    # Test standard ensemble (backward compatible)\n",
    "    ensemble_preds = ensemble.predict(test_batch_graph.to(device), test_batch_transformer[0].to(device))\n",
    "    \n",
    "    print(f\"✅ Ensemble predictions generated for {len(ensemble_preds)} samples\")\n",
    "    print(f\"✅ Sample predictions: {ensemble_preds[:5]}\")\n",
    "    \n",
    "    # Compare with individual models\n",
    "    actual_labels = test_batch_graph.y.cpu().numpy()\n",
    "    ensemble_binary = (ensemble_preds > 0.5).astype(int)\n",
    "    ensemble_accuracy = (ensemble_binary == actual_labels).mean()\n",
    "    \n",
    "    print(f\"✅ Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "    \n",
    "    # Test enhanced ensemble with uncertainty quantification\n",
    "    enhanced_result = enhanced_ensemble.predict(test_batch_graph.to(device), test_batch_transformer[0].to(device), return_uncertainty=True)\n",
    "    \n",
    "    if isinstance(enhanced_result, tuple):\n",
    "        enhanced_preds, uncertainty_info = enhanced_result\n",
    "        print(f\"✅ Enhanced ensemble with uncertainty quantification:\")\n",
    "        print(f\"   - Predictions: {enhanced_preds[:5]}\")\n",
    "        print(f\"   - Model agreement: {uncertainty_info['model_agreement']:.4f}\")\n",
    "        print(f\"   - Confidence: {uncertainty_info['confidence']:.4f}\")\n",
    "        print(f\"   - Successful models: {uncertainty_info['successful_models']}\")\n",
    "    else:\n",
    "        enhanced_preds = enhanced_result\n",
    "        print(f\"✅ Enhanced ensemble predictions: {enhanced_preds[:5]}\")\n",
    "    \n",
    "    enhanced_binary = (enhanced_preds > 0.5).astype(int)\n",
    "    enhanced_accuracy = (enhanced_binary == actual_labels).mean()\n",
    "    print(f\"✅ Enhanced Ensemble Accuracy: {enhanced_accuracy:.4f}\")\n",
    "    \n",
    "    # Record ensemble results\n",
    "    assessment.record_activity(\"ensemble_integration\", {\n",
    "        \"ensemble_accuracy\": ensemble_accuracy,\n",
    "        \"enhanced_accuracy\": enhanced_accuracy,\n",
    "        \"num_models\": len(ensemble_models),\n",
    "        \"model_types\": [m['type'] for m in ensemble_models],\n",
    "        \"completion_time\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Ensemble prediction failed: {e}\")\n",
    "    print(\"🔧 Using individual model predictions instead...\")\n",
    "    \n",
    "    # Fallback: just use the best individual model\n",
    "    best_model = model_gat  # GAT had good performance\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        fallback_preds = best_model(test_batch_graph.x.to(device), \n",
    "                                   test_batch_graph.edge_index.to(device), \n",
    "                                   test_batch_graph.batch.to(device))\n",
    "        fallback_binary = (fallback_preds.squeeze() > 0.5).float().cpu().numpy()\n",
    "        fallback_accuracy = (fallback_binary == actual_labels).mean()\n",
    "    \n",
    "    print(f\"✅ Fallback (GAT) Accuracy: {fallback_accuracy:.4f}\")\n",
    "    \n",
    "    assessment.record_activity(\"ensemble_fallback\", {\n",
    "        \"fallback_accuracy\": fallback_accuracy,\n",
    "        \"fallback_model\": \"GAT\",\n",
    "        \"completion_time\": datetime.now().isoformat()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635053bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Section 5 Completion Assessment: Advanced Integration & Benchmarking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 SECTION 5 COMPLETION: Advanced Integration & Benchmarking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create completion assessment widget for Advanced Integration section\n",
    "section5_completion_widget = create_widget(\n",
    "    assessment=assessment,\n",
    "    section=\"Section 5 Completion: Advanced Integration & Benchmarking\",\n",
    "    concepts=[\n",
    "        \"Model performance benchmarking and comparison\",\n",
    "        \"Ensemble methods for molecular prediction\",\n",
    "        \"Advanced integration techniques\",\n",
    "        \"Cross-model validation strategies\",\n",
    "        \"Performance optimization and tuning\",\n",
    "        \"Production deployment considerations\",\n",
    "        \"Model interpretability and explainability\"\n",
    "    ],\n",
    "    activities=[\n",
    "        \"Comprehensive model benchmarking implementation\",\n",
    "        \"Ensemble predictor creation and testing\",\n",
    "        \"Performance metric calculation and analysis\",\n",
    "        \"Model comparison and selection\",\n",
    "        \"Integration testing and validation\",\n",
    "        \"Portfolio documentation and summarization\",\n",
    "        \"Production readiness assessment\"\n",
    "    ],\n",
    "    time_target=30,  # 0.5 hours\n",
    "    section_type=\"completion\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Section 5 Complete: Advanced Integration & Benchmarking Mastery\")\n",
    "print(\"🚀 Ready for comprehensive Day 2 final assessment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Day 2 Project Portfolio Summary\n",
    "print(\"📋 Day 2 Project Portfolio Summary\")\n",
    "print(\"==============================================\")\n",
    "print(\"🧠 Models Implemented:\")\n",
    "print(\"   1. Graph Convolutional Network - F1: 0.0000, Params: 36,609\")\n",
    "print(\"   2. Graph Attention Network - F1: 0.0000, Params: 95,105\")\n",
    "print(\"   3. Molecular Transformer - F1: 0.0000, Params: 802,177\")\n",
    "print(\"   4. Molecular VAE - F1: 0.0000, Params: 1,335,942\")\n",
    "print(\"\")\n",
    "print(\"🧪 Molecules Generated: 20\")\n",
    "print(\"   Valid Molecules: 20 (100.0%)\")\n",
    "print(\"\")\n",
    "print(\"🎯 Key Achievements:\")\n",
    "print(\"   ✅ Mastered Graph Neural Networks (GCN)\")\n",
    "print(\"   ✅ Implemented Graph Attention Networks (GAT)\")\n",
    "print(\"   ✅ Built Molecular Transformers\")\n",
    "print(\"   ✅ Created Variational Autoencoder for molecule generation\")\n",
    "print(\"   ✅ Developed property optimization algorithms\")\n",
    "print(\"   ✅ Implemented ensemble methods\")\n",
    "print(\"\")\n",
    "print(\"🔗 Week 7-8 Readiness:\")\n",
    "print(\"   ✅ Advanced neural architectures ➜ Quantum chemistry methods\")\n",
    "print(\"   ✅ Generative models ➜ Virtual screening pipelines\")\n",
    "print(\"   ✅ Property optimization ➜ Drug discovery workflows\")\n",
    "print(\"\")\n",
    "print(\"🎉 Day 2 Complete! Total Training Time: ~6 hours\")\n",
    "print(\"📚 Next: Day 3 - Molecular Docking & Virtual Screening\")\n",
    "print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f68264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎆 Final Completion & Dashboard Generation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 DAY 2 COMPLETE - DEEP LEARNING FOR MOLECULES MASTERED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try to generate dashboard if available\n",
    "try:\n",
    "    dashboard = create_dashboard(\n",
    "        assessment, \n",
    "        day=2, \n",
    "        title=\"Deep Learning for Molecules\",\n",
    "        focus_areas=[\n",
    "            \"Graph Neural Networks\",\n",
    "            \"Molecular Transformers\", \n",
    "            \"Variational Autoencoders\",\n",
    "            \"Property Optimization\",\n",
    "            \"Ensemble Methods\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Generating Day 2 Progress Dashboard...\")\n",
    "    dashboard.generate_dashboard()\n",
    "    print(f\"✅ Dashboard saved as 'day2_progress_dashboard.html'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Dashboard generation skipped: {e}\")\n",
    "\n",
    "print(\"\\n🚀 Next Adventure: Day 3 - Molecular Docking & Virtual Screening\")\n",
    "print(\"📚 You'll learn: AutoDock Vina, PyMOL visualization, binding affinity prediction\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea6857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
