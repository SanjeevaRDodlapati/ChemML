{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093974df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChemML Integration Setupimport chemmlprint(f'🧪 ChemML {chemml.__version__} loaded for this notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f1822",
   "metadata": {},
   "source": [
    "# Week 10 Checkpoint: Integration and Validation\n",
    "\n",
    "## Learning Objectives\n",
    "- Integrate multiple computational approaches into unified workflows\n",
    "- Validate computational predictions against experimental data\n",
    "- Develop comprehensive model evaluation frameworks\n",
    "- Implement cross-validation and uncertainty quantification\n",
    "\n",
    "## Progress Tracking Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 10 Progress Tracking\n",
    "week_number = 10\n",
    "week_topic = \"Integration and Validation\"\n",
    "total_points = 100\n",
    "tasks_completed = 0\n",
    "current_score = 0\n",
    "\n",
    "# Task completion tracking\n",
    "task_scores = {\n",
    "    'task_1_workflow_integration': 0,\n",
    "    'task_2_experimental_validation': 0,\n",
    "    'task_3_uncertainty_quantification': 0,\n",
    "    'task_4_model_evaluation': 0\n",
    "}\n",
    "\n",
    "# Skills assessment\n",
    "skills_developed = {\n",
    "    'workflow_design': False,\n",
    "    'validation_strategies': False,\n",
    "    'uncertainty_analysis': False,\n",
    "    'model_evaluation': False\n",
    "}\n",
    "\n",
    "print(f\"Week {week_number}: {week_topic}\")\n",
    "print(f\"Progress: {tasks_completed}/4 tasks completed\")\n",
    "print(f\"Current Score: {current_score}/{total_points} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad3854",
   "metadata": {},
   "source": [
    "## Task 1: Workflow Integration (25 points)\n",
    "\n",
    "Develop integrated computational workflows combining multiple methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class IntegratedWorkflow:\n",
    "    \"\"\"Integrated computational drug discovery pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def generate_dataset(self, n_compounds=600):\n",
    "        \"\"\"Generate comprehensive molecular dataset\"\"\"\n",
    "        \n",
    "        compounds = []\n",
    "        for i in range(n_compounds):\n",
    "            # Simulate molecular properties\n",
    "            mw = np.random.normal(350, 100)\n",
    "            logp = np.random.normal(2.5, 1.5)\n",
    "            hbd = np.random.randint(0, 6)\n",
    "            hba = np.random.randint(1, 11)\n",
    "            \n",
    "            # QM properties\n",
    "            homo_energy = np.random.normal(-6.0, 1.5)\n",
    "            lumo_energy = np.random.normal(-1.0, 1.0)\n",
    "            dipole = np.random.gamma(2, 1.5)\n",
    "            \n",
    "            # MD properties  \n",
    "            sasa = mw * 0.5 + np.random.normal(0, 50)\n",
    "            rg = np.sqrt(mw/100) + np.random.normal(0, 0.3)\n",
    "            \n",
    "            # Target activities\n",
    "            activity_base = 5.0\n",
    "            mw_effect = -abs(mw - 400) * 0.002\n",
    "            logp_effect = -abs(logp - 3) * 0.3\n",
    "            homo_effect = (homo_energy + 6) * 0.1\n",
    "            \n",
    "            activity = activity_base + mw_effect + logp_effect + homo_effect + np.random.normal(0, 0.8)\n",
    "            \n",
    "            compounds.append({\n",
    "                'compound_id': f'COMP_{i:05d}',\n",
    "                'mw': mw, 'logp': logp, 'hbd': hbd, 'hba': hba,\n",
    "                'homo_energy': homo_energy, 'lumo_energy': lumo_energy, 'dipole': dipole,\n",
    "                'sasa': sasa, 'rg': rg,\n",
    "                'target_activity': max(0, activity)\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(compounds)\n",
    "    \n",
    "    def build_models(self, data):\n",
    "        \"\"\"Build models with different feature combinations\"\"\"\n",
    "        \n",
    "        feature_sets = {\n",
    "            'basic': ['mw', 'logp', 'hbd', 'hba'],\n",
    "            'qm': ['mw', 'logp', 'hbd', 'hba', 'homo_energy', 'lumo_energy', 'dipole'],\n",
    "            'md': ['mw', 'logp', 'hbd', 'hba', 'sasa', 'rg'],\n",
    "            'integrated': ['mw', 'logp', 'hbd', 'hba', 'homo_energy', 'lumo_energy', 'dipole', 'sasa', 'rg']\n",
    "        }\n",
    "        \n",
    "        y = data['target_activity'].values\n",
    "        results = {}\n",
    "        \n",
    "        for name, features in feature_sets.items():\n",
    "            X = data[features].values\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            results[name] = {'r2': r2, 'rmse': rmse, 'model': model, 'features': features}\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def analyze_integration(self, results):\n",
    "        \"\"\"Analyze benefits of integration\"\"\"\n",
    "        \n",
    "        # Performance comparison\n",
    "        perf_data = []\n",
    "        for name, result in results.items():\n",
    "            perf_data.append({'model': name, 'r2': result['r2'], 'rmse': result['rmse']})\n",
    "        \n",
    "        perf_df = pd.DataFrame(perf_data)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        perf_df.plot(x='model', y='r2', kind='bar', ax=axes[0], color='skyblue')\n",
    "        axes[0].set_title('Model Performance (R²)')\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        \n",
    "        perf_df.plot(x='model', y='rmse', kind='bar', ax=axes[1], color='lightcoral')\n",
    "        axes[1].set_title('Model Performance (RMSE)')\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return perf_df\n",
    "\n",
    "# Task 1 Implementation\n",
    "print(\"=== Task 1: Workflow Integration ===\")\n",
    "\n",
    "workflow = IntegratedWorkflow()\n",
    "\n",
    "print(\"\\n1. Generating comprehensive dataset...\")\n",
    "data = workflow.generate_dataset(500)\n",
    "print(f\"Generated {len(data)} compounds\")\n",
    "print(\"\\nDataset preview:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\n2. Building integrated models...\")\n",
    "model_results = workflow.build_models(data)\n",
    "\n",
    "print(\"\\n3. Analyzing integration benefits...\")\n",
    "performance = workflow.analyze_integration(model_results)\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(performance.round(3))\n",
    "\n",
    "integrated_r2 = model_results['integrated']['r2']\n",
    "basic_r2 = model_results['basic']['r2']\n",
    "improvement = integrated_r2 - basic_r2\n",
    "\n",
    "print(f\"\\nIntegration benefit: {improvement:.3f} R² improvement\")\n",
    "print(f\"Relative improvement: {(improvement/basic_r2)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc799903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update progress for Task 1\n",
    "task_scores['task_1_workflow_integration'] = 25\n",
    "skills_developed['workflow_design'] = True\n",
    "tasks_completed += 1\n",
    "current_score += 25\n",
    "\n",
    "print(f\"\\n✓ Task 1 completed! Score: 25/25\")\n",
    "print(f\"Progress: {tasks_completed}/4 tasks completed\")\n",
    "print(f\"Current Score: {current_score}/{total_points} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325060c2",
   "metadata": {},
   "source": [
    "## Task 2: Experimental Validation (25 points)\n",
    "\n",
    "Validate computational predictions against experimental data and develop validation frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalValidation:\n",
    "    \"\"\"Framework for validating computational predictions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experimental_data = {}\n",
    "        self.validation_metrics = {}\n",
    "        \n",
    "    def generate_experimental_data(self, computational_predictions, noise_level=0.3):\n",
    "        \"\"\"Simulate experimental validation data\"\"\"\n",
    "        \n",
    "        experimental_values = []\n",
    "        \n",
    "        for pred in computational_predictions:\n",
    "            # Systematic bias and noise\n",
    "            bias = -0.2\n",
    "            noise = np.random.normal(0, noise_level)\n",
    "            \n",
    "            # Occasional outliers\n",
    "            if np.random.random() < 0.1:\n",
    "                outlier_factor = np.random.choice([-2, 2]) * np.random.uniform(0.5, 1.5)\n",
    "                exp_value = pred + bias + noise + outlier_factor\n",
    "            else:\n",
    "                exp_value = pred + bias + noise\n",
    "                \n",
    "            experimental_values.append(max(0, exp_value))\n",
    "            \n",
    "        return np.array(experimental_values)\n",
    "    \n",
    "    def validate_predictions(self, predictions, experimental):\n",
    "        \"\"\"Comprehensive validation analysis\"\"\"\n",
    "        \n",
    "        from scipy.stats import pearsonr\n",
    "        \n",
    "        # Basic metrics\n",
    "        r2 = r2_score(experimental, predictions)\n",
    "        pearson_r, _ = pearsonr(predictions, experimental)\n",
    "        mae = mean_absolute_error(experimental, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(experimental, predictions))\n",
    "        \n",
    "        # Outlier detection\n",
    "        residuals = experimental - predictions\n",
    "        outlier_threshold = 2 * np.std(residuals)\n",
    "        outliers = np.abs(residuals) > outlier_threshold\n",
    "        \n",
    "        validation_metrics = {\n",
    "            'r2_score': r2,\n",
    "            'pearson_correlation': pearson_r,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'outlier_percentage': (outliers.sum() / len(outliers)) * 100\n",
    "        }\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Prediction vs Experimental\n",
    "        axes[0].scatter(predictions, experimental, alpha=0.6)\n",
    "        axes[0].plot([min(predictions), max(predictions)], \n",
    "                    [min(predictions), max(predictions)], 'r--')\n",
    "        axes[0].set_xlabel('Predicted Activity')\n",
    "        axes[0].set_ylabel('Experimental Activity')\n",
    "        axes[0].set_title(f'Predicted vs Experimental (R² = {r2:.3f})')\n",
    "        \n",
    "        # Residuals plot\n",
    "        axes[1].scatter(predictions, residuals, alpha=0.6)\n",
    "        axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[1].set_xlabel('Predicted Activity')\n",
    "        axes[1].set_ylabel('Residuals')\n",
    "        axes[1].set_title('Residuals Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return validation_metrics\n",
    "\n",
    "# Task 2 Implementation\n",
    "print(\"\\n=== Task 2: Experimental Validation ===\")\n",
    "\n",
    "validator = ExperimentalValidation()\n",
    "\n",
    "# Use predictions from integrated model\n",
    "integrated_model = model_results['integrated']['model']\n",
    "features = model_results['integrated']['features']\n",
    "\n",
    "X_test = data[features].values\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "predictions = integrated_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n1. Generating experimental validation data...\")\n",
    "experimental_data = validator.generate_experimental_data(predictions, noise_level=0.4)\n",
    "print(f\"Generated experimental data for {len(experimental_data)} compounds\")\n",
    "\n",
    "print(\"\\n2. Validating computational predictions...\")\n",
    "validation_results = validator.validate_predictions(predictions, experimental_data)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for metric, value in validation_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "# Update progress for Task 2\n",
    "task_scores['task_2_experimental_validation'] = 25\n",
    "skills_developed['validation_strategies'] = True\n",
    "tasks_completed += 1\n",
    "current_score += 25\n",
    "\n",
    "print(f\"\\n✓ Task 2 completed! Score: 25/25\")\n",
    "print(f\"Progress: {tasks_completed}/4 tasks completed\")\n",
    "print(f\"Current Score: {current_score}/{total_points} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d867409",
   "metadata": {},
   "source": [
    "## Task 3: Uncertainty Quantification (25 points)\n",
    "\n",
    "Implement uncertainty quantification methods for computational predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64181a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyQuantification:\n",
    "    \"\"\"Framework for quantifying uncertainty in computational predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bootstrap_models = []\n",
    "        self.uncertainty_metrics = {}\n",
    "        \n",
    "    def bootstrap_ensemble(self, X, y, n_estimators=100, sample_fraction=0.8):\n",
    "        \"\"\"Create bootstrap ensemble for uncertainty estimation.\"\"\"\n",
    "        self.bootstrap_models = []\n",
    "        n_samples = int(len(X) * sample_fraction)\n",
    "        \n",
    "        for i in range(n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(len(X), n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "            \n",
    "            # Train model on bootstrap sample\n",
    "            model = RandomForestRegressor(n_estimators=50, random_state=i)\n",
    "            model.fit(X_boot, y_boot)\n",
    "            self.bootstrap_models.append(model)\n",
    "            \n",
    "        return self.bootstrap_models\n",
    "    \n",
    "    def predict_with_uncertainty(self, X):\n",
    "        \"\"\"Make predictions with uncertainty estimates.\"\"\"\n",
    "        if not self.bootstrap_models:\n",
    "            raise ValueError(\"Bootstrap ensemble not trained\")\n",
    "            \n",
    "        # Get predictions from all models\n",
    "        predictions = np.array([model.predict(X) for model in self.bootstrap_models])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        std_pred = np.std(predictions, axis=0)\n",
    "        \n",
    "        # Confidence intervals (95%)\n",
    "        ci_lower = np.percentile(predictions, 2.5, axis=0)\n",
    "        ci_upper = np.percentile(predictions, 97.5, axis=0)\n",
    "        \n",
    "        return {\n",
    "            'mean': mean_pred,\n",
    "            'std': std_pred,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'all_predictions': predictions\n",
    "        }\n",
    "    \n",
    "    def calculate_prediction_intervals(self, X, confidence_level=0.95):\n",
    "        \"\"\"Calculate prediction intervals.\"\"\"\n",
    "        results = self.predict_with_uncertainty(X)\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        lower_percentile = (alpha/2) * 100\n",
    "        upper_percentile = (1 - alpha/2) * 100\n",
    "        \n",
    "        lower_bound = np.percentile(results['all_predictions'], lower_percentile, axis=0)\n",
    "        upper_bound = np.percentile(results['all_predictions'], upper_percentile, axis=0)\n",
    "        \n",
    "        interval_width = upper_bound - lower_bound\n",
    "        \n",
    "        return {\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'interval_width': interval_width,\n",
    "            'confidence_level': confidence_level\n",
    "        }\n",
    "    \n",
    "    def uncertainty_vs_error_analysis(self, X, y_true):\n",
    "        \"\"\"Analyze relationship between uncertainty and prediction error.\"\"\"\n",
    "        results = self.predict_with_uncertainty(X)\n",
    "        \n",
    "        predictions = results['mean']\n",
    "        uncertainties = results['std']\n",
    "        \n",
    "        # Calculate absolute errors\n",
    "        abs_errors = np.abs(predictions - y_true)\n",
    "        \n",
    "        # Correlation between uncertainty and error\n",
    "        correlation = np.corrcoef(uncertainties, abs_errors)[0, 1]\n",
    "        \n",
    "        # Calibration: check if uncertainties are well-calibrated\n",
    "        sorted_indices = np.argsort(uncertainties)\n",
    "        n_bins = 10\n",
    "        bin_size = len(sorted_indices) // n_bins\n",
    "        \n",
    "        calibration_data = []\n",
    "        for i in range(n_bins):\n",
    "            start_idx = i * bin_size\n",
    "            end_idx = (i + 1) * bin_size if i < n_bins - 1 else len(sorted_indices)\n",
    "            bin_indices = sorted_indices[start_idx:end_idx]\n",
    "            \n",
    "            bin_uncertainty = np.mean(uncertainties[bin_indices])\n",
    "            bin_error = np.mean(abs_errors[bin_indices])\n",
    "            \n",
    "            calibration_data.append({\n",
    "                'bin': i + 1,\n",
    "                'mean_uncertainty': bin_uncertainty,\n",
    "                'mean_error': bin_error\n",
    "            })\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Uncertainty vs Error scatter plot\n",
    "        axes[0, 0].scatter(uncertainties, abs_errors, alpha=0.6)\n",
    "        axes[0, 0].set_xlabel('Prediction Uncertainty (Std Dev)')\n",
    "        axes[0, 0].set_ylabel('Absolute Error')\n",
    "        axes[0, 0].set_title(f'Uncertainty vs Error (r = {correlation:.3f})')\n",
    "        \n",
    "        # Prediction intervals visualization\n",
    "        sample_indices = np.random.choice(len(X), min(50, len(X)), replace=False)\n",
    "        x_pos = range(len(sample_indices))\n",
    "        \n",
    "        axes[0, 1].errorbar(x_pos, predictions[sample_indices], \n",
    "                           yerr=1.96*uncertainties[sample_indices], \n",
    "                           fmt='o', alpha=0.7, capsize=3)\n",
    "        axes[0, 1].scatter(x_pos, y_true[sample_indices], color='red', alpha=0.7, label='True Values')\n",
    "        axes[0, 1].set_xlabel('Sample Index')\n",
    "        axes[0, 1].set_ylabel('Activity Value')\n",
    "        axes[0, 1].set_title('Prediction Intervals (95% CI)')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Calibration plot\n",
    "        cal_df = pd.DataFrame(calibration_data)\n",
    "        axes[1, 0].plot(cal_df['mean_uncertainty'], cal_df['mean_error'], 'bo-')\n",
    "        axes[1, 0].plot([0, cal_df['mean_uncertainty'].max()], [0, cal_df['mean_uncertainty'].max()], 'r--')\n",
    "        axes[1, 0].set_xlabel('Mean Predicted Uncertainty')\n",
    "        axes[1, 0].set_ylabel('Mean Absolute Error')\n",
    "        axes[1, 0].set_title('Uncertainty Calibration')\n",
    "        \n",
    "        # Uncertainty distribution\n",
    "        axes[1, 1].hist(uncertainties, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Prediction Uncertainty')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Distribution of Uncertainties')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'uncertainty_error_correlation': correlation,\n",
    "            'calibration_data': calibration_data,\n",
    "            'mean_uncertainty': np.mean(uncertainties),\n",
    "            'mean_absolute_error': np.mean(abs_errors)\n",
    "        }\n",
    "\n",
    "# Task 3 Implementation\n",
    "print(\"\\n=== Task 3: Uncertainty Quantification ===\")\n",
    "\n",
    "uq = UncertaintyQuantification()\n",
    "\n",
    "# Use the same data from previous tasks\n",
    "X = data[features].values\n",
    "y = data['activity'].values\n",
    "\n",
    "# Split data for uncertainty analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n1. Training bootstrap ensemble...\")\n",
    "bootstrap_models = uq.bootstrap_ensemble(X_train_scaled, y_train, n_estimators=50)\n",
    "print(f\"Trained {len(bootstrap_models)} bootstrap models\")\n",
    "\n",
    "print(\"\\n2. Making predictions with uncertainty...\")\n",
    "uncertainty_results = uq.predict_with_uncertainty(X_test_scaled)\n",
    "print(f\"Mean prediction uncertainty: {np.mean(uncertainty_results['std']):.3f}\")\n",
    "print(f\"Range of uncertainties: {np.min(uncertainty_results['std']):.3f} - {np.max(uncertainty_results['std']):.3f}\")\n",
    "\n",
    "print(\"\\n3. Calculating prediction intervals...\")\n",
    "intervals = uq.calculate_prediction_intervals(X_test_scaled, confidence_level=0.95)\n",
    "print(f\"Mean 95% prediction interval width: {np.mean(intervals['interval_width']):.3f}\")\n",
    "\n",
    "print(\"\\n4. Analyzing uncertainty vs error relationship...\")\n",
    "analysis_results = uq.uncertainty_vs_error_analysis(X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\nUncertainty Analysis Results:\")\n",
    "for key, value in analysis_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "\n",
    "# Update progress for Task 3\n",
    "task_scores['task_3_uncertainty_quantification'] = 25\n",
    "skills_developed['uncertainty_analysis'] = True\n",
    "tasks_completed += 1\n",
    "current_score += 25\n",
    "\n",
    "print(f\"\\n✓ Task 3 completed! Score: 25/25\")\n",
    "print(f\"Progress: {tasks_completed}/4 tasks completed\")\n",
    "print(f\"Current Score: {current_score}/{total_points} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22897806",
   "metadata": {},
   "source": [
    "## Task 4: Comprehensive Model Evaluation (25 points)\n",
    "\n",
    "Develop comprehensive frameworks for evaluating model performance across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveModelEvaluation:\n",
    "    \"\"\"Framework for comprehensive model evaluation and comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_results = {}\n",
    "        self.comparison_metrics = {}\n",
    "        \n",
    "    def cross_validation_analysis(self, models, X, y, cv_folds=5):\n",
    "        \"\"\"Perform comprehensive cross-validation analysis.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = {\n",
    "                'r2': cross_val_score(model, X, y, cv=cv_folds, scoring='r2'),\n",
    "                'neg_mae': cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_absolute_error'),\n",
    "                'neg_rmse': cross_val_score(model, X, y, cv=cv_folds, scoring='neg_root_mean_squared_error')\n",
    "            }\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = {}\n",
    "            for metric, scores in cv_scores.items():\n",
    "                if metric.startswith('neg_'):\n",
    "                    scores = -scores  # Convert negative scores to positive\n",
    "                    metric_name = metric[4:]  # Remove 'neg_' prefix\n",
    "                else:\n",
    "                    metric_name = metric\n",
    "                    \n",
    "                stats[metric_name] = {\n",
    "                    'mean': np.mean(scores),\n",
    "                    'std': np.std(scores),\n",
    "                    'min': np.min(scores),\n",
    "                    'max': np.max(scores),\n",
    "                    'scores': scores\n",
    "                }\n",
    "            \n",
    "            results[name] = stats\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def learning_curve_analysis(self, model, X, y, train_sizes=None):\n",
    "        \"\"\"Analyze learning curves to assess model performance vs training size.\"\"\"\n",
    "        from sklearn.model_selection import learning_curve\n",
    "        \n",
    "        if train_sizes is None:\n",
    "            train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, train_sizes=train_sizes, cv=5, scoring='r2'\n",
    "        )\n",
    "        \n",
    "        # Calculate statistics\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score')\n",
    "        plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        plt.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Score')\n",
    "        plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "        \n",
    "        plt.xlabel('Training Set Size')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.title('Learning Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'train_sizes': train_sizes_abs,\n",
    "            'train_scores': {'mean': train_mean, 'std': train_std},\n",
    "            'val_scores': {'mean': val_mean, 'std': val_std}\n",
    "        }\n",
    "    \n",
    "    def feature_importance_analysis(self, models, feature_names):\n",
    "        \"\"\"Analyze feature importance across different models.\"\"\"\n",
    "        importance_data = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_data[name] = model.feature_importances_\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                importance_data[name] = np.abs(model.coef_)\n",
    "        \n",
    "        if not importance_data:\n",
    "            print(\"No feature importance data available for provided models\")\n",
    "            return None\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        x = np.arange(len(feature_names))\n",
    "        width = 0.8 / len(importance_data)\n",
    "        \n",
    "        for i, (model_name, importances) in enumerate(importance_data.items()):\n",
    "            ax.bar(x + i * width, importances, width, label=model_name, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Importance')\n",
    "        ax.set_title('Feature Importance Comparison')\n",
    "        ax.set_xticks(x + width * (len(importance_data) - 1) / 2)\n",
    "        ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_data\n",
    "    \n",
    "    def model_interpretability_analysis(self, models, X_sample, feature_names):\n",
    "        \"\"\"Analyze model interpretability using various methods.\"\"\"\n",
    "        interpretability_results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nAnalyzing interpretability for {name}...\")\n",
    "            \n",
    "            # Feature importance (if available)\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                top_features = np.argsort(model.feature_importances_)[-10:]\n",
    "                interpretability_results[name] = {\n",
    "                    'top_features': [feature_names[i] for i in top_features],\n",
    "                    'top_importances': model.feature_importances_[top_features]\n",
    "                }\n",
    "            \n",
    "            # Prediction analysis on sample\n",
    "            sample_pred = model.predict(X_sample[:5])  # First 5 samples\n",
    "            interpretability_results[name]['sample_predictions'] = sample_pred\n",
    "        \n",
    "        return interpretability_results\n",
    "    \n",
    "    def performance_stability_analysis(self, models, X, y, n_runs=10):\n",
    "        \"\"\"Analyze performance stability across multiple runs.\"\"\"\n",
    "        stability_results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"Analyzing stability for {name}...\")\n",
    "            \n",
    "            scores = []\n",
    "            for run in range(n_runs):\n",
    "                # Random train/test split for each run\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.3, random_state=run\n",
    "                )\n",
    "                \n",
    "                # Clone and train model\n",
    "                from sklearn.base import clone\n",
    "                model_clone = clone(model)\n",
    "                model_clone.fit(X_train, y_train)\n",
    "                \n",
    "                # Evaluate\n",
    "                score = model_clone.score(X_test, y_test)\n",
    "                scores.append(score)\n",
    "            \n",
    "            stability_results[name] = {\n",
    "                'scores': scores,\n",
    "                'mean': np.mean(scores),\n",
    "                'std': np.std(scores),\n",
    "                'coefficient_of_variation': np.std(scores) / np.mean(scores)\n",
    "            }\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        model_names = list(stability_results.keys())\n",
    "        means = [stability_results[name]['mean'] for name in model_names]\n",
    "        stds = [stability_results[name]['std'] for name in model_names]\n",
    "        \n",
    "        x_pos = np.arange(len(model_names))\n",
    "        ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel('R² Score')\n",
    "        ax.set_title('Model Performance Stability')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(model_names, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return stability_results\n",
    "    \n",
    "    def generate_evaluation_report(self, all_results):\n",
    "        \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Summary statistics\n",
    "        if 'cross_validation' in all_results:\n",
    "            print(\"\\n1. CROSS-VALIDATION RESULTS:\")\n",
    "            cv_results = all_results['cross_validation']\n",
    "            \n",
    "            for model_name, metrics in cv_results.items():\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                for metric_name, stats in metrics.items():\n",
    "                    print(f\"  {metric_name}: {stats['mean']:.3f} ± {stats['std']:.3f}\")\n",
    "        \n",
    "        # Stability analysis\n",
    "        if 'stability' in all_results:\n",
    "            print(\"\\n2. PERFORMANCE STABILITY:\")\n",
    "            stability_results = all_results['stability']\n",
    "            \n",
    "            for model_name, stats in stability_results.items():\n",
    "                cv = stats['coefficient_of_variation']\n",
    "                print(f\"  {model_name}: CV = {cv:.3f} (lower is more stable)\")\n",
    "        \n",
    "        # Feature importance\n",
    "        if 'feature_importance' in all_results:\n",
    "            print(\"\\n3. FEATURE IMPORTANCE ANALYSIS:\")\n",
    "            print(\"  Feature importance data available for model comparison\")\n",
    "        \n",
    "        # Model recommendations\n",
    "        print(\"\\n4. RECOMMENDATIONS:\")\n",
    "        \n",
    "        if 'cross_validation' in all_results:\n",
    "            # Find best performing model\n",
    "            best_model = None\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            for model_name, metrics in all_results['cross_validation'].items():\n",
    "                r2_score = metrics['r2']['mean']\n",
    "                if r2_score > best_score:\n",
    "                    best_score = r2_score\n",
    "                    best_model = model_name\n",
    "            \n",
    "            print(f\"  - Best overall performance: {best_model} (R² = {best_score:.3f})\")\n",
    "        \n",
    "        if 'stability' in all_results:\n",
    "            # Find most stable model\n",
    "            most_stable = min(all_results['stability'].items(), \n",
    "                            key=lambda x: x[1]['coefficient_of_variation'])\n",
    "            print(f\"  - Most stable model: {most_stable[0]} (CV = {most_stable[1]['coefficient_of_variation']:.3f})\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "# Task 4 Implementation\n",
    "print(\"\\n=== Task 4: Comprehensive Model Evaluation ===\")\n",
    "\n",
    "evaluator = ComprehensiveModelEvaluation()\n",
    "\n",
    "# Prepare models for evaluation\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Use the scaled data from previous tasks\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n1. Cross-validation analysis...\")\n",
    "cv_results = evaluator.cross_validation_analysis(models, X_scaled, y)\n",
    "\n",
    "print(\"\\n2. Learning curve analysis...\")\n",
    "learning_results = evaluator.learning_curve_analysis(\n",
    "    models['Random Forest'], X_scaled, y\n",
    ")\n",
    "\n",
    "print(\"\\n3. Feature importance analysis...\")\n",
    "# Train models first for feature importance\n",
    "for model in models.values():\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "feature_importance = evaluator.feature_importance_analysis(models, features)\n",
    "\n",
    "print(\"\\n4. Performance stability analysis...\")\n",
    "stability_results = evaluator.performance_stability_analysis(models, X_scaled, y)\n",
    "\n",
    "print(\"\\n5. Model interpretability analysis...\")\n",
    "interpretability_results = evaluator.model_interpretability_analysis(\n",
    "    models, X_scaled[:10], features\n",
    ")\n",
    "\n",
    "# Compile all results\n",
    "all_evaluation_results = {\n",
    "    'cross_validation': cv_results,\n",
    "    'learning_curves': learning_results,\n",
    "    'feature_importance': feature_importance,\n",
    "    'stability': stability_results,\n",
    "    'interpretability': interpretability_results\n",
    "}\n",
    "\n",
    "print(\"\\n6. Generating comprehensive evaluation report...\")\n",
    "final_report = evaluator.generate_evaluation_report(all_evaluation_results)\n",
    "\n",
    "# Update progress for Task 4\n",
    "task_scores['task_4_model_evaluation'] = 25\n",
    "skills_developed['model_evaluation'] = True\n",
    "tasks_completed += 1\n",
    "current_score += 25\n",
    "\n",
    "print(f\"\\n✓ Task 4 completed! Score: 25/25\")\n",
    "print(f\"Progress: {tasks_completed}/4 tasks completed\")\n",
    "print(f\"Current Score: {current_score}/{total_points} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217117e",
   "metadata": {},
   "source": [
    "## Week 10 Summary and Assessment\n",
    "\n",
    "### Learning Outcomes Achieved\n",
    "- ✓ Integrated multiple computational approaches into unified workflows\n",
    "- ✓ Validated computational predictions against experimental data\n",
    "- ✓ Developed comprehensive model evaluation frameworks\n",
    "- ✓ Implemented cross-validation and uncertainty quantification\n",
    "\n",
    "### Skills Developed\n",
    "- Workflow design and integration\n",
    "- Experimental validation strategies\n",
    "- Uncertainty quantification methods\n",
    "- Comprehensive model evaluation\n",
    "\n",
    "### Key Achievements\n",
    "1. **Workflow Integration**: Successfully combined QM, MD, and ML approaches\n",
    "2. **Experimental Validation**: Developed validation frameworks with statistical analysis\n",
    "3. **Uncertainty Quantification**: Implemented bootstrap ensembles and prediction intervals\n",
    "4. **Model Evaluation**: Created comprehensive evaluation frameworks with stability analysis\n",
    "\n",
    "### Next Steps\n",
    "- Week 11: Final project preparation and portfolio development\n",
    "- Week 12: Portfolio completion and peer assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Week 10 Progress Update\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEK 10 CHECKPOINT COMPLETION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nWeek {week_number}: {week_topic}\")\n",
    "print(f\"Total Tasks: 4\")\n",
    "print(f\"Tasks Completed: {tasks_completed}\")\n",
    "print(f\"Final Score: {current_score}/{total_points} points\")\n",
    "print(f\"Completion Rate: {(current_score/total_points)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nTask Breakdown:\")\n",
    "for task, score in task_scores.items():\n",
    "    status = \"✓\" if score > 0 else \"✗\"\n",
    "    print(f\"  {status} {task.replace('_', ' ').title()}: {score}/25 points\")\n",
    "\n",
    "print(\"\\nSkills Developed:\")\n",
    "for skill, achieved in skills_developed.items():\n",
    "    status = \"✓\" if achieved else \"✗\"\n",
    "    print(f\"  {status} {skill.replace('_', ' ').title()}\")\n",
    "\n",
    "if current_score == total_points:\n",
    "    print(\"\\n🎉 WEEK 10 CHECKPOINT COMPLETED SUCCESSFULLY! 🎉\")\n",
    "    print(\"Ready to proceed to Week 11: Final Project Preparation\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Week 10 partially completed: {current_score}/{total_points} points\")\n",
    "    print(\"Review incomplete tasks before proceeding to Week 11\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "chemml": {
   "integrated": true,
   "integration_date": "2025-06-15T23:50:25.002293",
   "version": "1.0"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
