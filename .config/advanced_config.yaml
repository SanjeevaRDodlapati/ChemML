# ChemML Advanced Configuration
# ============================
# This configuration file shows the flexible backend selection
# for distributed training, hyperparameter optimization, and monitoring

# DISTRIBUTED COMPUTING CONFIGURATION
# ===================================
distributed:
  # Primary backend selection (ray, dask, horovod, pytorch_ddp)
  backend: "ray"

  # Ray-specific configuration
  ray:
    address: "auto"  # Use 'auto' for local cluster, or specific address
    dashboard_host: "127.0.0.1"
    dashboard_port: 8265
    num_cpus: null  # Auto-detect
    num_gpus: null  # Auto-detect
    memory: null    # Auto-detect

  # Dask-specific configuration
  dask:
    scheduler_address: "localhost:8786"
    n_workers: 4
    threads_per_worker: 2
    memory_limit: "4GB"

  # Horovod configuration
  horovod:
    np: 4  # Number of processes
    hosts: ["localhost:4"]

  # Auto-scaling configuration
  auto_scaling:
    enabled: true
    min_workers: 1
    max_workers: 16
    target_cpu_utilization: 0.8
    scale_up_threshold: 0.9
    scale_down_threshold: 0.3

# HYPERPARAMETER OPTIMIZATION CONFIGURATION
# =========================================
hyperparameter_optimization:
  # Primary optimizer (optuna_tpe, optuna_cmaes, ray_tune_bo, hyperopt_tpe, ax_botorch)
  optimizer: "optuna_tpe"

  # General optimization settings
  n_trials: 100
  timeout: 3600  # 1 hour timeout
  n_jobs: -1     # Use all available cores

  # Optuna-specific settings
  optuna:
    study_name: "chemml_optimization"
    storage: "sqlite:///optuna_studies.db"  # Can use PostgreSQL/MySQL for distributed
    pruning:
      enabled: true
      algorithm: "MedianPruner"
      n_startup_trials: 5
      n_warmup_steps: 10
    sampler: "TPESampler"  # TPESampler, CmaEsSampler, RandomSampler

  # Ray Tune settings
  ray_tune:
    search_algorithm: "bayesian"  # bayesian, random, grid, hyperopt
    scheduler: "AsyncHyperBand"   # AsyncHyperBand, ASHA, PopulationBased
    metric: "validation_accuracy"
    mode: "max"

  # Multi-objective optimization
  multi_objective:
    enabled: false
    objectives: ["accuracy", "inference_time"]
    directions: ["maximize", "minimize"]

# PERFORMANCE MONITORING CONFIGURATION
# ====================================
performance_monitoring:
  # Tracking backends (can enable multiple)
  tracking:
    mlflow:
      enabled: true
      tracking_uri: "sqlite:///mlruns.db"  # Can use remote server
      experiment_name: "chemml_experiments"

    wandb:
      enabled: true
      project: "chemml-research"
      entity: "your-wandb-username"

    tensorboard:
      enabled: true
      log_dir: "./tensorboard_logs"

  # Dashboard configuration
  dashboard:
    type: "custom"  # custom, tensorboard, wandb, mlflow
    host: "localhost"
    port: 8080
    auto_refresh: 30  # seconds

  # Metrics collection
  metrics:
    system_metrics: true
    gpu_metrics: true
    custom_metrics: true
    collection_interval: 10  # seconds

  # Alerting (optional)
  alerting:
    enabled: false
    slack_webhook: null
    email_recipients: []
    thresholds:
      cpu_usage: 90
      memory_usage: 85
      gpu_usage: 95

# QUANTUM COMPUTING CONFIGURATION
# ===============================
quantum:
  # Primary backend (pennylane, qiskit, cirq)
  backend: "pennylane"

  # PennyLane configuration
  pennylane:
    device: "default.qubit"  # default.qubit, lightning.qubit, qiskit.aer
    shots: 1024
    diff_method: "best"  # parameter-shift, finite-diff, adjoint

  # Qiskit configuration
  qiskit:
    backend: "aer_simulator"  # aer_simulator, qasm_simulator, or real hardware
    shots: 1024
    optimization_level: 1

  # Hardware access (when available)
  hardware:
    provider: null  # "IBMQ", "Rigetti", "IonQ"
    token: null
    queue_timeout: 300

# MODEL CONFIGURATION
# ==================
models:
  # Default model types for different tasks
  property_prediction:
    default: "random_forest"
    options: ["random_forest", "xgboost", "neural_network", "graph_neural_network"]

  molecular_generation:
    default: "vae"
    options: ["vae", "gan", "transformer", "diffusion"]

  # Auto-ML settings
  automl:
    enabled: false
    time_budget: 3600  # 1 hour
    task_type: "regression"  # classification, regression
    ensemble_size: 5

# DATA CONFIGURATION
# ==================
data:
  # Data loading and preprocessing
  batch_size: 32
  num_workers: 4
  pin_memory: true

  # Caching
  cache_enabled: true
  cache_dir: "./cache"
  cache_size_gb: 10

  # Data validation
  validation:
    enabled: true
    missing_value_threshold: 0.1
    duplicate_threshold: 0.05

# UNCERTAINTY QUANTIFICATION
# ==========================
uncertainty:
  # Uncertainty methods
  methods: ["ensemble", "dropout", "conformal"]

  # Ensemble settings
  ensemble:
    n_models: 5
    bootstrap: true

  # Conformal prediction
  conformal:
    confidence_level: 0.95
    calibration_split: 0.2

  # Bayesian methods
  bayesian:
    inference_method: "variational"  # variational, mcmc
    n_samples: 1000

# LOGGING CONFIGURATION
# ====================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./chemml.log"
  max_file_size_mb: 100
  backup_count: 5

# REPRODUCIBILITY
# ===============
reproducibility:
  random_seed: 42
  deterministic: true
  benchmark: true  # For PyTorch
